{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# import random\n",
    "\n",
    "# # start a new wandb run to track this script\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"my-awesome-project\",\n",
    "    \n",
    "#     # track hyperparameters and run metadata\n",
    "#     config={\n",
    "#     \"learning_rate\": 0.02,\n",
    "#     \"architecture\": \"CNN\",\n",
    "#     \"dataset\": \"CIFAR-100\",\n",
    "#     \"epochs\": 10,\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# # simulate training\n",
    "# epochs = 10\n",
    "# offset = random.random() / 5\n",
    "# for epoch in range(2, epochs):\n",
    "#     acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
    "#     loss = 2 ** -epoch + random.random() / epoch + offset\n",
    "    \n",
    "#     # log metrics to wandb\n",
    "#     wandb.log({\"acc\": acc, \"loss\": loss})\n",
    "    \n",
    "# # [optional] finish the wandb run, necessary in notebooks\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS not available because the current PyTorch install was not built with MPS enabled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Check that MPS is available\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    print(\"MPS is available on this machine - enabling MPS for PyTorch tensors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda 2\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import re\n",
    "import shutil\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    RobertaConfig,\n",
    "    RobertaForMaskedLM,\n",
    "    RobertaTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import wandb\n",
    "import random\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(device, n_gpu)\n",
    "if n_gpu < 1:\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    n_gpu = 1\n",
    "    print(device, n_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/04/2024 23:28:14 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 2, distributed training: False, 16-bits training: False\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "local_rank = -1\n",
    "fp16 = False\n",
    "logger.warning(\n",
    "    \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "    local_rank,\n",
    "    device,\n",
    "    n_gpu,\n",
    "    bool(local_rank != -1),\n",
    "    fp16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DBG] Data: /home/myoungkyu/Documents/2021MSR-StudyBert-RoBertaCode/roberta/DATASET/JAVA/TOKEN/RAW/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# DATASET_JAVA_TOKEN_RAW = '/home/myoungkyu@unomaha.edu/Documents/\\\n",
    "# 2021MSR-RoBertaCode/roberta/DATASET/JAVA/TOKEN/RAW/'\n",
    "\n",
    "DATASET_JAVA_TOKEN_RAW = os.environ['HOME']+'/Documents/2021MSR-StudyBert-RoBertaCode/roberta/DATASET/JAVA/TOKEN/RAW/'\n",
    "if os.path.isdir(DATASET_JAVA_TOKEN_RAW):\n",
    "    print('[DBG] Data:', DATASET_JAVA_TOKEN_RAW)\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.model_type = 'roberta'\n",
    "        self.line_by_line = True\n",
    "        self.model_name_or_path = ''\n",
    "        self.mlm = True\n",
    "        self.mlm_probability = 0.2\n",
    "        self.tokenizer_name = 'roberta-base'#tokenizer_name\n",
    "        self.cache_dir = 'cache'\n",
    "        self.block_size = -1\n",
    "        self.do_train = True\n",
    "        self.do_eval = True\n",
    "        self.evaluate_during_training = True\n",
    "        self.per_gpu_train_batch_size = 2 #4 #8 #wandb.config.batch_size\n",
    "        self.gradient_accumulation_steps = 4 #wandb.config.gradient_accumulation_steps\n",
    "        self.learning_rate = 5e-5 #wandb.config.learning_rate\n",
    "        self.weight_decay = 0.0\n",
    "        self.adam_epsilon = 1e-8\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.num_train_epochs = 1 # 50 # wandb.config.epochs\n",
    "        self.max_steps = -1\n",
    "        self.warmup_steps = 0\n",
    "        self.logging_steps = 500 #logging_steps\n",
    "        self.save_total_limit = None\n",
    "        self.eval_all_checkpoints = False\n",
    "        self.no_cuda = False\n",
    "        self.overwrite_output_dir = True\n",
    "        self.seed = 42\n",
    "        self.fp16 = False\n",
    "        self.fp16_opt_level = \"O1\"\n",
    "        self.local_rank = -1\n",
    "        self.server_ip = \"\"\n",
    "        self.server_port = \"\"\n",
    "\n",
    "args = Args(\n",
    "    # run, parsed_args.train_data_file, parsed_args.eval_data_file,\n",
    "    # parsed_args.output_root, parsed_args.tokenizer_name, parsed_args.vocab_size,\n",
    "    # parsed_args.early_stop, parsed_args.logging_steps, parsed_args.should_continue\n",
    ")\n",
    "args.device = device\n",
    "args.n_gpu = n_gpu\n",
    "args.vocab_size = 50_000\n",
    "args.train_data_file = DATASET_JAVA_TOKEN_RAW + 'training_masked_code-small20per' # 20percent small\n",
    "args.eval_data_file = DATASET_JAVA_TOKEN_RAW + 'eval_masked_code-small20per' # 20percent small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "seed = 42\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "set_seed(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(transformers.models.roberta.configuration_roberta.RobertaConfig,\n",
       " transformers.models.roberta.modeling_roberta.RobertaForMaskedLM,\n",
       " transformers.models.roberta.tokenization_roberta.RobertaTokenizer)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    RobertaConfig,\n",
    "    RobertaForMaskedLM,\n",
    "    RobertaTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "MODEL_CLASSES = {\n",
    "    \"roberta\": (RobertaConfig, RobertaForMaskedLM, RobertaTokenizer),\n",
    "}\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "config_class, model_class, tokenizer_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DBG] Removed /home/myoungkyu/Documents/2021MSR-StudyBert-RoBertaCode/roberta/DATASET/JAVA/TOKEN/RAW/tokenizer/output/vocab.json\n",
      "[DBG] Removed /home/myoungkyu/Documents/2021MSR-StudyBert-RoBertaCode/roberta/DATASET/JAVA/TOKEN/RAW/tokenizer/output/merges.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "output_trained_tokenizer = os.path.abspath(DATASET_JAVA_TOKEN_RAW + 'tokenizer/output')\n",
    "files = glob.glob(os.path.join(output_trained_tokenizer, '*'))\n",
    "for file in files:\n",
    "    try:\n",
    "        if os.path.isfile(file):\n",
    "            os.remove(file)\n",
    "            print('[DBG] Removed', file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while deleting {file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "/home/myoungkyu/Documents/2021MSR-StudyBert-RoBertaCode/roberta/DATASET/JAVA/TOKEN/RAW/tokenizer/output\n",
      "/home/myoungkyu/Documents/2021MSR-StudyBert-RoBertaCode/roberta/DATASET/JAVA/TOKEN/RAW/tokenizer/output/vocab.json\n",
      "/home/myoungkyu/Documents/2021MSR-StudyBert-RoBertaCode/roberta/DATASET/JAVA/TOKEN/RAW/tokenizer/output/merges.txt\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import os\n",
    "\n",
    "output_trained_tokenizer = os.path.abspath(DATASET_JAVA_TOKEN_RAW + 'tokenizer/output')\n",
    "\n",
    "# ###################################################################################\n",
    "# Note!! The following code should be executed once to save the result on the disk. \n",
    "# ###################################################################################\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "train_data_file_tokenizer = DATASET_JAVA_TOKEN_RAW + 'tokenizer_training'\n",
    "path = os.path.abspath(train_data_file_tokenizer)\n",
    "tokenizer.train(files=path, vocab_size=50000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n",
    "tokenizer.add_special_tokens([\"<x>\",\"<z>\"])\n",
    "# Save files to disk\n",
    "if not os.path.exists(output_trained_tokenizer):\n",
    "    os.makedirs(output_trained_tokenizer)\n",
    "saved_files = tokenizer.save_model(output_trained_tokenizer)\n",
    "print(str(output_trained_tokenizer))\n",
    "for f in saved_files:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizer'.\n",
      "01/04/2024 23:28:54 - INFO - __main__ -   Training new model from scratch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaTokenizer(name_or_path='/home/myoungkyu/Documents/2021MSR-StudyBert-RoBertaCode/roberta/DATASET/JAVA/TOKEN/RAW/tokenizer/output', vocab_size=50000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "1000000000000000019884624838656\n"
     ]
    }
   ],
   "source": [
    "def get_config(args):\n",
    "    config = {\n",
    "        \"model_type\": \"roberta\",\n",
    "        \"attention_probs_dropout_prob\": 0.1,\n",
    "        \"hidden_act\": \"gelu\",\n",
    "        \"hidden_dropout_prob\": 0.3,\n",
    "        \"hidden_size\": 768, #wandb.config.hidden_size,\n",
    "        \"initializer_range\": 0.02,\n",
    "        \"num_attention_heads\": 16, #wandb.config.num_attention_heads,\n",
    "        \"num_hidden_layers\": 12, #wandb.config.num_hidden_layers,\n",
    "        \"vocab_size\": 1_130, #args.vocab_size,\n",
    "        \"intermediate_size\": 4_096, #wandb.config.intermediate_size,\n",
    "        \"max_position_embeddings\": 1024,\n",
    "        \"cache_dir\": '' #args.cache_dir\n",
    "    }\n",
    "    return RobertaConfig(**config)\n",
    "\n",
    "config = get_config(args)\n",
    "tokenizer = tokenizer_class.from_pretrained(output_trained_tokenizer, cache_dir=args.cache_dir)\n",
    "args.block_size = tokenizer.model_max_length\n",
    "print(tokenizer)\n",
    "print(tokenizer.model_max_length)\n",
    "\n",
    "logger.info(\"Training new model from scratch\")\n",
    "try:\n",
    "    model = model_class(config=config)\n",
    "except Exception as e:\n",
    "    logger.error(f'{e} Configuration not correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_type: roberta\n",
      "line_by_line: True\n",
      "model_name_or_path: \n",
      "mlm: True\n",
      "mlm_probability: 0.2\n",
      "tokenizer_name: roberta-base\n",
      "cache_dir: cache\n",
      "block_size: 1000000000000000019884624838656\n",
      "do_train: True\n",
      "do_eval: True\n",
      "evaluate_during_training: True\n",
      "per_gpu_train_batch_size: 2\n",
      "gradient_accumulation_steps: 4\n",
      "learning_rate: 5e-05\n",
      "weight_decay: 0.0\n",
      "adam_epsilon: 1e-08\n",
      "max_grad_norm: 1.0\n",
      "num_train_epochs: 1\n",
      "max_steps: -1\n",
      "warmup_steps: 0\n",
      "logging_steps: 500\n",
      "save_total_limit: None\n",
      "eval_all_checkpoints: False\n",
      "no_cuda: False\n",
      "overwrite_output_dir: True\n",
      "seed: 42\n",
      "fp16: False\n",
      "fp16_opt_level: O1\n",
      "local_rank: -1\n",
      "server_ip: \n",
      "server_port: \n",
      "device: cuda\n",
      "n_gpu: 2\n",
      "vocab_size: 50000\n",
      "train_data_file: /home/myoungkyu/Documents/2021MSR-StudyBert-RoBertaCode/roberta/DATASET/JAVA/TOKEN/RAW/training_masked_code-small20per\n",
      "eval_data_file: /home/myoungkyu/Documents/2021MSR-StudyBert-RoBertaCode/roberta/DATASET/JAVA/TOKEN/RAW/eval_masked_code-small20per\n",
      "[DBG] Device: cuda\n"
     ]
    }
   ],
   "source": [
    "def add_special_tokens_(model, tokenizer):\n",
    "    \"\"\" Add special tokens to the tokenizer and the model if they have not already been added. \"\"\"\n",
    "    orig_num_tokens = len(tokenizer.encoder)\n",
    "    num_added_tokens = tokenizer.add_special_tokens({'additional_special_tokens': ['<z>', '<x>']}) # doesn't add if they are already there\n",
    "    if num_added_tokens > 0:\n",
    "        model.resize_token_embeddings(new_num_tokens=orig_num_tokens + num_added_tokens)\n",
    "\n",
    "add_special_tokens_(model, tokenizer)\n",
    "model.to(args.device)\n",
    "\n",
    "attributes = vars(args)\n",
    "for key, value in attributes.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print('[DBG] Device:', args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50002"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DBG] output_trained_tokenizer:  /home/myoungkyu/Documents/2021MSR-StudyBert-RoBertaCode/roberta/DATASET/JAVA/TOKEN/RAW/tokenizer/output\n",
      "/home/myoungkyu/Documents/2021MSR-StudyBert-RoBertaCode/roberta/DATASET/JAVA/TOKEN/RAW/training_masked_code-small20per\n",
      "/home/myoungkyu/Documents/2021MSR-StudyBert-RoBertaCode/roberta/DATASET/JAVA/TOKEN/RAW/tokenizer/output\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "\"\"\"\n",
    "In the given code snippet, `tokenizer.encode_batch(lines)` returns a list of `Encoding`\n",
    "objects, each representing the tokenization of a line. The `.ids` attribute of each\n",
    "`Encoding` object contains the actual token IDs produced during the tokenization process.\n",
    "\n",
    "When you're dealing with a pretrained tokenizer, token IDs are the integer representations\n",
    "of tokens in the vocabulary. For example, in a BPE tokenizer, each token is represented by\n",
    "a unique integer ID.\n",
    "\n",
    "So, `x.ids` is crucial because it gives you the actual token IDs after tokenizing the\n",
    "lines using the tokenizer. These token IDs are what the model will process during\n",
    "training.\n",
    "\"\"\"\n",
    "class LineByLineDatasetWithBPETokenizer(Dataset):\n",
    "    def __init__(self, data_file_path: str = None, tokenizer_path: str = None):\n",
    "        tokenizer = ByteLevelBPETokenizer(\n",
    "            tokenizer_path + \"/vocab.json\",\n",
    "            tokenizer_path + \"/merges.txt\",\n",
    "        )\n",
    "        tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "            (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "            (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "        )\n",
    "        tokenizer.enable_truncation(max_length=512)\n",
    "\n",
    "        self.examples = []\n",
    "\n",
    "        with open(data_file_path, encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            lines = [line for line in lines if (len(line) > 0 and not line.isspace())]\n",
    "            self.examples += [x.ids for x in tokenizer.encode_batch(lines)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # We’ll pad at the batch level.\n",
    "        return torch.tensor(self.examples[i])\n",
    "\n",
    "def load_and_cache_examples(args, evaluate=False):\n",
    "    data_file_path = args.eval_data_file if evaluate else args.train_data_file\n",
    "    print(data_file_path)\n",
    "    args.tokenizer_name = output_trained_tokenizer\n",
    "    print(args.tokenizer_name)\n",
    "    return LineByLineDatasetWithBPETokenizer(data_file_path, args.tokenizer_name)\n",
    "\n",
    "print('[DBG] output_trained_tokenizer: ', output_trained_tokenizer)\n",
    "train_dataset = load_and_cache_examples(args, evaluate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0,   316,   378,  1279,  1368,  2111,  7257,    12,  7704,   731,\n",
      "          630,    16,   359,   523,  8454,    13,   451,  2958,  9380,  3595,\n",
      "         2012,    16,  7276,    16, 10018,   535,    92,    34,  5012,   523,\n",
      "         1658,   283,   341,  5012,    63,    22, 16448,  1658,    63,    20,\n",
      "           65,   283,   341,  5012,  6205,   630,   512,   391,  7704,   512,\n",
      "          731,   630,  2316,  1658,    63,    21,    65,   283,   341,  5012,\n",
      "          393,  8488,   512,   391,   309, 24747,  8454,  2316, 14425,   429,\n",
      "         2608,   393,  1444,  1368,  2111,  7257,   512,  1658,    16,   350,\n",
      "         2316,   270,   203,     2])\n",
      "tensor([    0,   316,   378,  1279,  1368,  2111,  7257,    12,  7704,   731,\n",
      "          630,    16,   359,   523,  8454,    13,   451,  2958,  9380,  3595,\n",
      "         2012,    16,  7276,    16, 10018,    16,  2543,  1825,  1050,  5012,\n",
      "          523,  1658,   535,    92,    34,  1658,    63,    20,    65,   283,\n",
      "          341,  5012,  6205,   630,   512,   391,  7704,   512,   731,   630,\n",
      "         2316,  1658,    63,    21,    65,   283,   341,  5012,   393,  8488,\n",
      "          512,   391,   309, 24747,  8454,  2316, 14425,   429,  2608,   393,\n",
      "         1444,  1368,  2111,  7257,   512,  1658,    16,   350,  2316,   270,\n",
      "          203,     2])\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for info in train_dataset:\n",
    "    counter+=1\n",
    "    if (counter < 3):\n",
    "        print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision(probability):\n",
    "    return random.random() < probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function, `read_masked_dataset`, is designed to read a dataset of masked instances,\n",
    "preprocess it, and return tensors suitable for model training. It takes a tokenizer, a\n",
    "batch of masked instances, and a list of labels to process. The function decodes and\n",
    "encodes these instances, pads them to the maximum length, and creates tensors for both\n",
    "inputs and labels. Special tokens, such as `<s>` and `</s>`, are removed, and the labels\n",
    "are padded with -100 for unmasked tokens. The resulting tensors, `inputs` and `labels`,\n",
    "are then returned. Overall, the function prepares the data for masked language model\n",
    "training by ensuring consistency in sequence length and appropriate tokenization.\n",
    " \n",
    "The `read_masked_dataset` function processes a batch of masked instances and their\n",
    "corresponding labels for model training. It utilizes a tokenizer to decode and encode\n",
    "these instances, removing special tokens and ensuring a consistent length. The resulting\n",
    "labels are converted into a tensor, `labels_to_convert_in_tensor`, where unmasked tokens\n",
    "are padded with -100. Similarly, the masked instances are processed and converted into a\n",
    "tensor, `inputs_to_convert`, with padding for a uniform length. The function returns these\n",
    "two tensors, `inputs` and `labels`, providing preprocessed data suitable for training a\n",
    "masked language model. # \n",
    "\"\"\"\n",
    "def read_masked_dataset(tokenizer: PreTrainedTokenizer, batch, labels_to_process) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    # The inputs are already masked in the training file\n",
    "    tmp_inputs = batch.clone()\n",
    "\n",
    "    tmp_inputs_list = []\n",
    "    for input in tmp_inputs:\n",
    "        decoded_input = tokenizer.decode(input)\n",
    "        encoded_back = tokenizer.encode(decoded_input)[1:-1] # Removes the additional <s> and </s> added\n",
    "        tmp_inputs_list.append(encoded_back)\n",
    "\n",
    "    # Gets the maximum length between inputs and labels_lines\n",
    "    # We then need to adapt one or the other to have the same length through padding\n",
    "    max_length_inputs = max([len(input) for input in tmp_inputs_list])\n",
    "    max_length_labels_lines = max([len(label) for label in labels_to_process])\n",
    "    max_length = max_length_inputs\n",
    "    if max_length_labels_lines > max_length_inputs:\n",
    "        max_length = max_length_labels_lines\n",
    "\n",
    "    # Create the labels as tensor\n",
    "    labels_to_convert_in_tensor = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(batch):\n",
    "        l1_tmp = tokenizer.encode(labels_to_process[i])\n",
    "        label_to_add = []\n",
    "        for token in l1_tmp:\n",
    "            if token != tokenizer.bos_token_id and token != tokenizer.eos_token_id:  # Remove special tokens\n",
    "                label_to_add.append(token)\n",
    "\n",
    "        j = len(label_to_add)\n",
    "        while j < max_length:\n",
    "            label_to_add.append(-100)  # we only compute loss for masked tokens\n",
    "            j += 1\n",
    "\n",
    "        labels_to_convert_in_tensor.append(label_to_add)\n",
    "        i += 1\n",
    "\n",
    "    labels = torch.as_tensor(labels_to_convert_in_tensor)\n",
    "\n",
    "    # Create the train data set input batch as tensor.\n",
    "    inputs_to_convert = []\n",
    "    for input in tmp_inputs_list:\n",
    "        tmp_input = []\n",
    "        for token in input:\n",
    "            tmp_input.append(token)\n",
    "\n",
    "        i = len(tmp_input)\n",
    "        while i < max_length:\n",
    "            tmp_input.append(tokenizer.pad_token_id)\n",
    "            i += 1\n",
    "        inputs_to_convert.append(tmp_input)\n",
    "\n",
    "    inputs = torch.as_tensor(inputs_to_convert)\n",
    "\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the masked instances, but provide them as non-masked to the model\n",
    "# This is used in 10% of cases during training\n",
    "def get_non_masked_instances(tokenizer: PreTrainedTokenizer, batch, labels_to_process) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    tmp_inputs_list = []\n",
    "    i = 0\n",
    "    while i < len(batch):\n",
    "        decoded_input = tokenizer.decode(batch[i]).replace('<x>',str(labels_to_process[i]).replace('<z>\\n',''))\n",
    "        encoded_back = tokenizer.encode(decoded_input)[1:-1]\n",
    "        tmp_inputs_list.append(encoded_back)\n",
    "        i += 1\n",
    "\n",
    "    # Gets the maximum length\n",
    "    max_length = max([len(input) for input in tmp_inputs_list])\n",
    "\n",
    "    inputs_to_convert = []\n",
    "    labels_to_convert = []\n",
    "    for input in tmp_inputs_list:\n",
    "        tmp_input = []\n",
    "        tmp_label = []\n",
    "        tmp_label.append(tokenizer.convert_tokens_to_ids('<z>'))\n",
    "        for token in input:\n",
    "            tmp_input.append(token)\n",
    "            tmp_label.append(-100)\n",
    "\n",
    "        del tmp_label[-1] #Accounts for the fact that tmp_label already contains <z>\n",
    "\n",
    "        i = len(tmp_input)\n",
    "        while i < max_length:\n",
    "            tmp_input.append(tokenizer.pad_token_id)\n",
    "            tmp_label.append(-100)\n",
    "            i += 1\n",
    "        labels_to_convert.append(tmp_label)\n",
    "        inputs_to_convert.append(tmp_input)\n",
    "\n",
    "    inputs = torch.as_tensor(inputs_to_convert)\n",
    "    labels = torch.as_tensor(labels_to_convert)\n",
    "\n",
    "    # We train the model to understand that if no masked tokens are present, nothing must be produced, only <z>\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.embeddings.word_embeddings.weight\n",
      "roberta.embeddings.position_embeddings.weight\n",
      "roberta.embeddings.token_type_embeddings.weight\n",
      "roberta.embeddings.LayerNorm.weight\n",
      "no decay: roberta.embeddings.LayerNorm.weight\n",
      "roberta.embeddings.LayerNorm.bias\n",
      "no decay: roberta.embeddings.LayerNorm.bias\n",
      "roberta.encoder.layer.0.attention.self.query.weight\n",
      "roberta.encoder.layer.0.attention.self.query.bias\n",
      "no decay: roberta.encoder.layer.0.attention.self.query.bias\n",
      "roberta.encoder.layer.0.attention.self.key.weight\n",
      "roberta.encoder.layer.0.attention.self.key.bias\n",
      "no decay: roberta.encoder.layer.0.attention.self.key.bias\n",
      "Total parameters of model.named_parameters(): 202\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "for n, p in model.named_parameters():\n",
    "    counter+=1\n",
    "    if(counter < 10):\n",
    "        print(n)\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            print('no decay:',n)\n",
    "print('Total parameters of model.named_parameters():', counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from datetime import datetime\n",
    "\n",
    "def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    t0 = datetime.now()\n",
    "    # if args.local_rank in [-1, 0]:\n",
    "    #     tb_writer = SummaryWriter()\n",
    "\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "    # Designed to be used with PyTorch's `DataLoader`. This function takes a list of\n",
    "    # tensors (examples) and uses `pad_sequence` to pad them to the same length along the\n",
    "    # batch dimension. If the tokenizer has a defined padding token, it uses its ID for\n",
    "    # padding; otherwise, it uses a default padding value. The result is a batch of padded\n",
    "    # sequences ready for model training.\n",
    "    def collate(examples: List[torch.Tensor]):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    train_sampler = SequentialSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate\n",
    "    )\n",
    "\n",
    "    t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    \"\"\"\n",
    "    This code is preparing the parameters for optimization during training. It creates a\n",
    "    list of dictionaries, each corresponding to a parameter group for the optimizer. There\n",
    "    are two parameter groups:\n",
    "\n",
    "    1. **Weight Decay Group**: Parameters that should undergo weight decay. These are\n",
    "       selected based on their names, and any parameters with names containing specific\n",
    "       substrings (specified in `no_decay`) are excluded from weight decay. The weight\n",
    "       decay for this group is set to `args.weight_decay`.\n",
    "\n",
    "    2. **No Weight Decay Group**: Parameters that should not undergo weight decay. Again,\n",
    "       parameters are selected based on their names, and any parameters with names\n",
    "       containing substrings from `no_decay` are included in this group. The weight decay\n",
    "       for this group is set to 0.0.\n",
    "\n",
    "    This configuration is commonly used when fine-tuning models. Weight decay is a\n",
    "    regularization term added to the loss function during training to prevent overfitting.\n",
    "    However, it's often desirable not to apply weight decay to certain parameters, such as\n",
    "    bias terms, which can be achieved by specifying different weight decay values for\n",
    "    different parameter groups.    \n",
    "    \"\"\"\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \n",
    "            \"weight_decay\": 0.0\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    \"\"\" Train! \"\"\"\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  args.train_batch_size: %d\", args.train_batch_size)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
    "    logger.info(\n",
    "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "        args.train_batch_size\n",
    "        * args.gradient_accumulation_steps\n",
    "        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
    "    )\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    epochs_trained = 0\n",
    "    steps_trained_in_current_epoch = 0\n",
    "    \"\"\" \n",
    "        Check if continuing training from a checkpoint \n",
    "        Skipped \n",
    "    \"\"\"\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    \n",
    "    model_to_resize = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n",
    "    model_to_resize.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(\n",
    "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
    "    )\n",
    "    set_seed(args)  # Added here for reproducibility\n",
    "\n",
    "    labels_file = str(args.train_data_file).replace('masked_code_','mask_')\n",
    "    labels_lines = [line.rstrip() for line in open(labels_file)]\n",
    "\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
    "        # step is the count of the steps performed, batch contains the actual input data\n",
    "\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            # Skip past any already trained steps if resuming training\n",
    "            if steps_trained_in_current_epoch > 0:\n",
    "                steps_trained_in_current_epoch -= 1\n",
    "                continue\n",
    "\n",
    "            # Get the labels lines to process\n",
    "            start = step * len(batch)\n",
    "            end = start + len(batch) + 1\n",
    "            lables_to_process = labels_lines[start:end]\n",
    "\n",
    "            # In 90% of cases, we used the inputs with the masked tokens\n",
    "            # In 10% of cases we don't mask any token\n",
    "            if decision(0.9):\n",
    "                inputs, labels = read_masked_dataset(tokenizer, batch, lables_to_process)\n",
    "            else:\n",
    "                inputs, labels = get_non_masked_instances(tokenizer, batch, lables_to_process)\n",
    "\n",
    "            inputs = inputs.to(args.device)\n",
    "            labels = labels.to(args.device)\n",
    "            model.train()\n",
    "            # outputs = model(inputs, masked_lm_labels=labels) if args.mlm else model(inputs, labels=labels)\n",
    "            outputs = model(inputs, labels=labels) if args.mlm else model(inputs, labels=labels)\n",
    "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                # if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                #     # Log metrics\n",
    "                #     if (\n",
    "                #         args.local_rank == -1 and args.evaluate_during_training\n",
    "                #     ):  # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                #         results = evaluate(args, model, tokenizer)\n",
    "                #         if results is None:\n",
    "                #             print(\"Stopping condition reached, no improvement in evaluation set\")\n",
    "                #             sys.exit(0)\n",
    "\n",
    "                #     logging_loss = tr_loss\n",
    "\n",
    "\n",
    "    dt = datetime.now() - t0\n",
    "    print(f'[DBG] Duration: {dt}')\n",
    "    return global_step, tr_loss / global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, prefix=\"\") -> Dict:\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    eval_output_dir = args.output_dir\n",
    "\n",
    "    eval_dataset = load_and_cache_examples(args, evaluate=True)\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        os.makedirs(eval_output_dir, exist_ok=True)\n",
    "\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    # Note that DistributedSampler samples randomly\n",
    "\n",
    "    def collate(examples: List[torch.Tensor]):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate\n",
    "    )\n",
    "\n",
    "\n",
    "    result = None\n",
    "\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
    "logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import tokenizers\n",
    "\n",
    "print(f'transformers.__version__: {transformers.__version__}')\n",
    "print(f'torch.__version__: {torch.__version__}')\n",
    "print(f'tokenizers.__version__: {tokenizers.__version__}')\n",
    "print(f'wandb.__version__: {wandb.__version__}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_python3_11c",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
