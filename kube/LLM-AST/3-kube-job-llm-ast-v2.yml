apiVersion: batch/v1
kind: Job
metadata:
  name: job02-llmast-shradha-gp-engine-unoselab02-a40-01
spec:
  ttlSecondsAfterFinished: 86400 # a day
  template:
    spec:
      containers:
        - name: job-llmast-model-train-container
          image: gitlab-registry.nrp-nautilus.io/msong/research/env_mlm:v1
          workingDir: /datalarge
          command: ["sh", "-c"]
          args: ["cd /datalarge;
              ls -la /datalarge;
              mamba create -y -n llmast_env;
              source activate llmast_env;
              pip install scikit-learn==1.3.2 transformers==4.36.2 datasets==2.16.1 sentencepiece==0.1.99 sacremoses==0.1.1 accelerate==0.25.0 torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 prettytable==3.10.0;
              pip install nltk psutil gputil dataclasses chardet rouge==1.0.0 typing antlr4-tools;
              conda install -c conda-forge tensorboard;
              pip install tensorboardX;
              pip show transformers torch;
              pip check;
              pip install tree_sitter==0.21.0; 
              pip install antlr4-python3-runtime==4.9.2;
              tar xf llm_ast_code_gen.tar.gz;
              cd deploy-spt-code/spt-code/sources;
              python main.py --do-pre-train --train-from-scratch --pre-train-tasks cap --batch-size 128 --eval-batch-size 128 --cuda-visible-devices 0,1,2,3,4,5 --fp16 --model-name pre_train --n-epoch 30 --n-epoch-pre-train 30 --task completion --remove-existing-saved-file pre_train:fine_tune --ast-type \"jdt\" --dataset-size \"large\""]
          volumeMounts:
            - name: pvc-shra-llmast-gp-engine-unoselab02-ea
              mountPath: /datalarge
          resources:
            limits:
              memory: 32Gi
              cpu: "4"
              nvidia.com/a40: 6
              ephemeral-storage: 48Gi
            requests:
              memory: 32Gi
              cpu: "4"
              nvidia.com/a40: 6
              ephemeral-storage: 48Gi
          securityContext:
            allowPrivilegeEscalation: false
            runAsUser: 0
      volumes:
        - name: pvc-shra-llmast-gp-engine-unoselab02-ea
          persistentVolumeClaim:
            claimName: pvc-shra-llmast-gp-engine-unoselab02-ea
      restartPolicy: Never
      # affinity:
      #   nodeAffinity:
      #     requiredDuringSchedulingIgnoredDuringExecution:
      #       nodeSelectorTerms:
      #         - matchExpressions:
      #           - key: nvidia.com/gpu.product
      #             operator: In
      #             values:
      #               - NVIDIA-GeForce-RTX-3090
  backoffLimit: 1


# apiVersion: batch/v1
# kind: Job
# metadata:
#   name: job03-llmast-shradha-gp-engine-unoselab01
# spec:
#   ttlSecondsAfterFinished: 86400 # a day
#   template:
#     spec:
#       containers:
#         - name: job-llmast-model-train-container
#           image: gitlab-registry.nrp-nautilus.io/msong/research/env_mlm:v1
#           workingDir: /data
#           command: ["sh", "-c"]
#           args: ["cd /data;
#               ls -la /data;
#               mamba create -y -n llmast_env;
#               source activate llmast_env;
#               pip install scikit-learn==1.3.2 transformers==4.36.2 datasets==2.16.1 sentencepiece==0.1.99 sacremoses==0.1.1 accelerate==0.25.0 torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 prettytable==3.10.0;
#               pip install nltk psutil gputil dataclasses chardet rouge==1.0.0 typing antlr4-tools;
#               conda install -c conda-forge tensorboard;
#               pip install tensorboardX;
#               pip show transformers torch;
#               pip check;
#               pip install tree_sitter==0.21.0; 
#               pip install antlr4-python3-runtime==4.9.2;
#               tar xf llm_ast_code_gen_large.tar.gz;
#               cd deploy-spt-code/spt-code/sources;
#               python main.py --do-pre-train --pre-train-tasks cap --batch-size 16 --eval-batch-size 32 --cuda-visible-devices 0,1,2,3,4,5,6,7 --fp16 --model-name pre_train --n-epoch 1 --n-epoch-pre-train 1 --task completion --remove-existing-saved-file pre_train:fine_tune --ast-type \"jdt\""]
#           volumeMounts:
#             - name: pvc-shradha-llmast-gp-engine-unoselab01
#               mountPath: /data
#           resources:
#             limits:
#               memory: 32Gi
#               cpu: "4"
#               nvidia.com/gpu: 8
#               ephemeral-storage: 48Gi
#             requests:
#               memory: 32Gi
#               cpu: "4"
#               nvidia.com/gpu: 8
#               ephemeral-storage: 48Gi
#           securityContext:
#             allowPrivilegeEscalation: false
#             runAsUser: 0
#       volumes:
#         - name: pvc-shradha-llmast-gp-engine-unoselab01
#           persistentVolumeClaim:
#             claimName: pvc-shradha-llmast-gp-engine-unoselab01
#       restartPolicy: Never
#   backoffLimit: 1