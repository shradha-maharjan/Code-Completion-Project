apiVersion: batch/v1
kind: Job
metadata:
  name: job11-llmast-shradha-gp-engine-unoselab02-a6000-v3
spec:
  ttlSecondsAfterFinished: 86400 # a day
  template:
    spec:
      containers:
        - name: job-llmast-model-train-container
          image: gitlab-registry.nrp-nautilus.io/msong/research/env_mlm:v1
          workingDir: /datasmall
          command: ["sh", "-c"]
          args: ["cd /datasmall;
              ls -la /datasmall;
              mamba create -y -n llmast_env;
              source activate llmast_env;
              pip install scikit-learn==1.3.2 transformers==4.36.2 datasets==2.16.1 sentencepiece==0.1.99 sacremoses==0.1.1 accelerate==0.25.0 torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 prettytable==3.10.0;
              pip install nltk psutil gputil dataclasses chardet rouge==1.0.0 typing antlr4-tools;
              conda install -c conda-forge tensorboard;
              pip install tensorboardX;
              pip show transformers torch;
              pip check;
              pip install tree_sitter==0.21.0; 
              pip install antlr4-python3-runtime==4.9.2;
              tar xf llm_ast_code_second.tar.gz;
              cd deploy-spt-code-01/spt-code/sources;
              python main.py --do-pre-train --train-from-scratch --pre-train-tasks cap,mass --batch-size 96 --eval-batch-size 96 --cuda-visible-devices 0,1,2,3 --fp16 --model-name pre_train --n-epoch 30 --n-epoch-pre-train 30 --task completion --remove-existing-saved-file pre_train:fine_tune --ast-type \"jdt\" --dataset-size \"small\""]
          volumeMounts:
            - name: pvc-shra-llmast-gp-engine-unoselab02-03
              mountPath: /datasmall
          resources:
            limits:
              memory: 48Gi
              cpu: "4"
              nvidia.com/rtxa6000: 4 #nvidia.com/gpu: 8
              ephemeral-storage: 48Gi
            requests:
              memory: 48Gi
              cpu: "4"
              nvidia.com/rtxa6000: 4 #nvidia.com/gpu: 8
              ephemeral-storage: 48Gi
          securityContext:
            allowPrivilegeEscalation: false
            runAsUser: 0
      volumes:
        - name: pvc-shra-llmast-gp-engine-unoselab02-03
          persistentVolumeClaim:
            claimName: pvc-shra-llmast-gp-engine-unoselab02-03
      restartPolicy: Never
      # affinity:
      #   nodeAffinity:
      #     requiredDuringSchedulingIgnoredDuringExecution:
      #       nodeSelectorTerms:
      #         - matchExpressions:
      #           - key: nvidia.com/gpu.product
      #             operator: In
      #             values:
      #               - NVIDIA-A100-SXM4-80GB
  backoffLimit: 1
# apiVersion: batch/v1
# kind: Job
# metadata:
#   name: job14-llmast-shradha-gp-engine-unoselab01
# spec:
#   ttlSecondsAfterFinished: 86400 # a day
#   template:
#     spec:
#       containers:
#         - name: job-llmast-model-train-container
#           image: docker.io/shradhamaharjan/env_llmast:v1
#           #gitlab-registry.nrp-nautilus.io/msong/research/env_mlm:v1
#           workingDir: /data
#           command: ["sh", "-c"]
#           args:
#             - |
#               cd /data;
#               pip install nltk psutil gputil dataclasses chardet rouge==1.0.0 typing antlr4-tools;
#               conda install -c conda-forge tensorboard;
#               pip install tensorboardX;
#               pip show transformers torch;
#               pip check;
#               pip install tree_sitter==0.21.0;
#               pip install antlr4-python3-runtime==4.9.2;
#               tar xf llm_ast_code.tar.gz;
#               cd deploy-spt-code/spt-code/sources;
#               python main.py --do-pre-train --pre-train-tasks cap --batch-size 16 --eval-batch-size 32 --cuda-visible-devices 0,1,2,3,4,5,6,7 --fp16 --model-name pre_train --n-epoch 1 --n-epoch-pre-train 1 --task completion --remove-existing-saved-file pre_train:fine_tune --ast-type "jdt";
#           volumeMounts:
#             - name: pvc-shradha-llmast-gp-engine-unoselab01
#               mountPath: /data
#           resources:
#             limits:
#               memory: 32Gi
#               cpu: "4"
#               nvidia.com/gpu: 8
#               ephemeral-storage: 48Gi
#             requests:
#               memory: 32Gi
#               cpu: "4"
#               nvidia.com/gpu: 8
#               ephemeral-storage: 48Gi
#           securityContext:
#             allowPrivilegeEscalation: false
#             runAsUser: 0
#       volumes:
#         - name: pvc-shradha-llmast-gp-engine-unoselab01
#           persistentVolumeClaim:
#             claimName: pvc-shradha-llmast-gp-engine-unoselab01
#       restartPolicy: Never
#   backoffLimit: 1


# python -m venv old_version_env;
# source old_version_env/bin/activate;
# source activate old_version_env;
# pip install scikit-learn==1.3.2 transformers==4.36.2 datasets==2.16.1 sentencepiece==0.1.99 sacremoses==0.1.1 accelerate==0.25.0 torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 prettytable==3.10.0;
#tar -xzf llm_ast_sptcode.tar.gz;
#tar xf llm_ast_code.tar.gz;
#python main.py --do-pre-train --pre-train-tasks cap --batch-size 16 --eval-batch-size 32 --cuda-visible-devices 0,1,2,3,4,5,6,7 --fp16 --model-name pre_train --n-epoch 1 --n-epoch-pre-train 1 --pre-train-subset-ratio 0.1 --parse-subset-ratio 0.01 --task completion --remove-existing-saved-file pre_train:fine_tune --ast-type "jdt";