{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found config: /home/user1-selab3/Documents/research-shradha/kube/kube1-sklearn/config\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "cfg_path = Path.home().resolve() / \"config\"\n",
    "\n",
    "if cfg_path.is_file():\n",
    "    print(f\"Found config: {cfg_path}\")\n",
    "else: \n",
    "    cfg_dir = input(\"Enter directory with config: \")\n",
    "    cfg_path = Path(cfg_dir).resolve() / \"config\"\n",
    "    if cfg_path.is_file():\n",
    "        print(f\"Found config: {cfg_path}\")\n",
    "    else:\n",
    "        print(\"ERROR: Re-run this cell and give correct directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Config correctly configured\n"
     ]
    }
   ],
   "source": [
    "from shutil import copy\n",
    "from pathlib import Path\n",
    "\n",
    "dest_dir = Path.home().resolve() / \".kube\"\n",
    "dest_dir.mkdir(exist_ok=True)\n",
    "dest_path = dest_dir / \"config\"\n",
    "\n",
    "if dest_path.is_file():\n",
    "    print(\"SUCCESS: Config correctly configured\")\n",
    "elif cfg_path.is_file() and dest_dir.is_dir():\n",
    "    copy(cfg_path, dest_path)\n",
    "    print(\"SUCCESS: Copied config\")\n",
    "else:\n",
    "    print(\"ERROR: Ensure you have correct config path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gp-engine-unoselab01"
     ]
    }
   ],
   "source": [
    "! kubectl config view --minify -o jsonpath='{..namespace}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Template\n",
    "\n",
    "# read in the template\n",
    "with open('/home/user1-selab3/Documents/research-shradha/kube/kube2/example1-kube/CODE-RajulShakywar/CODE/example/yaml/yaml_templates/pvc_template.yml') as file_:\n",
    "    template = Template(file_.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace None \n",
    "PVC_NAME = 'pvc-shradha-gp-engine-unoselab01'\n",
    "\n",
    "pvc_spec = template.render(name=PVC_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: v1\n",
      "kind: PersistentVolumeClaim\n",
      "metadata:\n",
      "  name: pvc-shradha-gp-engine-unoselab01\n",
      "spec:\n",
      "  storageClassName: rook-cephfs-central\n",
      "  accessModes:\n",
      "  - ReadWriteMany\n",
      "  resources:\n",
      "    requests:\n",
      "      storage: 50Gi\n"
     ]
    }
   ],
   "source": [
    "print(pvc_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/user1-selab3/Documents/research-shradha/kube/kube1-sklearn/1-kube-pvc-sklean.yml\", \"w\") as file:\n",
    "    file.write(pvc_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error from server (AlreadyExists): error when creating \"../kube1-sklearn/1-kube-pvc-sklean.yml\": persistentvolumeclaims \"pvc-shradha-gp-engine-unoselab01\" already exists\n"
     ]
    }
   ],
   "source": [
    "! kubectl create -f ../kube1-sklearn/1-kube-pvc-sklean.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE\n",
      "pvc-gp-engine-unoselab01           Bound    pvc-3689372b-bf1f-40b3-b2aa-f2ed83257150   50Gi       RWX            rook-cephfs-central   50d\n",
      "pvc-shradha-gp-engine-unoselab01   Bound    pvc-d12e42d5-6421-4540-ac64-3beeae853d13   50Gi       RWX            rook-cephfs-central   6d1h\n"
     ]
    }
   ],
   "source": [
    "! kubectl get pvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running random forest with 3 trees and 1 jobs\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /home/user1-selab3/data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:05<00:00, 1977944.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/user1-selab3/data/MNIST/raw/train-images-idx3-ubyte.gz to /home/user1-selab3/data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /home/user1-selab3/data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 800621.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/user1-selab3/data/MNIST/raw/train-labels-idx1-ubyte.gz to /home/user1-selab3/data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /home/user1-selab3/data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 2133067.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/user1-selab3/data/MNIST/raw/t10k-images-idx3-ubyte.gz to /home/user1-selab3/data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /home/user1-selab3/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 1773792.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/user1-selab3/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /home/user1-selab3/data/MNIST/raw\n",
      "\n",
      "Generating Train Features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 60000/60000 [00:09<00:00, 6057.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Test Features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 10000/10000 [00:01<00:00, 6066.27it/s]\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model\n",
      "Evaluating the model\n",
      "Model Accuracy = 82.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from skimage.feature import hog\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "NUM_TREES = int(os.environ.get(\"SK_NUM_TREES\", \"3\"))\n",
    "NUM_JOBS = int(os.environ.get(\"SK_NUM_JOBS\", \"1\"))\n",
    "\n",
    "print(f\"Running random forest with {NUM_TREES} trees and {NUM_JOBS} jobs\")\n",
    "\n",
    "######\n",
    "# Download MNIST\n",
    "######\n",
    "train_dataset = MNIST(download=True, root=\"~/data\", train=True)\n",
    "test_dataset = MNIST(download=True, root=\"~/data\", train=False)\n",
    "\n",
    "##### \n",
    "# Generate Train Features\n",
    "#####\n",
    "print(\"Generating Train Features\")\n",
    "train_features = np.empty((len(train_dataset), 108))\n",
    "train_labels = np.empty(len(train_dataset), np.int32)\n",
    "for i, (img, label) in tqdm(enumerate(train_dataset), ncols=80, total=len(train_dataset)):\n",
    "    train_features[i] = hog(np.asarray(img), orientations=12, cells_per_block=(3,3))\n",
    "    train_labels[i] = label\n",
    "\n",
    "#####\n",
    "# Generate Test Features\n",
    "#####\n",
    "print(\"Generating Test Features\")\n",
    "test_features = np.empty((len(test_dataset), 108))\n",
    "test_labels = np.empty(len(test_dataset), np.int32)\n",
    "for i, (img, label) in tqdm(enumerate(test_dataset), ncols=80, total=len(test_dataset)):\n",
    "    test_features[i] = hog(np.asarray(img), orientations=12, cells_per_block=(3,3))\n",
    "    test_labels[i] = label\n",
    "\n",
    "######\n",
    "# Train Model\n",
    "#######\n",
    "print(\"Training the model\")\n",
    "model = RandomForestClassifier(n_estimators=NUM_TREES, n_jobs=NUM_JOBS, verbose=1)\n",
    "model.fit(train_features, train_labels)\n",
    "\n",
    "####\n",
    "# Score Model\n",
    "#####\n",
    "print(\"Evaluating the model\")\n",
    "model_accuracy = model.score(test_features, test_labels)\n",
    "print(f\"Model Accuracy = {model_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/shradha-gp-engine-unoselab01-pod created\n"
     ]
    }
   ],
   "source": [
    "! kubectl create -f ../kube1-sklearn/2-kube-pod-sklean.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                               READY   STATUS    RESTARTS   AGE\n",
      "gp-engine-unoselab01-pod1          0/1     Error     0          27h\n",
      "shradha-gp-engine-unoselab01-pod   1/1     Running   0          30s\n"
     ]
    }
   ],
   "source": [
    "! kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl cp /home/user1-selab3/Documents/research-shradha/kube/kube1-sklearn/RandomForestMNIST.py shradha-gp-engine-unoselab01-pod:/data/RandomForestMNIST.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from torchvision.datasets import MNIST\n",
      "from skimage.feature import hog\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn import metrics\n",
      "from tqdm import tqdm\n",
      "import numpy as np\n",
      "import os\n",
      "\n",
      "NUM_TREES = int(os.environ.get(\"SK_NUM_TREES\", \"3\"))\n",
      "NUM_JOBS = int(os.environ.get(\"SK_NUM_JOBS\", \"1\"))\n",
      "\n",
      "print(f\"Running random forest with {NUM_TREES} trees and {NUM_JOBS} jobs\")\n",
      "\n",
      "######\n",
      "# Download MNIST\n",
      "######\n",
      "train_dataset = MNIST(download=True, root=\"~/data\", train=True)\n",
      "test_dataset = MNIST(download=True, root=\"~/data\", train=False)\n",
      "\n",
      "##### \n",
      "# Generate Train Features\n",
      "#####\n",
      "print(\"Generating Train Features\")\n",
      "train_features = np.empty((len(train_dataset), 108))\n",
      "train_labels = np.empty(len(train_dataset), np.int32)\n",
      "for i, (img, label) in tqdm(enumerate(train_dataset), ncols=80, total=len(train_dataset)):\n",
      "    train_features[i] = hog(np.asarray(img), orientations=12, cells_per_block=(3,3))\n",
      "    train_labels[i] = label\n",
      "\n",
      "#####\n",
      "# Generate Test Features\n",
      "#####\n",
      "print(\"Generating Test Features\")\n",
      "test_features = np.empty((len(test_dataset), 108))\n",
      "test_labels = np.empty(len(test_dataset), np.int32)\n",
      "for i, (img, label) in tqdm(enumerate(test_dataset), ncols=80, total=len(test_dataset)):\n",
      "    test_features[i] = hog(np.asarray(img), orientations=12, cells_per_block=(3,3))\n",
      "    test_labels[i] = label\n",
      "\n",
      "######\n",
      "# Train Model\n",
      "#######\n",
      "print(\"Training the model\")\n",
      "model = RandomForestClassifier(n_estimators=NUM_TREES, n_jobs=NUM_JOBS, verbose=1)\n",
      "model.fit(train_features, train_labels)\n",
      "\n",
      "####\n",
      "# Score Model\n",
      "#####\n",
      "print(\"Evaluating the model\")\n",
      "model_accuracy = model.score(test_features, test_labels)\n",
      "print(f\"Model Accuracy = {model_accuracy*100:.2f}%\")"
     ]
    }
   ],
   "source": [
    "! kubectl exec shradha-gp-engine-unoselab01-pod -- cat /data/RandomForestMNIST.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-kube-pvc-create-sklean.yml  3-kube-job-sklean.yml  RandomForestMNIST.py\n",
      "1-kube-pvc-sklean.yml\t      config\n",
      "2-kube-pod-sklean.yml\t      learn.ipynb\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Template\n",
    "\n",
    "# read in the template\n",
    "with open('/home/user1-selab3/Documents/research-shradha/kube/kube2/example1-kube/CODE-RajulShakywar/CODE/example/yaml/yaml_templates/sklearn_job_template.yml') as file_:\n",
    "    template = Template(file_.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: batch/v1\n",
      "kind: Job\n",
      "metadata:\n",
      "  name: job01-sklearn-shradha-gp-engine-unoselab01\n",
      "spec:\n",
      "  template:\n",
      "    spec:\n",
      "      automountServiceAccountToken: false\n",
      "      affinity:\n",
      "        nodeAffinity:\n",
      "          requiredDuringSchedulingIgnoredDuringExecution:\n",
      "            nodeSelectorTerms:\n",
      "            - matchExpressions:\n",
      "              - key: topology.kubernetes.io/region\n",
      "                operator: In\n",
      "                values:\n",
      "                - us-central \n",
      "      containers:\n",
      "      - name: sklearn-train-container\n",
      "        image: gitlab-registry.nrp-nautilus.io/gp-engine/jupyter-stacks/bigdata-2023:latest\n",
      "        workingDir: /data\n",
      "        env:\n",
      "            - name: SK_NUM_TREES\n",
      "              value: \"1\"\n",
      "            - name: SK_NUM_JOBS\n",
      "              value: \"1\"\n",
      "        command: [\"python3\", \"/data/RandomForestMNIST.py\"]\n",
      "        volumeMounts:\n",
      "            - name: pvc-shradha-gp-engine-unoselab01\n",
      "              mountPath: /data\n",
      "        resources:\n",
      "            limits:\n",
      "              memory: 1Gi\n",
      "              cpu: \"1\"\n",
      "            requests:\n",
      "              memory: 1Gi\n",
      "              cpu: \"1\"    \n",
      "      volumes:\n",
      "      - name: pvc-shradha-gp-engine-unoselab01\n",
      "        persistentVolumeClaim:\n",
      "            claimName: pvc-shradha-gp-engine-unoselab01\n",
      "      restartPolicy: Never      \n",
      "  backoffLimit: 1\n"
     ]
    }
   ],
   "source": [
    "# render the job spec\n",
    "job_spec = template.render(\n",
    "    job_name=\"job01-sklearn-shradha-gp-engine-unoselab01\",\n",
    "    pvc_name=\"pvc-shradha-gp-engine-unoselab01\",\n",
    "    num_trees=1,\n",
    "    num_jobs=1\n",
    ")\n",
    "\n",
    "# print the job spec\n",
    "print(job_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/user1-selab3/Documents/research-shradha/kube/kube1-sklearn/3-kube-job-sklean.yml\", \"w\") as file:\n",
    "    file.write(job_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job.batch/job01-sklearn-shradha-gp-engine-unoselab01 created\n"
     ]
    }
   ],
   "source": [
    "! kubectl create -f ../kube1-sklearn/3-kube-job-sklean.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                               READY   STATUS              RESTARTS   AGE\n",
      "gp-engine-unoselab01-pod1                          0/1     Error               0          28h\n",
      "job01-sklearn-shradha-gp-engine-unoselab01-72mr8   0/1     ContainerCreating   0          14s\n",
      "shradha-gp-engine-unoselab01-pod                   1/1     Running             0          16m\n"
     ]
    }
   ],
   "source": [
    "! kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "! kubectl logs --tail=1 job01-sklearn-shradha-gp-engine-unoselab01-72mr8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! kubectl delete job job01-sklearn-shradha-gp-engine-unoselab01-72mr8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! kubectl delete pod sklearn-gp-engine-unoselab01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kubectl get pods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_python3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
