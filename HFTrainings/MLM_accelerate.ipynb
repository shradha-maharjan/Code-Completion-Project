{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from accelerate import Accelerator,notebook_launcher\n",
    "from transformers import get_scheduler\n",
    "from huggingface_hub import Repository, get_full_repo_name\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import default_data_collator\n",
    "import math\n",
    "import time\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1-selab3/miniconda3/envs/myenv_python3_11/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for code_search_net contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/code_search_net\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ds_train = load_dataset(\"code_search_net\", \"java\", split=\"train\")\n",
    "ds_test = load_dataset(\"code_search_net\", \"java\", split=\"test\")\n",
    "ds_valid = load_dataset(\"code_search_net\", \"java\", split=\"validation\")\n",
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": ds_train.shuffle().select(range(4000)), #train_size)), # \"train\": ds_train,  # .shuffle().select(range(50000)),\n",
    "        \"test\": ds_test.shuffle().select(range(500)),\n",
    "        \"valid\": ds_valid.shuffle().select(range(500)) # \"valid\": ds_valid,  # .shuffle().select(range(500))\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public void deleteTableStyle(String featureTable, GeometryType geometryType) {\n",
      "        deleteTableMapping(getTableStyleMappingDao(featureTable), featureTable,\n",
      "                geometryType);\n",
      "    }\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"test\"][0][\"whole_func_string\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY_NAME: avaje-common/avaje-jetty-runner\n",
      "FUNC_PATH_IN_REPOSITORY: src/main/java/org/avaje/jettyrunner/BaseRunner.java\n",
      "FUNC_NAME: BaseRunner.startServer\n",
      "WHOLE_FUNC_STRING: public void startServer() {\n",
      "\n",
      "    server = new Server(httpPort);\n",
      "    server.setHandler(wrapHandlers());\n",
      "\n",
      "    if (isWebSocketInClassPath()) {\n",
      "      setupForWebSocket();\n",
      "    }\n",
      "    try {\n",
      "      server.start();\n",
      "      log().info(\"server started\");\n",
      "\n",
      "      Runtime.getRuntime().addShutdownHook(new Thread(new ShutdownRunnable()));\n",
      "      \n",
      "      if (useStdInShutdown) {\n",
      "        // generally for use in IDE via JettyRun, Use CTRL-D in IDE console to shutdown\n",
      "        BufferedReader systemIn = new BufferedReader(new InputStreamReader(System.in, \"UTF-8\"));\n",
      "        while((systemIn.readLine()) != null) {\n",
      "          // ignore anything except CTRL-D by itself\n",
      "        }\n",
      "        System.out.println(\"Shutdown via CTRL-D\");\n",
      "        System.exit(0);\n",
      "      }\n",
      "\n",
      "    } catch (Exception e) {\n",
      "      e.printStackTrace();\n",
      "      System.exit(100);\n",
      "    }\n",
      "  }\n",
      "LANGUAGE: java\n",
      "FUNC_CODE_STRING: public void startServer() {\n",
      "\n",
      "    server = new Server(httpPort);\n",
      "    server.setHandler(wrapHandlers());\n",
      "\n",
      "    if (isWebSocketInClassPath()) {\n",
      "      setupForWebSocket();\n",
      "    }\n",
      "    try {\n",
      "      server.start();\n",
      "      log().info(\"server started\");\n",
      "\n",
      "      Runtime.getRuntime().addShutdownHook(new Thread(new ShutdownRunnable()));\n",
      "      \n",
      "      if (useStdInShutdown) {\n",
      "        // generally for use in IDE via JettyRun, Use CTRL-D in IDE console to shutdown\n",
      "        BufferedReader systemIn = new BufferedReader(new InputStreamReader(System.in, \"UTF-8\"));\n",
      "        while((systemIn.readLine()) != null) {\n",
      "          // ignore anything except CTRL-D by itself\n",
      "        }\n",
      "        System.out.println(\"Shutdown via CTRL-D\");\n",
      "        System.exit(0);\n",
      "      }\n",
      "\n",
      "    } catch (Exception e) {\n",
      "      e.printStackTrace();\n",
      "      System.exit(100);\n",
      "    }\n",
      "  }\n",
      "FUNC_CODE_TOKENS: ['public', 'void', 'startServer', '(', ')', '{', 'server', '=', 'new', 'Server', '(', 'httpPort', ')', ';', 'server', '.', 'setHandler', '(', 'wrapHandlers', '(', ')', ')', ';', 'if', '(', 'isWebSocketInClassPath', '(', ')', ')', '{', 'setupForWebSocket', '(', ')', ';', '}', 'try', '{', 'server', '.', 'start', '(', ')', ';', 'log', '(', ')', '.', 'info', '(', '\"server started\"', ')', ';', 'Runtime', '.', 'getRuntime', '(', ')', '.', 'addShutdownHook', '(', 'new', 'Thread', '(', 'new', 'ShutdownRunnable', '(', ')', ')', ')', ';', 'if', '(', 'useStdInShutdown', ')', '{', '// generally for use in IDE via JettyRun, Use CTRL-D in IDE console to shutdown', 'BufferedReader', 'systemIn', '=', 'new', 'BufferedReader', '(', 'new', 'InputStreamReader', '(', 'System', '.', 'in', ',', '\"UTF-8\"', ')', ')', ';', 'while', '(', '(', 'systemIn', '.', 'readLine', '(', ')', ')', '!=', 'null', ')', '{', '// ignore anything except CTRL-D by itself', '}', 'System', '.', 'out', '.', 'println', '(', '\"Shutdown via CTRL-D\"', ')', ';', 'System', '.', 'exit', '(', '0', ')', ';', '}', '}', 'catch', '(', 'Exception', 'e', ')', '{', 'e', '.', 'printStackTrace', '(', ')', ';', 'System', '.', 'exit', '(', '100', ')', ';', '}', '}']\n",
      "FUNC_DOCUMENTATION_STRING: Start the Jetty server.\n",
      "FUNC_DOCUMENTATION_TOKENS: ['Start', 'the', 'Jetty', 'server', '.']\n",
      "SPLIT_NAME: train\n",
      "FUNC_CODE_URL: https://github.com/avaje-common/avaje-jetty-runner/blob/acddc23754facc339233fa0b9736e94abc8ae842/src/main/java/org/avaje/jettyrunner/BaseRunner.java#L161-L189\n"
     ]
    }
   ],
   "source": [
    "for key in raw_datasets[\"train\"][0]:\n",
    "    print(f\"{key.upper()}: {raw_datasets['train'][0][key][:1000]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use bert model checkpoint tokenizer\n",
    "model_checkpoint = \"microsoft/codebert-base-mlm\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca84adc87b1435eb75d9bf83af9cf56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (573 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3328303924484d6a9c0055540120acc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dea7892b3384b92b696a4546103ab6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DBG] tokenized_dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 4000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "})\n",
      "[DBG] len(tokenizer): 50265\n",
      "[DBG] tokenizer.bos_token_id: 0\n",
      "[DBG] tokenizer.eos_token_id: 2\n"
     ]
    }
   ],
   "source": [
    "#define tokenize function to tokenize the dataset\n",
    "def tokenize_function(data):\n",
    "    result = tokenizer(data[\"whole_func_string\"])\n",
    "    return result\n",
    "\n",
    "# batched is set to True to activate fast multithreading!\n",
    "tokenize_dataset = raw_datasets.map(tokenize_function, batched = True, remove_columns = raw_datasets[\"train\"].column_names)\n",
    "\n",
    "print(f'[DBG] tokenized_dataset: {tokenize_dataset}')\n",
    "print(f'[DBG] len(tokenizer): {len(tokenizer)}')\n",
    "print(f'[DBG] tokenizer.bos_token_id: {tokenizer.bos_token_id}')\n",
    "print(f'[DBG] tokenizer.eos_token_id: {tokenizer.eos_token_id}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "333bee2ee03049548736366778c1cbf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b659b08a6f9f4d7aa0092f0fbf24bc34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd69f10e102b4b44bcb482c7495909b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def concat_chunk_dataset(data):\n",
    "    chunk_size = 128\n",
    "    # concatenate texts\n",
    "    concatenated_sequences = {k: sum(data[k], []) for k in data.keys()}\n",
    "    #compute length of concatenated texts\n",
    "    total_concat_length = len(concatenated_sequences[list(data.keys())[0]])\n",
    "\n",
    "    # drop the last chunk if is smaller than the chunk size\n",
    "    total_length = (total_concat_length // chunk_size) * chunk_size\n",
    "\n",
    "    # split the concatenated sentences into chunks using the total length\n",
    "    result = {k: [t[i: i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, t in concatenated_sequences.items()}\n",
    "\n",
    "    '''we create a new labels column which is a copy of the input_ids of the processed text data,the labels column serve as \n",
    "    ground truth for our masked language model to learn from. '''\n",
    "    \n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "processed_dataset = tokenize_dataset.map(concat_chunk_dataset, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b8fae6d92b4e958b4412749bfdb5c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1212 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "''' Apply random masking once on the whole test data, then uses the default data collector to handle the test dataset in batches '''\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm_probability = 0.15)\n",
    "\n",
    "# Function to insert random mask\n",
    "def insert_random_mask(batch):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = data_collator(features)\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}\n",
    "\n",
    "# Map insert_random_mask function to test dataset\n",
    "eval_dataset = processed_dataset[\"test\"].map(insert_random_mask,batched=True,remove_columns=processed_dataset[\"test\"].column_names\n",
    ")\n",
    "\n",
    "# Rename columns\n",
    "eval_dataset = eval_dataset.rename_columns({\n",
    "    \"masked_input_ids\": \"input_ids\",\n",
    "    \"masked_attention_mask\": \"attention_mask\",\n",
    "    \"masked_labels\": \"labels\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Disable tokenizers parallelism\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'microsoft/codebert-base-mlm'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/codebert-base-mlm were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at microsoft/codebert-base-mlm were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      " 20%|██        | 149/745 [00:49<03:16,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 0: Loss: 0.1353398859500885>>> Epoch 0: Loss: 0.1353398859500885\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213963/2119783122.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  perplexity = torch.exp(torch.tensor(loss))\n",
      "/tmp/ipykernel_213963/2119783122.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  perplexity = torch.exp(torch.tensor(loss))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 0: Perplexity: 1.144925832748413>>> Epoch 0: Perplexity: 1.144925832748413\n",
      "\n",
      ">>> Epoch 0: Entropy: 7.102756023406982>>> Epoch 0: Entropy: 7.102756023406982\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 298/745 [01:43<02:30,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 1: Loss: 0.1361062228679657>>> Epoch 1: Loss: 0.1361062228679657\n",
      "\n",
      ">>> Epoch 1: Perplexity: 1.145803689956665>>> Epoch 1: Perplexity: 1.145803689956665\n",
      "\n",
      ">>> Epoch 1: Entropy: 7.102712631225586>>> Epoch 1: Entropy: 7.102712631225586\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 447/745 [02:37<01:41,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 2: Loss: 0.12380828708410263>>> Epoch 2: Loss: 0.12380828708410263\n",
      "\n",
      ">>> Epoch 2: Perplexity: 1.1317988634109497>>> Epoch 2: Perplexity: 1.1317988634109497\n",
      "\n",
      ">>> Epoch 2: Entropy: 7.102789878845215>>> Epoch 2: Entropy: 7.102789878845215\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 596/745 [03:32<00:50,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 3: Loss: 0.1227213442325592>>> Epoch 3: Loss: 0.1227213442325592\n",
      "\n",
      ">>> Epoch 3: Perplexity: 1.130569338798523>>> Epoch 3: Perplexity: 1.130569338798523\n",
      "\n",
      ">>> Epoch 3: Entropy: 7.102800369262695>>> Epoch 3: Entropy: 7.102800369262695\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 745/745 [04:27<00:00,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 4: Loss: 0.1270119547843933>>> Epoch 4: Loss: 0.1270119547843933\n",
      "\n",
      ">>> Epoch 4: Perplexity: 1.1354305744171143\n",
      ">>> Epoch 4: Perplexity: 1.1354305744171143>>> Epoch 4: Entropy: 7.102749347686768\n",
      "\n",
      ">>> Epoch 4: Entropy: 7.102749347686768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 745/745 [04:30<00:00,  2.75it/s]\n",
      "100%|██████████| 745/745 [04:30<00:00,  2.75it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def training_function():\n",
    "\n",
    "    # set batch size to 32, a larger bacth size when using a more powerful gpu\n",
    "    batch_size = 32\n",
    "\n",
    "    train_dataloader = DataLoader(processed_dataset[\"train\"], shuffle=True, batch_size=batch_size, collate_fn=data_collator)\n",
    "    eval_dataloader = DataLoader(processed_dataset[\"test\"], batch_size=batch_size, collate_fn=default_data_collator)\n",
    "\n",
    "    # initialize pretrained bert model\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "    # set the optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "    # initialize accelerator for training\n",
    "    accelerator = Accelerator()\n",
    "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)\n",
    "\n",
    "    # set the number of epochs which is set to 30\n",
    "    num_train_epochs = 5\n",
    "    num_update_steps_per_epoch = len(train_dataloader)\n",
    "    num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "    # define the learning rate scheduler for training\n",
    "    lr_scheduler = get_scheduler(\"linear\",optimizer=optimizer,num_warmup_steps=0,num_training_steps=num_training_steps)\n",
    "\n",
    "\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    # directory to save the models\n",
    "    output_dir = \"MLP_TrainedModels\"\n",
    "\n",
    "    for epoch in range(num_train_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        losses = []\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "            loss = outputs.loss # <===== Added.\n",
    "            losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
    "            # loss = outputs.loss\n",
    "            # losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
    "\n",
    "        # losses = torch.cat(losses)\n",
    "        # losses = losses[: len(eval_dataset)]\n",
    "        loss = torch.mean(torch.cat(losses))\n",
    "        print(f\">>> Epoch {epoch}: Loss: {loss.item()}\")\n",
    "\n",
    "        # perplexity metric used for mask language model training\n",
    "        try:\n",
    "            #perplexity = math.exp(torch.mean(losses))\n",
    "            perplexity = torch.exp(torch.tensor(loss))\n",
    "        except OverflowError:\n",
    "            perplexity = float(\"inf\")\n",
    "        print(f\">>> Epoch {epoch}: Perplexity: {perplexity.item()}\")\n",
    "\n",
    "        # Calculate probabilities\n",
    "        losses_tensor = torch.cat(losses)  # Concatenate the list of tensors into a single tensor\n",
    "        # losses_np = losses_tensor.cpu().numpy()  # Convert concatenated tensor to NumPy array\n",
    "        # probabilities = torch.nn.functional.softmax(torch.tensor(losses_np), dim=0)  # Calculate probabilities\n",
    "        probabilities = torch.nn.functional.softmax(-losses_tensor, dim=0)  # Taking negative of losses_tensor to ensure proper softmax calculation\n",
    "\n",
    "        # Calculate entropy\n",
    "        #entropy = -torch.sum(probabilities * torch.log(probabilities))\n",
    "        entropy = -torch.sum(probabilities * torch.log(probabilities + 1e-20)) \n",
    "        print(f\">>> Epoch {epoch}: Entropy: {entropy.item()}\")  # Print entropy\n",
    "\n",
    "        # Save model\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "        if accelerator.is_main_process:\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "notebook_launcher(training_function, num_processes= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.9685537219047546, 'token': 1045, 'token_str': ' create', 'sequence': 'public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return create(node, source, false);\\n    }'}, {'score': 0.005051335785537958, 'token': 10516, 'token_str': ' evaluate', 'sequence': 'public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return evaluate(node, source, false);\\n    }'}, {'score': 0.0024361107498407364, 'token': 146, 'token_str': ' make', 'sequence': 'public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return make(node, source, false);\\n    }'}, {'score': 0.0014410096919164062, 'token': 609, 'token_str': ' process', 'sequence': 'public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return process(node, source, false);\\n    }'}, {'score': 0.0009554359130561352, 'token': 1119, 'token_str': ' build', 'sequence': 'public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return build(node, source, false);\\n    }'}]\n"
     ]
    }
   ],
   "source": [
    "model = \"MLP_TrainedModels\"\n",
    "\n",
    "pred_model = pipeline(\"fill-mask\", model = \"MLP_TrainedModels\")\n",
    "\n",
    "text = \"public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return <mask>(node, source, false);\\n    }\"\n",
    "\n",
    "preds = pred_model(text)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.9833874702453613, 'token': 5124, 'token_str': ' register', 'sequence': 'public FileWatcher register(final Path path, final Class<? extends FileEventHandler> handler) {\\n    return register(path, handler, EMPTY);\\n  }'}, {'score': 0.004609870258718729, 'token': 1606, 'token_str': ' add', 'sequence': 'public FileWatcher register(final Path path, final Class<? extends FileEventHandler> handler) {\\n    return add(path, handler, EMPTY);\\n  }'}, {'score': 0.0015960102900862694, 'token': 1045, 'token_str': ' create', 'sequence': 'public FileWatcher register(final Path path, final Class<? extends FileEventHandler> handler) {\\n    return create(path, handler, EMPTY);\\n  }'}, {'score': 0.0011484508868306875, 'token': 23379, 'token_str': ' bind', 'sequence': 'public FileWatcher register(final Path path, final Class<? extends FileEventHandler> handler) {\\n    return bind(path, handler, EMPTY);\\n  }'}, {'score': 0.0005712391575798392, 'token': 29662, 'token_str': 'register', 'sequence': 'public FileWatcher register(final Path path, final Class<? extends FileEventHandler> handler) {\\n    returnregister(path, handler, EMPTY);\\n  }'}]\n",
      " register\n",
      "Mean Reciprocal Rank (MRR): 1.0\n"
     ]
    }
   ],
   "source": [
    "model = \"MLP_TrainedModels\"\n",
    "pred_model = pipeline(\"fill-mask\", model=model)\n",
    "text = \"public FileWatcher register(final Path path, final Class<? extends FileEventHandler> handler) {\\n    return <mask>(path, handler, EMPTY);\\n  }\"\n",
    "\n",
    "# Get predictions\n",
    "preds = pred_model(text)\n",
    "print(preds)\n",
    "\n",
    "# Correct answer (assuming the correct answer is the first mask prediction)\n",
    "correct_answer = preds[0]['token_str']\n",
    "print(correct_answer)\n",
    "\n",
    "# Compute the reciprocal ranks\n",
    "ranks = [i + 1 for i, pred in enumerate(preds) if pred['token_str'] == correct_answer]  # Ranks start from 1\n",
    "reciprocal_ranks = [1 / rank for rank in ranks]  # Compute reciprocals\n",
    "\n",
    "# Calculate Mean Reciprocal Rank\n",
    "mrr = sum(reciprocal_ranks) / len(reciprocal_ranks)\n",
    "\n",
    "print(\"Mean Reciprocal Rank (MRR):\", mrr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.9685537219047546, 'token': 1045, 'token_str': ' create', 'sequence': 'public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return create(node, source, false);\\n    }'}, {'score': 0.005051335785537958, 'token': 10516, 'token_str': ' evaluate', 'sequence': 'public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return evaluate(node, source, false);\\n    }'}, {'score': 0.0024361107498407364, 'token': 146, 'token_str': ' make', 'sequence': 'public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return make(node, source, false);\\n    }'}, {'score': 0.0014410096919164062, 'token': 609, 'token_str': ' process', 'sequence': 'public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return process(node, source, false);\\n    }'}, {'score': 0.0009554359130561352, 'token': 1119, 'token_str': ' build', 'sequence': 'public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return build(node, source, false);\\n    }'}]\n",
      "[{'score': 0.9685537219047546, 'token': 1045, 'token_str': ' create', 'sequence': 'public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return create(node, source, false);\\n    }'}, {'score': 0.005051335785537958, 'token': 10516, 'token_str': ' evaluate', 'sequence': 'public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return evaluate(node, source, false);\\n    }'}, {'score': 0.0024361107498407364, 'token': 146, 'token_str': ' make', 'sequence': 'public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return make(node, source, false);\\n    }'}, {'score': 0.0014410096919164062, 'token': 609, 'token_str': ' process', 'sequence': 'public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return process(node, source, false);\\n    }'}, {'score': 0.0009554359130561352, 'token': 1119, 'token_str': ' build', 'sequence': 'public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return build(node, source, false);\\n    }'}]\n",
      " create\n",
      "1\n",
      "[1.0, 0.5, 0.3333333333333333, 0.25, 0.2]\n",
      "Mean Reciprocal Rank (MRR): 0.45666666666666667\n"
     ]
    }
   ],
   "source": [
    "model = \"MLP_TrainedModels\"\n",
    "pred_model = pipeline(\"fill-mask\", model=model)\n",
    "text = \"public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return <mask>(node, source, false);\\n    }\"\n",
    "\n",
    "# Get predictions\n",
    "preds = pred_model(text)\n",
    "print(preds)\n",
    "\n",
    "# Sort predictions by score in descending order\n",
    "sorted_preds = sorted(preds, key=lambda x: x['score'], reverse=True)\n",
    "print(sorted_preds)\n",
    "# Determine the rank of the correct answer\n",
    "correct_answer = preds[0]['token_str']\n",
    "print(correct_answer)\n",
    "correct_rank = next(i+1 for i, pred in enumerate(sorted_preds) if pred['token_str'] == correct_answer)\n",
    "print(correct_rank)\n",
    "\n",
    "# Compute the reciprocal ranks\n",
    "reciprocal_ranks = [1 / rank for rank in range(1, len(sorted_preds) + 1)]\n",
    "print(reciprocal_ranks)\n",
    "\n",
    "# Calculate Mean Reciprocal Rank\n",
    "mrr = sum(reciprocal_ranks) / len(reciprocal_ranks)\n",
    "\n",
    "print(\"Mean Reciprocal Rank (MRR):\", mrr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reciprocal Rank (MRR): 0.45666666666666667\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get the correct answer (assuming it's the first mask prediction)\n",
    "correct_answer = preds[0]['token_str']\n",
    "\n",
    "# Create a DataFrame from the predictions\n",
    "df = pd.DataFrame(preds)\n",
    "\n",
    "# Sort the DataFrame by score in descending order\n",
    "df_sorted = df.sort_values(by='score', ascending=False)\n",
    "\n",
    "# Reset the index of the sorted DataFrame\n",
    "df_sorted.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Determine the rank of the correct answer\n",
    "correct_rank = df_sorted.index[df_sorted['token_str'] == correct_answer].tolist()[0] + 1  # Add 1 to start ranks from 1\n",
    "\n",
    "# Calculate the reciprocal ranks\n",
    "df_sorted['rank'] = df_sorted.index + 1\n",
    "df_sorted['reciprocal_rank'] = 1 / df_sorted['rank']\n",
    "\n",
    "# Calculate Mean Reciprocal Rank (MRR)\n",
    "mrr = df_sorted['reciprocal_rank'].mean()\n",
    "\n",
    "print(\"Mean Reciprocal Rank (MRR):\", mrr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_python3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
