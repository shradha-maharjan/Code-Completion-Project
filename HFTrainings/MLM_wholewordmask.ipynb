{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from accelerate import Accelerator,notebook_launcher\n",
    "from transformers import get_scheduler\n",
    "from huggingface_hub import Repository, get_full_repo_name\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import default_data_collator\n",
    "import math\n",
    "import time\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1-selab3/miniconda3/envs/myenv_python3_11/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for code_search_net contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/code_search_net\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "        num_rows: 454451\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "        num_rows: 26909\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "        num_rows: 15328\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codesearchnet_dataset = load_dataset(\"code_search_net\", \"java\")\n",
    "codesearchnet_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_train = load_dataset(\"code_search_net\", \"java\", split=\"train\")\n",
    "# ds_test = load_dataset(\"code_search_net\", \"java\", split=\"test\")\n",
    "# ds_valid = load_dataset(\"code_search_net\", \"java\", split=\"validation\")\n",
    "# raw_datasets = DatasetDict(\n",
    "#     {\n",
    "#         \"train\": ds_train.shuffle().select(range(12000)), #train_size)), # \"train\": ds_train,  # .shuffle().select(range(50000)),\n",
    "#         \"test\": ds_test.shuffle().select(range(1500)),\n",
    "#         \"valid\": ds_valid.shuffle().select(range(1500)) # \"valid\": ds_valid,  # .shuffle().select(range(500))\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> code: public Boolean isWriteLocked(K token) {\n",
      "\tRWLock<K> lock = locks.get(token);\n",
      "\tif (lock == null) return null;\n",
      "\treturn lock.isWriteLocked();\n",
      "    }'\n",
      "\n",
      "'>>> code: @Override\n",
      "    public int getLevel() {\n",
      "        Level level = log4jLogger.getLevel();\n",
      "        if (level == null)\n",
      "            level = Logger.getRootLogger().getLevel();\n",
      "        switch (level.toInt()) {\n",
      "            case Level.TRACE_INT:\n",
      "                return TRACE;\n",
      "            case Level.DEBUG_INT:\n",
      "                return DEBUG;\n",
      "            case Level.INFO_INT:\n",
      "                return INFO;\n",
      "            case Level.WARN_INT:\n",
      "                return WARN;\n",
      "            case Level.ERROR_INT:\n",
      "                return ERROR;\n",
      "            case Level.FATAL_INT:\n",
      "                return FATAL;\n",
      "            default:\n",
      "                throw new IllegalArgumentException(\"Unsupported log4j level: \" + level);\n",
      "        }\n",
      "    }'\n",
      "\n",
      "'>>> code: public TerminalEmulatorDeviceConfiguration withCursorBlinking(boolean cursorBlinking) {\n",
      "        if(this.cursorBlinking == cursorBlinking) {\n",
      "            return this;\n",
      "        } else {\n",
      "            return new TerminalEmulatorDeviceConfiguration(\n",
      "                    this.lineBufferScrollbackSize,\n",
      "                    this.blinkLengthInMilliSeconds,\n",
      "                    this.cursorStyle,\n",
      "                    this.cursorColor,\n",
      "                    cursorBlinking,\n",
      "                    this.clipboardAvailable);\n",
      "        }\n",
      "    }'\n"
     ]
    }
   ],
   "source": [
    "sample = codesearchnet_dataset[\"train\"].shuffle(seed=42).select(range(3))\n",
    "\n",
    "for row in sample:\n",
    "    print(f\"\\n'>>> code: {row['whole_func_string']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"microsoft/codebert-base-mlm\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "        num_rows: 454451\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "        num_rows: 26909\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "        num_rows: 15328\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codesearchnet_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9077df204c4f51b5eb451dd5b36d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26909 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (955 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 454451\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 26909\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 15328\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"whole_func_string\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = codesearchnet_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url']\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #define tokenize function to tokenize the dataset\n",
    "# def tokenize_function(data):\n",
    "#     result = tokenizer(data[\"whole_func_string\"])\n",
    "#     if tokenizer.is_fast:\n",
    "#         result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "#     return result\n",
    "\n",
    "# # batched is set to True to activate fast multithreading!\n",
    "# tokenize_dataset = raw_datasets.map(tokenize_function, batched = True, remove_columns = raw_datasets[\"train\"].column_names)\n",
    "# tokenize_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> code 0 length: 65'\n",
      "'>>> code 1 length: 84'\n",
      "'>>> code 2 length: 168'\n"
     ]
    }
   ],
   "source": [
    "tokenized_samples = tokenized_datasets[\"train\"][:3]\n",
    "\n",
    "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\"'>>> code {idx} length: {len(sample)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Concatenated code length: 317'\n"
     ]
    }
   ],
   "source": [
    "concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "total_length = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\"'>>> Concatenated code length: {total_length}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 61'\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 128\n",
    "chunks = {\n",
    "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, t in concatenated_examples.items()\n",
    "}\n",
    "\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3503673e4da64ba3b1f677cb919b6a36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26909 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 1132440\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 61821\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 31402\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Name, resourceGroupName, fabricName, containerName), serviceCallback);\\n    }</s><s>public Observable<Void> inquireAsync(String vaultName, String resourceGroupName, String fabricName, String containerName, String filter) {\\n        return inquireWithServiceResponseAsync(vaultName, resourceGroupName, fabricName, containerName, filter).map(new Func1<ServiceResponse<Void>, Void>() {\\n            @Override\\n            public'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Name, resourceGroupName, fabricName, containerName), serviceCallback);\\n    }</s><s>public Observable<Void> inquireAsync(String vaultName, String resourceGroupName, String fabricName, String containerName, String filter) {\\n        return inquireWithServiceResponseAsync(vaultName, resourceGroupName, fabricName, containerName, filter).map(new Func1<ServiceResponse<Void>, Void>() {\\n            @Override\\n            public'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][1][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> <s>public void inquire(String vaultName, String resourceGroupName<mask> String fabricName<mask> String containerName) {\n",
      " <mask>   <mask>  inquireWithServiceResponseAsync(vaultName, resourceGroupName, fabricName, containerName).to<mask>ocking().single().body();\n",
      " <mask>  }</s><s><mask> ServiceFuture<mask><mask>oid<mask> inquireAsync(String vaultName, String resourceGroupName, String fabric<mask>, String containerName, final ServiceCallback<V<mask>> serviceCallback) {\n",
      "<mask>  <mask>    return ServiceFuture.fromResponse(inquireWithServiceResponseAsync(vault'\n",
      "\n",
      "'>>> Name, resourceGroupName<mask> fabricName, containerName),<mask>Callback);73    }</s><s>public Observable<mask> Rankingoid> inquire<mask>(<mask> vault<mask>, String resourceGroupName, String fabric<mask>, String containerName<mask> String filter) {\n",
      " <mask>      return inquireWithServiceResponseAsync(v congenName, resourceGroupName, fabricName, containerName, filter).<mask>(new Func1<ServiceResponse<V<mask><mask> Void>() {\n",
      " Gö   <mask>      Cardinals @<mask>\n",
      "  <mask>         public'\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from transformers import default_data_collator\n",
    "\n",
    "wwm_probability = 0.2\n",
    "\n",
    "\n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "        feature[\"labels\"] = new_labels\n",
    "\n",
    "    return default_data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> <s><mask><mask> inquire(String vaultName, String resourceGroupName, String<mask><mask>, String containerName)<mask>\n",
      "        inquireWithServiceResponseAsync<mask>vaultName,<mask><mask><mask><mask> fabricName, containerName).toBlocking().single<mask><mask>();\n",
      "    }</s><s><mask> ServiceFuture<Void<mask> inquireAsync<mask>String vaultName, String resourceGroupName<mask> String fabricName, String containerName<mask> final ServiceCallback<Void<mask> serviceCallback) {\n",
      "       <mask> ServiceFuture.fromResponse(inquireWithServiceResponseAsync(vault'\n",
      "\n",
      "'>>> Name<mask> resourceGroupName<mask> fabricName, containerName), serviceCallback);\n",
      "    }</s><s><mask> Observable<Void<mask> inquireAsync(String<mask><mask>, String resourceGroupName<mask> String fabricName, String containerName<mask> String filter<mask><mask>\n",
      "        return inquireWithServiceResponseAsync<mask>vaultName, resourceGroupName, fabricName,<mask><mask>,<mask>).map<mask>new<mask><mask><mask><ServiceResponse<Void>,<mask>>() {\n",
      "            @Override\n",
      "           <mask>'\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "batch = whole_word_masking_data_collator(samples)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_size = 5000\n",
    "# test_size = int(0.1 * train_size)\n",
    "# valid_size = int(0.1 * train_size)\n",
    "\n",
    "# downsampled_dataset = lm_datasets[\"train\"].train_test_split(\n",
    "#     train_size=train_size, test_size=test_size, seed=42\n",
    "# )\n",
    "# downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f7b882dc3d41de9a7daa15ed7bde72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 4000\n",
      "Validation dataset size: 500\n",
      "Test dataset size: 500\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_size = 4000\n",
    "valid_size = 500\n",
    "test_size = 500\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_dataset = lm_datasets[\"train\"].shuffle(seed=42).select(range(train_size))\n",
    "remaining_dataset = lm_datasets[\"train\"].shuffle(seed=42).select(range(train_size, len(lm_datasets[\"train\"])))\n",
    "valid_dataset = remaining_dataset.shuffle(seed=42).select(range(valid_size))\n",
    "test_dataset = remaining_dataset.shuffle(seed=42).select(range(valid_size, valid_size + test_size))\n",
    "\n",
    "# Print sizes of each split\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(valid_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(train_dataset) // batch_size\n",
    "# print(train_dataset[\"train\"])\n",
    "# print(len(train_dataset[\"train\"]))\n",
    "# print(train_dataset)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"MLM_FinetunedModel\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=True,\n",
    "    fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/codebert-base-mlm were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
      "    num_rows: 4000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
      "    num_rows: 500\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model= AutoModelForMaskedLM.from_pretrained(model_checkpoint),\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print (train_dataset)\n",
    "print (test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'MLM_wholewordmask.ipynb'\n",
    "os.environ['WANDB_MODE'] = 'disabled'\n",
    "#  WANDB_MODE=disabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1-selab3/miniconda3/envs/myenv_python3_11/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 3.50\n",
      ">>> Loss: 1.25\n"
     ]
    }
   ],
   "source": [
    "# import math\n",
    "\n",
    "# eval_results = trainer.evaluate()\n",
    "# print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "import math\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# Calculate perplexity\n",
    "perplexity = math.exp(eval_results['eval_loss'])\n",
    "\n",
    "\n",
    "# Calculate loss\n",
    "loss = eval_results['eval_loss']\n",
    "\n",
    "print(f\">>> Perplexity: {perplexity:.2f}\")\n",
    "print(f\">>> Loss: {loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 01:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.553422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.677300</td>\n",
       "      <td>0.514992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.677300</td>\n",
       "      <td>0.512817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=96, training_loss=0.6415141224861145, metrics={'train_runtime': 80.5957, 'train_samples_per_second': 148.891, 'train_steps_per_second': 1.191, 'total_flos': 789796389888000.0, 'train_loss': 0.6415141224861145, 'epoch': 3.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 1.63\n",
      ">>> Loss: 0.49\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# Calculate perplexity\n",
    "perplexity = math.exp(eval_results['eval_loss'])\n",
    "\n",
    "# Calculate loss\n",
    "loss = eval_results['eval_loss']\n",
    "\n",
    "print(f\">>> Perplexity: {perplexity:.2f}\")\n",
    "print(f\">>> Loss: {loss:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.9904773235321045, 'token': 1045, 'token_str': ' create', 'sequence': 'public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return create(node, source, false);\\n    }'}, {'score': 0.0020682169124484062, 'token': 10516, 'token_str': ' evaluate', 'sequence': 'public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return evaluate(node, source, false);\\n    }'}, {'score': 0.0005082740099169314, 'token': 146, 'token_str': ' make', 'sequence': 'public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return make(node, source, false);\\n    }'}, {'score': 0.00044398324098438025, 'token': 1119, 'token_str': ' build', 'sequence': 'public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return build(node, source, false);\\n    }'}, {'score': 0.00033154641278088093, 'token': 120, 'token_str': ' get', 'sequence': 'public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return get(node, source, false);\\n    }'}]\n"
     ]
    }
   ],
   "source": [
    "model = \"MLM_FinetunedModel\"\n",
    "\n",
    "pred_model = pipeline(\"fill-mask\", model = \"MLM_FinetunedModel\")\n",
    "\n",
    "text = \"public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return <mask>(node, source, false);\\n    }\"\n",
    "\n",
    "preds = pred_model(text)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.9904773235321045, 'token': 1045, 'token_str': ' create', 'sequence': 'public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return create(node, source, false);\\n    }'}, {'score': 0.0020682169124484062, 'token': 10516, 'token_str': ' evaluate', 'sequence': 'public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return evaluate(node, source, false);\\n    }'}, {'score': 0.0005082740099169314, 'token': 146, 'token_str': ' make', 'sequence': 'public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return make(node, source, false);\\n    }'}, {'score': 0.00044398324098438025, 'token': 1119, 'token_str': ' build', 'sequence': 'public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return build(node, source, false);\\n    }'}, {'score': 0.00033154641278088093, 'token': 120, 'token_str': ' get', 'sequence': 'public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return get(node, source, false);\\n    }'}]\n",
      "[{'score': 0.9904773235321045, 'token': 1045, 'token_str': ' create', 'sequence': 'public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return create(node, source, false);\\n    }'}, {'score': 0.0020682169124484062, 'token': 10516, 'token_str': ' evaluate', 'sequence': 'public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return evaluate(node, source, false);\\n    }'}, {'score': 0.0005082740099169314, 'token': 146, 'token_str': ' make', 'sequence': 'public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return make(node, source, false);\\n    }'}, {'score': 0.00044398324098438025, 'token': 1119, 'token_str': ' build', 'sequence': 'public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return build(node, source, false);\\n    }'}, {'score': 0.00033154641278088093, 'token': 120, 'token_str': ' get', 'sequence': 'public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return get(node, source, false);\\n    }'}]\n",
      " create\n",
      "1\n",
      "[1.0, 0.5, 0.3333333333333333, 0.25, 0.2]\n",
      "Mean Reciprocal Rank (MRR): 0.45666666666666667\n"
     ]
    }
   ],
   "source": [
    "model = \"MLM_FinetunedModel\"\n",
    "pred_model = pipeline(\"fill-mask\", model=model)\n",
    "text = \"public Evaluation create(SimpleNode node, Object source)\\n    {\\n        return <mask>(node, source, false);\\n    }\"\n",
    "\n",
    "# Get predictions\n",
    "preds = pred_model(text)\n",
    "print(preds)\n",
    "\n",
    "# Sort predictions by score in descending order\n",
    "sorted_preds = sorted(preds, key=lambda x: x['score'], reverse=True)\n",
    "print(sorted_preds)\n",
    "# Determine the rank of the correct answer\n",
    "correct_answer = preds[0]['token_str']\n",
    "print(correct_answer)\n",
    "correct_rank = next(i+1 for i, pred in enumerate(sorted_preds) if pred['token_str'] == correct_answer)\n",
    "print(correct_rank)\n",
    "\n",
    "# Compute the reciprocal ranks\n",
    "reciprocal_ranks = [1 / rank for rank in range(1, len(sorted_preds) + 1)]\n",
    "print(reciprocal_ranks)\n",
    "\n",
    "# Calculate Mean Reciprocal Rank\n",
    "mrr = sum(reciprocal_ranks) / len(reciprocal_ranks)\n",
    "\n",
    "print(\"Mean Reciprocal Rank (MRR):\", mrr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " create\n",
      "      score  token  token_str  \\\n",
      "0  0.948937   1045     create   \n",
      "1  0.018123  10516   evaluate   \n",
      "2  0.002204  37131       eval   \n",
      "3  0.001642   1119      build   \n",
      "4  0.001627    146       make   \n",
      "\n",
      "                                            sequence  \n",
      "0  public Evaluation create(SimpleNode node, Obje...  \n",
      "1  public Evaluation create(SimpleNode node, Obje...  \n",
      "2  public Evaluation create(SimpleNode node, Obje...  \n",
      "3  public Evaluation create(SimpleNode node, Obje...  \n",
      "4  public Evaluation create(SimpleNode node, Obje...  \n",
      "1\n",
      "0    1\n",
      "1    2\n",
      "2    3\n",
      "3    4\n",
      "4    5\n",
      "Name: rank, dtype: int64\n",
      "Mean Reciprocal Rank (MRR): 0.45666666666666667\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get the correct answer (assuming it's the first mask prediction)\n",
    "correct_answer = preds[0]['token_str']\n",
    "print(correct_answer)\n",
    "\n",
    "# Create a DataFrame from the predictions\n",
    "df = pd.DataFrame(preds)\n",
    "\n",
    "# Sort the DataFrame by score in descending order\n",
    "df_sorted = df.sort_values(by='score', ascending=False)\n",
    "print(df_sorted)\n",
    "\n",
    "# Reset the index of the sorted DataFrame\n",
    "df_sorted.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Determine the rank of the correct answer\n",
    "correct_rank = df_sorted.index[df_sorted['token_str'] == correct_answer].tolist()[0] + 1  # Add 1 to start ranks from 1\n",
    "print(correct_rank)\n",
    "\n",
    "# Calculate the reciprocal ranks\n",
    "df_sorted['rank'] = df_sorted.index + 1\n",
    "print(df_sorted['rank'])\n",
    "df_sorted['reciprocal_rank'] = 1 / df_sorted['rank']\n",
    "\n",
    "# Calculate Mean Reciprocal Rank (MRR)\n",
    "mrr = df_sorted['reciprocal_rank'].mean()\n",
    "\n",
    "print(\"Mean Reciprocal Rank (MRR):\", mrr)\n",
    "\n",
    "#The correct answer (\"create\") is at index 0 after sorting. So, its rank is 1.\n",
    "# The ranks would be [1, 2, 3, 4, 5], and the reciprocal ranks would be [1.0, 0.5, 0.333, 0.25, 0.2].\n",
    "# MRR is the mean of the reciprocal ranks, which is (1.0 + 0.5 + 0.333 + 0.25 + 0.2) / 5 = 0.45666666666666667."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      score  token token_str\n",
      "0  0.948937   1045    create\n",
      "1  0.018123  10516  evaluate\n",
      "2  0.002204  37131      eval\n",
      "3  0.001642   1119     build\n",
      "4  0.001627    146      make\n",
      "1\n",
      "Reciprocal Rank (RR): 1.0\n",
      "Mean Reciprocal Rank (MRR): 0.45666666666666667\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample outputs\n",
    "data = {\n",
    "    'score': [0.948937, 0.018123, 0.002204, 0.001642, 0.001627],\n",
    "    'token': [1045, 10516, 37131, 1119, 146],\n",
    "    'token_str': ['create', 'evaluate', 'eval', 'build', 'make'],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Assuming 'create' is the correct token for the query\n",
    "ground_truth = 'create'\n",
    "\n",
    "# Sort the outputs by score\n",
    "sorted_df = df.sort_values(by='score', ascending=False)\n",
    "print(sorted_df)\n",
    "\n",
    "# Find the rank of the correct token\n",
    "rank = sorted_df.index[sorted_df['token_str'] == ground_truth][0] + 1\n",
    "print(rank)\n",
    "\n",
    "# Calculate reciprocal rank\n",
    "RR = 1 / rank\n",
    "\n",
    "print(\"Reciprocal Rank (RR):\", RR)\n",
    "\n",
    "# Calculate Mean Reciprocal Rank (MRR)\n",
    "MRR = df.apply(lambda row: 1 / (sorted_df.index[sorted_df['token_str'] == row['token_str']][0] + 1), axis=1).mean()\n",
    "\n",
    "print(\"Mean Reciprocal Rank (MRR):\", MRR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_python3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
