{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a620b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tree_sitter\n",
    "from tree_sitter import Language, Parser\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81370f40",
   "metadata": {},
   "source": [
    "all the declarations as in enums.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adea49f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG_JAVA = 'java'\n",
    "LANG_PYTHON = 'python'\n",
    "LANG_GO = 'go'\n",
    "LANG_PHP = 'php'\n",
    "LANG_JAVASCRIPT = 'javascript'\n",
    "LANG_RUBY = 'ruby'\n",
    "LANG_C_SHARP = 'c_sharp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3732fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training mode\n",
    "TRAINING_MODE_PRE_TRAIN = 'pre_train'\n",
    "TRAINING_MODE_FINE_TUNE = 'fine_tune'\n",
    "\n",
    "# pre-training task names\n",
    "TASK_CODE_AST_PREDICTION = 'cap'\n",
    "TASK_MASS = 'mass'\n",
    "TASK_METHOD_NAME_PREDICTION = 'mng'\n",
    "\n",
    "PRE_TRAIN_TASKS = [\n",
    "    TASK_CODE_AST_PREDICTION,\n",
    "    TASK_MASS,\n",
    "    TASK_METHOD_NAME_PREDICTION\n",
    "]\n",
    "\n",
    "# downstream task names\n",
    "TASK_SUMMARIZATION = 'summarization'\n",
    "TASK_TRANSLATION = 'translation'\n",
    "TASK_SEARCH = 'search'\n",
    "TASK_CLONE_DETECTION = 'clone'\n",
    "TASK_COMPLETION = 'completion'\n",
    "TASK_BUG_FIX = 'bug_fix'\n",
    "\n",
    "ALL_DOWNSTREAM_TASKS = [\n",
    "    TASK_SUMMARIZATION,\n",
    "    TASK_TRANSLATION,\n",
    "    TASK_SEARCH,\n",
    "    TASK_CLONE_DETECTION,\n",
    "    TASK_COMPLETION,\n",
    "    TASK_BUG_FIX\n",
    "]\n",
    "\n",
    "# programming language\n",
    "LANG_JAVA = 'java'\n",
    "LANG_PYTHON = 'python'\n",
    "LANG_GO = 'go'\n",
    "LANG_PHP = 'php'\n",
    "LANG_JAVASCRIPT = 'javascript'\n",
    "LANG_RUBY = 'ruby'\n",
    "LANG_C_SHARP = 'c_sharp'\n",
    "\n",
    "# BART model mode\n",
    "MODEL_MODE_CLS = 'bart_cls'\n",
    "MODEL_MODE_GEN = 'bart_gen'\n",
    "MODEL_MODE_SEARCH = 'bart_search'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4458fa8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1-selab3/miniconda3/envs/myenv_python3_11/lib/python3.11/site-packages/tree_sitter/__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "LANGUAGE = {LANG_GO: Language('/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/spt-code/sources/data/asts/build/my-languages.so', 'go'),\n",
    "            LANG_JAVASCRIPT: Language('/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/spt-code/sources/data/asts/build/my-languages.so', 'javascript'),\n",
    "            LANG_PYTHON: Language('/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/spt-code/sources/data/asts/build/my-languages.so', 'python'),\n",
    "            LANG_JAVA: Language('/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/spt-code/sources/data/asts/build/my-languages.so', 'java'),\n",
    "            LANG_PHP: Language('/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/spt-code/sources/data/asts/build/my-languages.so', 'php'),\n",
    "            LANG_RUBY: Language('/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/spt-code/sources/data/asts/build/my-languages.so', 'ruby'),\n",
    "            LANG_C_SHARP: Language('/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/spt-code/sources/data/asts/build/my-languages.so', 'c_sharp')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dfa7d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = Parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f101b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It contains language-specific prefix and postfix strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50da385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_PREFIX_POSTFIX = {\n",
    "    LANG_PHP: ['<?php ', ' ?>'],\n",
    "    LANG_JAVA: ['class A{ ', ' }']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f093c0",
   "metadata": {},
   "source": [
    "defining a dictionary in Python named PATTERNS_METHOD_ROOT. It contains language-specific patterns for identifying the root node of a method in a code AST (Abstract Syntax Tree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51b132cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It will search for a method_declaration within a class_body, which in turn is within a class_declaration. @method_root seems to be a placeholder for where the root of the method should be.\n",
    "PATTERNS_METHOD_ROOT = {\n",
    "    LANG_JAVA: \"\"\"\n",
    "    (program\n",
    "        (class_declaration\n",
    "            body: (class_body\n",
    "                (method_declaration) @method_root)\n",
    "        )\n",
    "    )\n",
    "    \"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff433b7",
   "metadata": {},
   "source": [
    "This dictionary contains language-specific patterns for identifying the body of a method/function in different programming languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ea885ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This pattern looks for a method_declaration node with a block as its body, where @body is a placeholder for capturing the body of the method.\n",
    "PATTERNS_METHOD_BODY = {\n",
    "    LANG_JAVA: \"\"\"\n",
    "    (method_declaration\n",
    "        body: (block) @body\n",
    "    )\n",
    "    \"\"\",\n",
    "\n",
    "#This pattern searches for a function_declaration in a JavaScript program, and captures its statement_block as the body of the function.\n",
    "    LANG_JAVASCRIPT: \"\"\"\n",
    "    (program\n",
    "        (function_declaration\n",
    "            body: (statement_block) @body\n",
    "        )\n",
    "    )\n",
    "    \"\"\",\n",
    "#This pattern looks for either a function_declaration or a method_declaration in a Go source file, and captures the block as the body of the function/method.\n",
    "    LANG_GO: \"\"\"\n",
    "    (source_file\n",
    "        [\n",
    "        (function_declaration\n",
    "            body: (block) @body)\n",
    "\n",
    "        (method_declaration\n",
    "            body: (block) @body)\n",
    "        ]\n",
    "    )\n",
    "    \"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638ee25d",
   "metadata": {},
   "source": [
    "it contains language-specific patterns for identifying the names of methods/functions in various programming languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d82bf206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This pattern looks for a method_declaration in Java and captures its name, represented by an identifier.\n",
    "PATTERNS_METHOD_NAME = {\n",
    "    LANG_JAVA: \"\"\"\n",
    "    (method_declaration\n",
    "        name: (identifier) @method_name\n",
    "    )\n",
    "    \"\"\",\n",
    "\n",
    "    LANG_PYTHON: \"\"\"\n",
    "    (module\n",
    "        (function_definition\n",
    "            name: (identifier) @method_name\n",
    "        )\n",
    "    )\n",
    "    \"\"\",\n",
    "\n",
    "    LANG_GO: \"\"\"\n",
    "    [\n",
    "        (source_file\n",
    "            (method_declaration\n",
    "                name: (field_identifier) @method_name\n",
    "            )\n",
    "        )\n",
    "        (source_file\n",
    "            (function_declaration\n",
    "                name: (identifier) @method_name\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "    \"\"\",\n",
    "\n",
    "    LANG_JAVASCRIPT: \"\"\"\n",
    "    (program\n",
    "        (function_declaration\n",
    "            name: (identifier) @method_name\n",
    "        )\n",
    "    )\n",
    "    \"\"\",\n",
    "\n",
    "    LANG_RUBY: \"\"\"\n",
    "    (program\n",
    "        (method\n",
    "            name: (identifier) @method_name\n",
    "        )\n",
    "    )\n",
    "    \"\"\",\n",
    "\n",
    "    LANG_PHP: \"\"\"\n",
    "    (program\n",
    "        (function_definition\n",
    "            name: (name) @method_name\n",
    "        )\n",
    "    )\n",
    "    \"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28ea039",
   "metadata": {},
   "source": [
    "It contains language-specific patterns for identifying method invocations in various programming languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff4dd930",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This pattern looks for a method_invocation in Java and captures its name, represented by an identifier.\n",
    "PATTERNS_METHOD_INVOCATION = {\n",
    "    LANG_JAVA: \"\"\"\n",
    "    (method_invocation\n",
    "        name: (identifier) @method_invocation\n",
    "    )\n",
    "    \"\"\",\n",
    "#It looks for either a function call or a method call on an object (attribute access), capturing the name of the method in both cases.\n",
    "    LANG_PYTHON: \"\"\"\n",
    "    [\n",
    "        (call\n",
    "            function: (identifier) @method_invocation\n",
    "        )\n",
    "        (call\n",
    "            function: (attribute\n",
    "                attribute: (identifier) @method_invocation\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "    \"\"\",\n",
    "\n",
    "    LANG_GO: \"\"\"\n",
    "    [\n",
    "        (call_expression\n",
    "            function: (selector_expression\n",
    "                field: (field_identifier) @method_invocation\n",
    "            )\n",
    "        )\n",
    "        (call_expression\n",
    "            function: (identifier) @method_invocation\n",
    "        )\n",
    "    ]\n",
    "    \"\"\",\n",
    "\n",
    "    LANG_JAVASCRIPT: \"\"\"\n",
    "    [\n",
    "        (call_expression\n",
    "            function: (member_expression\n",
    "                property: (property_identifier) @method_invocation\n",
    "            )\n",
    "        )\n",
    "        (call_expression\n",
    "            function: (identifier) @method_invocation\n",
    "        )\n",
    "    ]\n",
    "    \"\"\",\n",
    "\n",
    "    LANG_RUBY: \"\"\"\n",
    "    (call\n",
    "        method: (identifier) @method_invocation\n",
    "    )\n",
    "    \"\"\",\n",
    "\n",
    "    LANG_PHP: \"\"\"\n",
    "    [\n",
    "        (scoped_call_expression\n",
    "            name: (name) @method_invocation\n",
    "        )\n",
    "        (function_call_expression\n",
    "            (name) @method_invocation\n",
    "        )\n",
    "        (member_call_expression\n",
    "            name: (name) @method_invocation\n",
    "        )\n",
    "        (object_creation_expression\n",
    "            (qualified_name\n",
    "                (name) @method_invocation\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "    \"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc22ad3",
   "metadata": {},
   "source": [
    "The STATEMENT_ENDING_STRINGS dictionary defines common types of statement endings for different programming languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5af676e3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "STATEMENT_ENDING_STRINGS = {\n",
    "    LANG_JAVA: ['statement', 'expression', 'declaration'],\n",
    "    LANG_PYTHON: ['statement', 'assignment'],\n",
    "    LANG_GO: ['statement', 'declaration', 'expression'],\n",
    "    LANG_JAVASCRIPT: ['statement', 'expression'],\n",
    "    LANG_RUBY: ['call', 'assignment', 'if', 'unless_modifier', 'operator_assignment', 'if_modifier', 'return',\n",
    "                'rescue', 'else', 'unless', 'when', 'for', 'while_modifier', 'until'],\n",
    "    LANG_PHP: ['statement', 'expression']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107ce0bd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "To split a camel case identifier into its constituent parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9aa9e5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def camel_split(identifier):\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return [m.group(0) for m in matches]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92976ee",
   "metadata": {},
   "source": [
    "This function takes an identifier string and splits it into a list of subtokens, eliminating tokens that are not characters or digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9fe0d00",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def split_identifier(identifier):\n",
    "    \"\"\"\n",
    "    Split identifier into a list of subtokens.\n",
    "    Tokens except characters and digits will be eliminated.\n",
    "\n",
    "    Args:\n",
    "        identifier (str): given identifier\n",
    "\n",
    "    Returns:\n",
    "        list[str]: list of subtokens\n",
    "    \"\"\"\n",
    "    words = []\n",
    "\n",
    "    word = re.sub(r'[^a-zA-Z0-9]', ' ', identifier)\n",
    "    word = re.sub(r'(\\d+)', r' \\1 ', word)\n",
    "    split_words = word.strip().split()\n",
    "    for split_word in split_words:\n",
    "        camel_words = camel_split(split_word)\n",
    "        for camel_word in camel_words:\n",
    "            words.append(camel_word.lower())\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b9048d9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f682c7",
   "metadata": {},
   "source": [
    "To parse code into an Abstract Syntax Tree (AST) using a parser specific to the language provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "664c7a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ast(source, lang):\n",
    "    \"\"\"\n",
    "    Parse the given code into corresponding ast.\n",
    "    Args:\n",
    "        source (str): code in string\n",
    "        lang (str): Set the language\n",
    "\n",
    "    Returns:\n",
    "        tree_sitter.Node: Method/Function root node\n",
    "\n",
    "    \"\"\"\n",
    "    #Attempts to set the language of the parser. If it fails, it prints an error message and exits the program.\n",
    "    try:\n",
    "        parser.set_language(LANGUAGE[lang])\n",
    "    except Exception as err:\n",
    "        print(f'[ERR]: {err}')\n",
    "        sys.exit(0)\n",
    "\n",
    "    #If the language has specified prefix and postfix strings in SOURCE_PREFIX_POSTFIX, it adds them to the source code.\n",
    "    if lang in SOURCE_PREFIX_POSTFIX:\n",
    "        source = SOURCE_PREFIX_POSTFIX[lang][0] + source + SOURCE_PREFIX_POSTFIX[lang][1]\n",
    "    #It then parses the source code using the parser for the specified language. The code is encoded and decoded to handle Unicode characters.\n",
    "    tree = parser.parse(source.encode('utf-8').decode('unicode_escape').encode())\n",
    "    root = tree.root_node\n",
    "    # tree = parser.parse(str.encode(source))\n",
    "    #If there's a pattern specified for finding the root of a method/function in PATTERNS_METHOD_ROOT, it uses that pattern to locate the root node.\n",
    "    if lang in PATTERNS_METHOD_ROOT:\n",
    "        query = LANGUAGE[lang].query(PATTERNS_METHOD_ROOT[lang])\n",
    "        captures = query.captures(root)\n",
    "        root = captures[0][0]\n",
    "    return root\n",
    "    #it returns the root node of the parsed AST."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb9366f",
   "metadata": {},
   "source": [
    "The get_node_name function retrieves the name of a given node in an Abstract Syntax Tree (AST). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acb68331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_name(source, node, lang):\n",
    "    \"\"\"\n",
    "    Get node name, for php is shifted by prefix.\n",
    "\n",
    "    Args:\n",
    "        source (str): Source code string\n",
    "        node (tree_sitter.Node): Node instance\n",
    "        lang (str): Source code language\n",
    "\n",
    "    Returns:\n",
    "        str: Name of node\n",
    "\n",
    "    \"\"\"\n",
    "    #This condition checks if the node is a named node. A named node is one that represents an actual part of the code (like an identifier, a keyword, etc.).\n",
    "    if node.is_named:\n",
    "    # If the language has specified prefix and postfix strings in SOURCE_PREFIX_POSTFIX, it adjusts the byte positions of the node to accommodate for these strings and extracts the corresponding substring from the source code.\n",
    "        if lang in SOURCE_PREFIX_POSTFIX:\n",
    "            return source[node.start_byte - len(SOURCE_PREFIX_POSTFIX[lang][0]):\n",
    "                          node.end_byte - len(SOURCE_PREFIX_POSTFIX[lang][0])]\n",
    "        else:\n",
    "        #If there's no prefix and postfix for the language, it simply extracts the substring corresponding to the node from the source code.\n",
    "            return source[node.start_byte: node.end_byte]\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7713deaf",
   "metadata": {},
   "source": [
    "#This function aims to return the name of a method/function given its root node in an AST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "491edb70",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def get_method_name(source, root, lang):\n",
    "    \"\"\"\n",
    "    Return the name of method/function.\n",
    "\n",
    "    Args:\n",
    "        source (str): Source code string\n",
    "        root (tree_sitter.Node): Method/Function root node\n",
    "        lang (str): Source code language\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    #It queries the AST for the pattern to find the method/function name based on the language specified.\n",
    "    query = LANGUAGE[lang].query(PATTERNS_METHOD_NAME[lang])\n",
    "    #it captures the matches\n",
    "    captures = query.captures(root)\n",
    "    #If no captures are found, it returns an empty string.\n",
    "    if len(captures) == 0:\n",
    "        return ''\n",
    "    #returns the name of the first captured node using the get_node_name function. \n",
    "    return get_node_name(source, captures[0][0], lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d0e5ff",
   "metadata": {},
   "source": [
    "To check whether a given node in an Abstract Syntax Tree (AST) represents a statement-level construct in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a34eef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_statement_node(node, lang):\n",
    "    \"\"\"\n",
    "    Return whether the node is a statement level node.\n",
    "\n",
    "    Args:\n",
    "        node (tree_sitter.Node): Node to be queried\n",
    "        lang (str): Source code language\n",
    "\n",
    "    Returns:\n",
    "        bool: True if given node is a statement node\n",
    "\n",
    "    \"\"\"\n",
    "    #Retrieves the list of statement ending strings specific to the language from the STATEMENT_ENDING_STRINGS dictionary.\n",
    "    endings = STATEMENT_ENDING_STRINGS[lang]\n",
    "    #Splits the node's type string by underscores and takes the last part. \n",
    "    end = node.type.split('_')[-1]\n",
    "    #if the extracted ending is found in the list of statement endings for the language, it returns True, indicating that the node represents a statement-level construct. Otherwise, it returns False\n",
    "    if end in endings:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4721a9b",
   "metadata": {},
   "source": [
    "this function retrieves the type of a given node in the Abstract Syntax Tree (AST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "284faf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the language is Ruby (LANG_RUBY), it appends \"_statement\" to the end of the node's type. \n",
    "#This is because in the Ruby language, many node types are followed by \"_statement\" (e.g., if_statement, while_statement). For other languages, it returns the node's type as it is.\n",
    "def get_node_type(node, lang):\n",
    "    \"\"\"\n",
    "    Return the type of node, for ruby, add ``_statement`` to the end.\n",
    "\n",
    "    Args:\n",
    "        node (tree_sitter.Node): Node to be queried\n",
    "        lang (str): Source code language\n",
    "\n",
    "    Returns:\n",
    "        str: Type of the node\n",
    "\n",
    "    \"\"\"\n",
    "    return f'{node.type}_statement' if lang == LANG_RUBY else node.type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fbf210",
   "metadata": {},
   "source": [
    "This function generates a representation of a source code AST in X-SBT format (Extended-Simple Binary Tree) recursively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9552735c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __statement_xsbt(node, lang):\n",
    "    \"\"\"\n",
    "    Method used to generate X-SBT recursively.\n",
    "\n",
    "    Args:\n",
    "        node (tree_sitter.Node): Root node to traversal\n",
    "        lang (str): Source code language\n",
    "\n",
    "    Returns:\n",
    "        list[str]: List of strings representing node types\n",
    "\n",
    "    \"\"\"\n",
    "    xsbt = []\n",
    "#If the current node has no children and is a statement node (determined by is_statement_node function), its type is appended to xsbt\n",
    "    if len(node.children) == 0:\n",
    "        if is_statement_node(node, lang):\n",
    "            xsbt.append(get_node_type(node, lang))\n",
    "    else:\n",
    "    #If the current node has children:\n",
    "    #If the current node is a statement node, its type followed by \"__\" is appended to xsbt.\n",
    "        if is_statement_node(node, lang):\n",
    "            xsbt.append(f'{get_node_type(node, lang)}__')\n",
    "    #the current length of xsbt is stored in len_before.\n",
    "        len_before = len(xsbt)\n",
    "    #Each child node is processed recursively, and their X-SBT representations are concatenated to xsbt.\n",
    "        for child in node.children:\n",
    "            xsbt += __statement_xsbt(node=child, lang=lang)\n",
    "    #After processing children, if the length of xsbt remains unchanged and is not zero (i.e., no new nodes were added), the last item in xsbt is replaced with the current node's type. This ensures correct representation of the parent node.\n",
    "        if len_before == len(xsbt) and len_before != 0:\n",
    "            xsbt[-1] = get_node_type(node, lang)\n",
    "    #If new nodes were added to xsbt and the current node is a statement node, the current node's type preceded by \"__\" is appended to xsbt. This marks the end of the parent node's representation and the start of child nodes' representations\n",
    "        elif is_statement_node(node, lang):\n",
    "            xsbt.append(f'__{get_node_type(node, lang)}')\n",
    "\n",
    "    return xsbt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfce80da",
   "metadata": {},
   "source": [
    "This function generates an X-SBT (Extended-Simple Binary Tree) string from a given root node in the Abstract Syntax Tree (AST)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec261b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_statement_xsbt(node, lang):\n",
    "    \"\"\"\n",
    "    Generate X-SBT string.\n",
    "\n",
    "    Args:\n",
    "        node (tree_sitter.Node): Root node to traversal\n",
    "        lang (str): Source code language\n",
    "\n",
    "    Returns:\n",
    "        str: X-SBT string\n",
    "\n",
    "    \"\"\"\n",
    "#If the language has specified patterns for method bodies in PATTERNS_METHOD_BODY, it queries the AST using this pattern and retrieves the first capture. \n",
    "#This is useful for extracting the method body node if necessary.\n",
    "    if lang in PATTERNS_METHOD_BODY:\n",
    "        query = LANGUAGE[lang].query(PATTERNS_METHOD_BODY[lang])\n",
    "        captures = query.captures(node)\n",
    "        node = captures[0][0]\n",
    "    #It generates the X-SBT tokens using the __statement_xsbt function with the given node and language.\n",
    "    tokens = __statement_xsbt(node=node, lang=lang)\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80317660",
   "metadata": {},
   "source": [
    "This function aims to extract method invocation sequences from a given root node in an Abstract Syntax Tree (AST). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8c7fe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_method_invocation(source, root, lang):\n",
    "    \"\"\"\n",
    "    Extract method invocation sequence from given root.\n",
    "\n",
    "    Args:\n",
    "        source (str): Source code string\n",
    "        root (tree_sitter.Node): Node to be extracted from\n",
    "        lang (str): Source code language\n",
    "\n",
    "    Returns:\n",
    "        list[str]: List of method invocation strings\n",
    "\n",
    "    \"\"\"\n",
    "#It queries the AST using the method invocation pattern specified in PATTERNS_METHOD_INVOCATION for the given language. It captures all instances of method invocations.\n",
    "    query = LANGUAGE[lang].query(PATTERNS_METHOD_INVOCATION[lang])\n",
    "    captures = query.captures(root)\n",
    "#It iterates over each captured method invocation node, retrieves its name using the get_node_name function, and collects these names into a list.\n",
    "    return [get_node_name(source=source, node=capture[0], lang=lang) for capture in captures]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d665fe9",
   "metadata": {},
   "source": [
    "This function is designed to extract natural language tokens from source code, including splitting the method/function name and method invocation names if necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7daa6fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nl_from_code(source, root, lang, name=None, replace_method_name=False):\n",
    "    \"\"\"\n",
    "    Extract nl tokens from given source code, including split name and method invocations.\n",
    "\n",
    "    Args:\n",
    "        source (str): Source code string`\n",
    "        root (tree_sitter.Node): Root of code\n",
    "        lang (str): Source code language\n",
    "        name (str): optional, name of method/function\n",
    "        replace_method_name (bool): Whether to replace method name and returns a version that without names additionally\n",
    "\n",
    "    Returns:\n",
    "        Union[(str, str), str]:\n",
    "            - Nl string\n",
    "            - Nl string without method name\n",
    "\n",
    "    \"\"\"\n",
    "#Two lists, tokens and tokens_wo_name, are initialized to hold the NL tokens. tokens will hold all NL tokens, including method/function names and method invocation names. tokens_wo_name will hold NL tokens excluding method/function names.\n",
    "    tokens = []\n",
    "    tokens_wo_name = []\n",
    "\n",
    "#If the method/function name is not provided, it's extracted using get_method_name. The name is then split into tokens using split_identifier, and these tokens are added to both tokens and tokens_wo_name.\n",
    "    if name is None:\n",
    "        name = get_method_name(source=source, root=root, lang=lang)\n",
    "    name_tokens = split_identifier(name)\n",
    "    tokens += name_tokens\n",
    "\n",
    "#Each invocation name is split into tokens using split_identifier, and these tokens are added to both tokens and tokens_wo_name.\n",
    "    invocations = extract_method_invocation(source=source, root=root, lang=lang)\n",
    "    for invocation in invocations:\n",
    "        subtokens = split_identifier(invocation)\n",
    "        tokens += subtokens\n",
    "        tokens_wo_name += subtokens\n",
    "\n",
    "#If replace_method_name is True, it returns both the NL string with method/function names (' '.join(tokens)) and the NL string without method/function names (' '.join(tokens_wo_name)).\n",
    "#If replace_method_name is False, it returns only the NL string with method/function names (' '.join(tokens)).\n",
    "    if replace_method_name:\n",
    "        return ' '.join(tokens), ' '.join(tokens_wo_name)\n",
    "    else:\n",
    "        return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb5da4b",
   "metadata": {},
   "source": [
    "This generate_single_ast_nl function generates AST sequences and natural language (NL) sequences for a single source code sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1bb5ce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_single_ast_nl(source, lang, name=None, replace_method_name=False):\n",
    "    \"\"\"\n",
    "    Generate AST sequence and nl sequence for a single source code sample.\n",
    "\n",
    "    Args:\n",
    "        source (str): Source code string\n",
    "        lang (str): Source code language\n",
    "        name (str): optional, name of method/function\n",
    "        replace_method_name (bool): Whether to replace method name and returns a version that without names additionally\n",
    "\n",
    "    Returns:\n",
    "        Union[(str, str), (str, str, str)]:\n",
    "            - AST sequence in string\n",
    "            - Nl sequence in string\n",
    "\n",
    "    \"\"\"\n",
    "#The source code is parsed into an AST (root) using the parse_ast function.\n",
    "#The AST is converted into an AST sequence (ast) using the generate_statement_xsbt function.\n",
    "    root = parse_ast(source=source, lang=lang)\n",
    "    ast = generate_statement_xsbt(node=root, lang=lang)\n",
    "#If replace_method_name is True, NL sequences are extracted with and without method/function names using the extract_nl_from_code function. \n",
    "#Both the AST sequence, NL sequence with names (nl), and NL sequence without names (nl_wo_name) are returned.\n",
    "    if replace_method_name:\n",
    "        nl, nl_wo_name = extract_nl_from_code(source=source,\n",
    "                                              root=root,\n",
    "                                              lang=lang,\n",
    "                                              name=name,\n",
    "                                              replace_method_name=replace_method_name)\n",
    "        return ast, nl, nl_wo_name\n",
    "    else:\n",
    "#f replace_method_name is False, NL sequence is extracted only with method/function names.\n",
    "        nl = extract_nl_from_code(source=source, root=root, lang=lang, name=name)\n",
    "        return ast, nl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f9906e",
   "metadata": {},
   "source": [
    "This function generates AST sequences and natural language (NL) sequences for a list of source code samples. It filters out any exceptions that occur during processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69a77b2f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def generate_asts_nls(sources, langs):\n",
    "    \"\"\"\n",
    "    Generate AST sequence and nl sequence for a list of source code samples, exceptions will be eliminate.\n",
    "\n",
    "    Args:\n",
    "        sources (str): List of source code strings\n",
    "        langs (str): List of source code languages\n",
    "\n",
    "    Returns:\n",
    "        (list[str], list[str], list[str], list[str]):\n",
    "            - List of language strings\n",
    "            - List of source code strings\n",
    "            - List of AST sequence strings\n",
    "            - List of nl sequence strings\n",
    "\n",
    "    \"\"\"\n",
    "    assert len(sources) == len(langs)\n",
    "    new_langs = []\n",
    "    new_sources = []\n",
    "    asts = []\n",
    "    nls = []\n",
    "    #It iterates through each pair of language and source code using zip(langs, sources)\n",
    "    for lang, source in zip(langs, sources):\n",
    "        try:\n",
    "        #For each pair, it attempts to generate AST and NL sequences using generate_single_ast_nl\n",
    "            ast, nl = generate_single_ast_nl(source=source, lang=lang)\n",
    "        #If successful, it appends the language, source code, AST sequence, and NL sequence to the respective lists.\n",
    "            new_langs.append(lang)\n",
    "            new_sources.append(source)\n",
    "            asts.append(ast)\n",
    "            nls.append(nl)\n",
    "        #If an exception occurs, it continues to the next pair without raising the exception.\n",
    "        except Exception:\n",
    "            continue\n",
    "    #it returns the filtered lists of language strings, source code strings, AST sequences, and NL sequences.\n",
    "    return new_langs, new_sources, asts, nls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f83d0d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_java_code(source_code):\n",
    "    root = parse_ast(source_code, 'java')\n",
    "    method_names = get_method_name(source_code, root, 'java')\n",
    "    method_invocations = extract_method_invocation(source_code, root, 'java')\n",
    "    print(\"Method Names:\", method_names)\n",
    "    print(\"Method Invocations:\", method_invocations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "782d5d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method Names: Y\n",
      "Method Invocations: ['J', 'log', 'sqrt', 'sin', 'cos']\n"
     ]
    }
   ],
   "source": [
    "# Example code\n",
    "java_code = \"public static double Y(double x) {\\r\\n        if (x < 8.0) {\\r\\n            double y = x * x;\\r\\n            double ans1 = x * (-0.4900604943e13 + y * (0.1275274390e13\\r\\n                    + y * (-0.5153438139e11 + y * (0.7349264551e9\\r\\n                    + y * (-0.4237922726e7 + y * 0.8511937935e4)))));\\r\\n            double ans2 = 0.2499580570e14 + y * (0.4244419664e12\\r\\n                    + y * (0.3733650367e10 + y * (0.2245904002e8\\r\\n                    + y * (0.1020426050e6 + y * (0.3549632885e3 + y)))));\\r\\n            return (ans1 / ans2) + 0.636619772 * (J(x) * Math.log(x) - 1.0 / x);\\r\\n        } else {\\r\\n            double z = 8.0 / x;\\r\\n            double y = z * z;\\r\\n            double xx = x - 2.356194491;\\r\\n            double ans1 = 1.0 + y * (0.183105e-2 + y * (-0.3516396496e-4\\r\\n                    + y * (0.2457520174e-5 + y * (-0.240337019e-6))));\\r\\n            double ans2 = 0.04687499995 + y * (-0.2002690873e-3\\r\\n                    + y * (0.8449199096e-5 + y * (-0.88228987e-6\\r\\n                    + y * 0.105787412e-6)));\\r\\n            return Math.sqrt(0.636619772 / x) *\\r\\n                    (Math.sin(xx) * ans1 + z * Math.cos(xx) * ans2);\\r\\n        }\\r\\n    }\"\n",
    "\n",
    "analyze_java_code(java_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5ed498",
   "metadata": {},
   "source": [
    "It retrieves the AST sequence, method/function names, and source code for a given list of code lines in multiple languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74421c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ast_name(languages, code_lines):\n",
    "    assert len(languages) == len(code_lines)\n",
    "    langs = []\n",
    "    asts = []\n",
    "    names = []\n",
    "    codes = []\n",
    "#Iterates through each pair of language and code line using zip(languages, code_lines)\n",
    "    for lang, line in zip(languages, code_lines):\n",
    "        try:\n",
    "            #Tries to parse the code line into an AST using parse_ast.\n",
    "            tree = parse_ast(line, lang=lang)\n",
    "            #Generates the AST sequence using generate_statement_xsbt.\n",
    "            ast = generate_statement_xsbt(line, tree.root_node)\n",
    "            #retrieves the method/function name using get_method_name\n",
    "            name = get_method_name(line, lang=lang, root=tree.root_node)\n",
    "            #appends the language, AST sequence (as a string), method/function name, and source code to the respective lists.\n",
    "            langs.append(lang)\n",
    "            asts.append(' '.join(ast))\n",
    "            names.append(name)\n",
    "            codes.append(line)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return langs, codes, asts, names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252e29b3",
   "metadata": {},
   "source": [
    "These functions are used to generate AST (Abstract Syntax Tree) representations for a single source code sample, optionally including the method/function name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85f847ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_single_ast(lang, source):\n",
    "    tree = parse_ast(source=source, lang=lang)\n",
    "    ast = generate_statement_xsbt(tree.root_node)\n",
    "    return ' '.join(ast)\n",
    "\n",
    "\n",
    "def get_single_ast_name(lang, source):\n",
    "    tree = parse_ast(source=source, lang=lang)\n",
    "    ast = generate_statement_xsbt(tree.root_node)\n",
    "    name = get_method_name(source, lang=lang, root=tree.root_node)\n",
    "    return ast, name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98d537e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_sample(lang):\n",
    "    import random, json\n",
    "    with open(f'/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/pre_train/java/traintest.jsonl') as f:\n",
    "        line = f.readline()#f.readlines()[random.randint(0, 1000)]\n",
    "        data = json.loads(line)#json.loads(line.strip())\n",
    "        name = data['func_name']\n",
    "        source = data['code']\n",
    "    return source, name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17f76bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "source:\n",
      "public static double Sinc(double x) {\n",
      "        return Math.sin(Math.PI * x) / (Math.PI * x);\n",
      "    }\n",
      "----------------------------------------------------------------------------------------------------\n",
      "name in json:\n",
      "Tools.Sinc\n",
      "----------------------------------------------------------------------------------------------------\n",
      "method name:\n",
      "Sinc\n",
      "----------------------------------------------------------------------------------------------------\n",
      "method_invocation:\n",
      "['sin']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "xsbt in statements:\n",
      "return_statement__ binary_expression__ binary_expression parenthesized_expression__ binary_expression __parenthesized_expression __binary_expression __return_statement\n",
      "----------------------------------------------------------------------------------------------------\n",
      "nl from code:\n",
      "tools sinc sin\n",
      "----------------------------------------------------------------------------------------------------\n",
      "single ast from code:\n",
      "('return_statement__ binary_expression__ binary_expression parenthesized_expression__ binary_expression __parenthesized_expression __binary_expression __return_statement', 'tools sinc sin')\n"
     ]
    }
   ],
   "source": [
    "lang = 'java'\n",
    "\n",
    "source, name = lang_sample(lang)\n",
    "print('-' * 100)\n",
    "print('source:')\n",
    "print(source)\n",
    "print('-' * 100)\n",
    "print('name in json:')\n",
    "print(name)\n",
    "print('-' * 100)\n",
    "\n",
    "root = parse_ast(source, lang=lang)\n",
    "print('method name:')\n",
    "print(get_method_name(source=source, root=root, lang=lang))\n",
    "print('-' * 100)\n",
    "print('method_invocation:')\n",
    "print(extract_method_invocation(source, root, lang))\n",
    "print('-' * 100)\n",
    "print('xsbt in statements:')\n",
    "print(generate_statement_xsbt(root, lang))\n",
    "print('-' * 100)\n",
    "print('nl from code:')\n",
    "print(extract_nl_from_code(source,root,lang,name))\n",
    "print('-' * 100)\n",
    "print('single ast from code:')\n",
    "print(generate_single_ast_nl(source, lang, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb5e2267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, WordLevel\n",
    "from tokenizers.trainers import BpeTrainer, WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import Strip, Lowercase, NFD, StripAccents\n",
    "from tokenizers.processors import TemplateProcessing, BertProcessing\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import logging\n",
    "\n",
    "from typing import List, Union\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3aa5edd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517862c4",
   "metadata": {},
   "source": [
    "This class will manage vocabulary creation, tokenization, and encoding/decoding of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38401535",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    # special vocabulary symbols\n",
    "    PAD_TOKEN = '[PAD]'  # padding token\n",
    "    SOS_TOKEN = '[SOS]'  # start of sequence, also CLS\n",
    "    EOS_TOKEN = '[EOS]'  # end of sequence\n",
    "    UNK_TOKEN = '[UNK]'  # unknown token\n",
    "    MSK_TOKEN = '[MSK]'  # mask token\n",
    "    SEP_TOKEN = '[SEP]'  # sentence separator token\n",
    "    # CLS_TOKEN = '[CLS]'     # classification placeholder\n",
    "\n",
    "    # default special symbols, if need additional symbols, use init parameter 'additional_special_symbols'\n",
    "    START_VOCAB = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN, MSK_TOKEN, SEP_TOKEN]\n",
    "\n",
    "    # post-processors\n",
    "    # bert processor: add SOS at the beginning and SEP at the end of sequence\n",
    "    # bert_processor = BertProcessing(sep=(SEP_TOKEN, START_VOCAB.index(EOS_TOKEN)),\n",
    "    #                                 cls=(SOS_TOKEN, START_VOCAB.index(SOS_TOKEN)))\n",
    "    # sos processor: add SOS at the beginning of sequence\n",
    "    sos_processor = TemplateProcessing(single=f'{SOS_TOKEN} $', pair=f'{SOS_TOKEN} $A $B',\n",
    "                                       special_tokens=[(SOS_TOKEN, START_VOCAB.index(SOS_TOKEN))])\n",
    "    # eos processor: add EOS at the end of sequence\n",
    "    eos_processor = TemplateProcessing(single=f'$ {EOS_TOKEN}', pair=f'$A $B {EOS_TOKEN}',\n",
    "                                       special_tokens=[(EOS_TOKEN, START_VOCAB.index(EOS_TOKEN))])\n",
    "    # sep processor: add SEP at the end of sequence\n",
    "    sep_processor = TemplateProcessing(single=f'$ {SEP_TOKEN}', pair=f'$A $B {SEP_TOKEN}',\n",
    "                                       special_tokens=[(SEP_TOKEN, START_VOCAB.index(SEP_TOKEN))])\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            name,\n",
    "            method,\n",
    "            vocab_size=None,\n",
    "            datasets: Union[List[str], List[List[str]]] = None,\n",
    "            additional_special_symbols=None,\n",
    "            ignore_case=False,\n",
    "            save_root=None,\n",
    "            index_offset=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize a vocabulary and train the tokenizer.\n",
    "\n",
    "        Args:\n",
    "            name (str): Vocabulary name\n",
    "            method (str): Tokenize method\n",
    "            vocab_size (int): Maximum size of the vocabulary\n",
    "            datasets (Union[List[str], List[List[str]]]): List of (file paths/list of string) to train the tokenizer\n",
    "            additional_special_symbols (list[str]): Optional, list of custom special symbols\n",
    "            ignore_case (bool): Ignore cases if True, default False\n",
    "            save_root (str): Optional, if given, save to given root\n",
    "            index_offset (int): Optional, the index offset when encoding and decoding.\n",
    "\n",
    "        \"\"\"\n",
    "        assert method in ['word', 'bpe'], \\\n",
    "            'Tokenize method not supported, given {}, expect \\'word\\' or \\'bpe\\''.format(method)\n",
    "\n",
    "        self.name = name\n",
    "        self.method = method\n",
    "        #It initializes __special_symbols with the default special symbols from Vocab.START_VOCAB\n",
    "        self.__special_symbols = Vocab.START_VOCAB.copy()\n",
    "        if additional_special_symbols:\n",
    "            self.add_special_symbols(additional_special_symbols)\n",
    "        self.ignore_case = ignore_case\n",
    "\n",
    "        #It sets index_offset to the provided value or None.\n",
    "        if index_offset is not None and index_offset != 0:\n",
    "            self.index_offset = index_offset\n",
    "        else:\n",
    "            self.index_offset = None\n",
    "\n",
    "        # tokenizer and trainer\n",
    "        if method == 'word':\n",
    "            tokenize_class = WordLevel\n",
    "            trainer_class = WordLevelTrainer\n",
    "        else:\n",
    "            if vocab_size is None:\n",
    "                logger.warning('It is recommended to specific the vocabulary size for BPE tokenize method')\n",
    "            tokenize_class = BPE\n",
    "            trainer_class = BpeTrainer\n",
    "\n",
    "        #The tokenizer is initialized with an unknown token.\n",
    "        self.tokenizer = Tokenizer(tokenize_class(unk_token=Vocab.UNK_TOKEN))\n",
    "        #If vocab_size is provided, a trainer is created with special symbols and the specified vocabulary size; otherwise, only special symbols are used.\n",
    "        if vocab_size:\n",
    "            trainer = trainer_class(special_tokens=self.__special_symbols, vocab_size=vocab_size)\n",
    "        else:\n",
    "            trainer = trainer_class(special_tokens=self.__special_symbols)\n",
    "        #It sets up the pre-tokenizer to use whitespace.\n",
    "        self.tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "        # normalizer\n",
    "        #It sets up a normalizer to normalize the input text, optionally converting it to lowercase if ignore_case is True.\n",
    "        if ignore_case:\n",
    "            normalizer = normalizers.Sequence([NFD(), StripAccents(), Strip(), Lowercase()])\n",
    "        else:\n",
    "            normalizer = normalizers.Sequence([NFD(), StripAccents(), Strip()])\n",
    "        self.tokenizer.normalizer = normalizer\n",
    "\n",
    "    #If the dataset is a list of file paths, the tokenizer is trained using files with the specified trainer.\n",
    "    #If the dataset is a list of lists of strings, the tokenizer is trained from the provided iterator.\n",
    "    #If the dataset type is not supported, a TypeError is raised.\n",
    "        # train tokenizer\n",
    "        if isinstance(datasets[0], str):\n",
    "            self.tokenizer.train(files=datasets, trainer=trainer)\n",
    "        elif isinstance(datasets[0], list):\n",
    "            self.tokenizer.train_from_iterator(iterator=datasets, trainer=trainer)\n",
    "        else:\n",
    "            raise TypeError('The type of datasets is not support, expect list of paths or list of lines')\n",
    "\n",
    "        # pad idx\n",
    "        #The pad_token_id is set to the index of the padding token.\n",
    "        self.pad_token_id = self.get_pad_index()\n",
    "\n",
    "        # save\n",
    "        #If save_root is provided, the vocabulary is saved to the specified directory.\n",
    "        if save_root:\n",
    "            self.save(vocab_root=save_root)\n",
    "\n",
    "#The add_special_symbols method allows adding custom special symbols to the vocabulary.\n",
    "    def add_special_symbols(self, symbols: list):\n",
    "        assert isinstance(symbols, list)\n",
    "        for symbol in symbols:\n",
    "            assert isinstance(symbol, str)\n",
    "    #If the symbol is not already in the list of special symbols (__special_symbols), it appends the symbol to the list.\n",
    "            if symbol not in self.__special_symbols:\n",
    "                self.__special_symbols.append(symbol)\n",
    "\n",
    "# returns the index of a given word in the vocabulary\n",
    "    def get_index(self, token: str) -> int:\n",
    "        \"\"\"\n",
    "        Return the index of given word, if the given word is not in the vocabulary, return the index of UNK token.\n",
    "\n",
    "        Args:\n",
    "            token (str): Word in str\n",
    "\n",
    "        Returns:\n",
    "            int: Index of the given word, [UNK] if OOV\n",
    "\n",
    "        \"\"\"\n",
    "    #It first checks if the vocabulary is case-insensitive (ignore_case is True). If so, it converts the token to lowercase.\n",
    "        if self.ignore_case:\n",
    "            token = token.lower()\n",
    "    #Then, it uses the tokenizer.token_to_id() method to retrieve the index of the token.\n",
    "#If an index_offset is specified and the token is not in the list of special symbols (__special_symbols), it adds the offset to the index.\n",
    "        index = self.tokenizer.token_to_id(token)\n",
    "#If the token is not in the vocabulary, it returns the index of the UNK token.\n",
    "        if self.index_offset and token not in self.__special_symbols:\n",
    "            index += self.index_offset\n",
    "#Otherwise, it returns the obtained index.\n",
    "        return index if index else self.tokenizer.token_to_id(Vocab.UNK_TOKEN)\n",
    "\n",
    "    def get_token(self, index: int) -> str:\n",
    "        \"\"\"\n",
    "        Return the corresponding token of the given index, if not in the vocabulary, return index of UNK.\n",
    "\n",
    "        Args:\n",
    "            index: Given index\n",
    "\n",
    "        Returns:\n",
    "            str: Token of the given index\n",
    "\n",
    "        \"\"\"\n",
    "        if self.index_offset:\n",
    "            if index >= (len(self.__special_symbols) + self.index_offset):\n",
    "                index -= self.index_offset\n",
    "            elif len(self.__special_symbols) <= index < (len(self.__special_symbols) + self.index_offset):\n",
    "                index = self.get_unk_index()\n",
    "        token = self.tokenizer.id_to_token(index)\n",
    "        return token if token else Vocab.UNK_TOKEN\n",
    "\n",
    "    def transfer_index(self, index):\n",
    "        \"\"\"\n",
    "        Return the transferred index based on the index offset\n",
    "\n",
    "        Args:\n",
    "            index (int): Index to transfer\n",
    "\n",
    "        Returns:\n",
    "            int: Transferred index\n",
    "\n",
    "        \"\"\"\n",
    "        if not self.index_offset or index < len(self.__special_symbols):\n",
    "            return index\n",
    "        return index + self.index_offset\n",
    "\n",
    "    def restore_index(self, index):\n",
    "        \"\"\"\n",
    "        Return the restored index based on the base index\n",
    "\n",
    "        Args:\n",
    "            index (int): Index to restore\n",
    "\n",
    "        Returns:\n",
    "            int: Restored index\n",
    "\n",
    "        \"\"\"\n",
    "        if not self.index_offset or index < len(self.__special_symbols):\n",
    "            return index\n",
    "        if index < self.index_offset:\n",
    "            return self.get_unk_index()\n",
    "        return index - self.index_offset\n",
    "    # Methods like encode_sequence, encode_batch, decode, and decode_batch handle encoding and decoding of sequences using the tokenizer.\n",
    "    def encode_sequence(self, sequence: Union[str, List[str]], is_pre_tokenized=False):\n",
    "        \"\"\"\n",
    "        Encode a sequence to corresponding ids.\n",
    "\n",
    "        Args:\n",
    "            sequence (Union[str, List[str]]): Sequence to be encoded,\n",
    "                when is_pre_tokenized is False, the type should be str,\n",
    "                when is_pre_tokenized is True, the type should be List[str]\n",
    "            is_pre_tokenized (bool): Whether the input is already pre-tokenized\n",
    "\n",
    "        Returns:\n",
    "            list[int], list[int]: indices and mask for sequence\n",
    "\n",
    "        \"\"\"\n",
    "        if self.ignore_case:\n",
    "            sequence = sequence.lower()\n",
    "        encoded = self.tokenizer.encode(sequence=sequence, is_pretokenized=is_pre_tokenized)\n",
    "        ids = [self.transfer_index(index) for index in encoded.ids]\n",
    "        return ids, encoded.attention_mask\n",
    "\n",
    "    def encode_batch(self, batch: Union[List[str], List[List[str]]], is_pre_tokenized=False,\n",
    "                     pad=False, max_length=None):\n",
    "        \"\"\"\n",
    "        Encode a batch of sequences.\n",
    "\n",
    "        Args:\n",
    "            batch (Union[List[str], List[List[str]]]): batch of sequences to be encoded,\n",
    "                when is_pre_tokenized is False, the type should be List[str],\n",
    "                when is_pre_tokenized is True, the type should be List[List[str]]\n",
    "            is_pre_tokenized (bool): Whether the input is already pre-tokenized\n",
    "            pad (bool): Whether to pad each of the sequence\n",
    "            max_length (int): The length to padding\n",
    "\n",
    "        Returns:\n",
    "            (list[list[int]], list[list[int]]):\n",
    "                - encoded batch of indices\n",
    "                - encoded batch of attention masks\n",
    "\n",
    "        \"\"\"\n",
    "        if self.ignore_case:\n",
    "            batch = [sequence.lower() if isinstance(sequence, str) else [token.lower() for token in sequence]\n",
    "                     for sequence in batch]\n",
    "        if pad:\n",
    "            self.tokenizer.enable_padding(length=max_length)\n",
    "        else:\n",
    "            self.tokenizer.no_padding()\n",
    "        encoded_batch = self.tokenizer.encode_batch(input=batch, is_pretokenized=is_pre_tokenized)\n",
    "        ids = [[self.transfer_index(index) for index in encoded.ids] for encoded in encoded_batch]\n",
    "        attention_mask = [encoded.attention_mask for encoded in encoded_batch]\n",
    "        return ids, attention_mask\n",
    "\n",
    "    def decode(self, ids: List[int], skip_special_tokens=True) -> str:\n",
    "        \"\"\"\n",
    "        Decode the given list of ids back to a string.\n",
    "\n",
    "        Args:\n",
    "            ids (list[int]): The list of ids that we want to decode\n",
    "            skip_special_tokens (bool): Whether the special tokens should be removed from the decoded string,\n",
    "                default True\n",
    "\n",
    "        Returns:\n",
    "            str: The decoded string\n",
    "\n",
    "        \"\"\"\n",
    "        if self.index_offset:\n",
    "            ids = [self.restore_index(index) for index in ids]\n",
    "        return self.tokenizer.decode(ids=ids, skip_special_tokens=skip_special_tokens)\n",
    "\n",
    "    def decode_batch(self, batch: List[List[int]], skip_special_tokens=True) -> List[str]:\n",
    "        \"\"\"\n",
    "        Decode a batch of ids back to their corresponding string.\n",
    "\n",
    "        Args:\n",
    "            batch (list[list[str]]): The batch of sequences we want to decode\n",
    "            skip_special_tokens (bool): Whether the special tokens should be removed from the decoded string,\n",
    "                default True\n",
    "\n",
    "        Returns:\n",
    "            list[str]: The list of decoded string\n",
    "\n",
    "        \"\"\"\n",
    "        if self.index_offset:\n",
    "            batch = [[self.restore_index(index) for index in seq] for seq in batch]\n",
    "        return self.tokenizer.decode_batch(sequences=batch, skip_special_tokens=skip_special_tokens)\n",
    "    \n",
    "#Methods like get_pad_index, get_sos_index, get_eos_index, get_unk_index, get_mask_index, num_special_token, __len__, and __contains__ \n",
    "#provide various utility functionalities such as getting special token indices, checking if a token is in the vocabulary, etc.\n",
    "    def get_pad_index(self):\n",
    "        return self.tokenizer.token_to_id(Vocab.PAD_TOKEN)\n",
    "\n",
    "    def get_sos_index(self):\n",
    "        return self.tokenizer.token_to_id(Vocab.SOS_TOKEN)\n",
    "\n",
    "    def get_eos_index(self):\n",
    "        return self.tokenizer.token_to_id(Vocab.EOS_TOKEN)\n",
    "\n",
    "    def get_unk_index(self):\n",
    "        return self.tokenizer.token_to_id(Vocab.UNK_TOKEN)\n",
    "\n",
    "    def get_mask_index(self):\n",
    "        return self.tokenizer.token_to_id(Vocab.MSK_TOKEN)\n",
    "\n",
    "    def save(self, vocab_root, name=None):\n",
    "        \"\"\"\n",
    "        Save the vocabulary to the given directory.\n",
    "\n",
    "        Args:\n",
    "            vocab_root (str): Parent directory to be saved\n",
    "            name (str): Vocabulary name\n",
    "\n",
    "        \"\"\"\n",
    "        vocab_name = name if name else self.name\n",
    "        vocab_dir = os.path.join(vocab_root, vocab_name)\n",
    "        if not os.path.exists(vocab_dir) or not os.path.isdir(vocab_dir):\n",
    "            os.makedirs(vocab_dir)\n",
    "\n",
    "        # save pickle file for whole instance\n",
    "        with open(os.path.join(vocab_dir, '{}.pk'.format(vocab_name)), mode='wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "        # save tokenizer\n",
    "        self.tokenizer.save(os.path.join(vocab_dir, '{}_tokenizer.json'.format(vocab_name)))\n",
    "        # save token to id mapping as a txt file\n",
    "        with open(os.path.join(vocab_dir, '{}_mapping.txt'.format(vocab_name)), mode='w', encoding='utf-8') as f:\n",
    "            for token, index in sorted(self.tokenizer.get_vocab().items(), key=lambda item: item[1]):\n",
    "                f.write('{}\\t{}\\n'.format(token, index))\n",
    "\n",
    "    def save_pretrained(self, output_dir):\n",
    "        return\n",
    "\n",
    "    def num_special_token(self):\n",
    "        return len(self.__special_symbols)\n",
    "    \n",
    "#The save method saves the vocabulary to a directory, including the tokenizer and token-to-id mappings. \n",
    "    def save_pickle(self, path):\n",
    "        \"\"\"Save to binary pickle file\"\"\"\n",
    "        with open(path, mode='wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "        logger.info(f'Vocab saved to {path}')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tokenizer.get_vocab_size()\n",
    "\n",
    "    def __contains__(self, item):\n",
    "        \"\"\"\n",
    "        Return True if the given token is in the vocab, else False.\n",
    "\n",
    "        Args:\n",
    "            item (str): Word to query\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the given token is in the vocab, else False.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.ignore_case:\n",
    "            item = item.lower()\n",
    "        return True if self.tokenizer.token_to_id(item) else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ec6a49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special symbols after adding: ['[PAD]', '[SOS]', '[EOS]', '[UNK]', '[MSK]', '[SEP]', '[CLS]', '[MASK]']\n",
      "Index of 'token': 3\n",
      "Token at index 5:  [SEP]\n",
      "Transferred index for 3:  3\n",
      "Restored index for 8:  8\n",
      "Encoded sequence indices: [22, 37, 67, 3, 6, 67, 3, 7, 11, 16, 3, 8, 3, 6, 3, 8, 3, 82, 3, 7, 81, 6, 3, 8, 3, 82, 3, 3, 12]\n",
      "Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Encoded batch indices: [[22, 37, 67, 3, 6, 67, 3, 7, 11, 16, 3, 8, 3, 6, 3, 8, 3, 82, 3, 7, 81, 6, 3, 8, 3, 82, 3, 3, 12], [22, 37, 67, 3, 6, 26, 87, 10, 67, 3, 7, 11, 67, 3, 10, 3, 10, 3, 10, 3, 9, 15, 6, 87, 31, 27, 7, 16, 3, 6, 3, 3, 15, 6, 87, 31, 39, 7, 16, 3, 6, 3, 3, 3, 13, 97, 8, 27, 81, 3, 9, 3, 13, 3, 6, 3, 3, 3, 13, 3, 6, 3, 3, 35, 6, 26, 3, 13, 39, 9, 3, 17, 87, 9, 3, 3, 11, 3, 13, 3, 82, 3, 82, 3, 40, 3, 9, 3, 13, 3, 9, 3, 13, 3, 9, 12, 16, 3, 9, 12]]\n",
      "Batch attention masks: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "Decoded sequence: public static double ( double ) { return . ( . * ) / ( . * }\n",
      "Decoded batch: ['public static double ( double ) { return . ( . * ) / ( . * }', 'public static double ( int n , double ) { double , , , ; if ( n == 0 ) return ( if ( n == 1 ) return ( = 2 . 0 / ; = ( = ( for ( int = 1 ; < n ; { = * * - ; = ; = ; } return ; }']\n",
      "PAD token index: 0\n",
      "SOS token index: 1\n",
      "EOS token index: 2\n",
      "UNK token index: 3\n",
      "MSK token index: 4\n",
      "Number of special tokens: 8\n",
      "Vocabulary size: 100\n",
      "'token' is in vocabulary: False\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(\n",
    "    name=\"example_vocab\",\n",
    "    method=\"word\",\n",
    "    vocab_size=100,\n",
    "    datasets=[\"/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/pre_train/java/train/token.code\"]\n",
    ")\n",
    "\n",
    "# Test add_special_symbols\n",
    "additional_special_symbols = ['[CLS]', '[MASK]']\n",
    "vocab.add_special_symbols(additional_special_symbols)\n",
    "print(\"Special symbols after adding:\", vocab._Vocab__special_symbols)\n",
    "\n",
    "# Test get_index\n",
    "token = 'token'\n",
    "print(\"Index of '{}':\".format(token), vocab.get_index(token))\n",
    "\n",
    "# Test get_token\n",
    "index = 5\n",
    "print(\"Token at index {}: \".format(index), vocab.get_token(index))\n",
    "\n",
    "# Test transfer_index\n",
    "index_to_transfer = 3\n",
    "print(\"Transferred index for {}: \".format(index_to_transfer), vocab.transfer_index(index_to_transfer))\n",
    "\n",
    "# Test restore_index\n",
    "index_to_restore = 8\n",
    "print(\"Restored index for {}: \".format(index_to_restore), vocab.restore_index(index_to_restore))\n",
    "\n",
    "# Test encode_sequence\n",
    "sequence = \"public static double Sinc(double x) {\\r\\n        return Math.sin(Math.PI * x) / (Math.PI * x);\\r\\n    }\"\n",
    "ids, attention_mask = vocab.encode_sequence(sequence)\n",
    "print(\"Encoded sequence indices:\", ids)\n",
    "print(\"Attention mask:\", attention_mask)\n",
    "\n",
    "# Test encode_batch\n",
    "batch = [\"public static double Sinc(double x) {\\r\\n        return Math.sin(Math.PI * x) / (Math.PI * x);\\r\\n    }\",\"public static double Y(int n, double x) {\\r\\n        double by, bym, byp, tox;\\r\\n\\r\\n        if (n == 0) return Y0(x);\\r\\n        if (n == 1) return Y(x);\\r\\n\\r\\n        tox = 2.0 / x;\\r\\n        by = Y(x);\\r\\n        bym = Y0(x);\\r\\n        for (int j = 1; j < n; j++) {\\r\\n            byp = j * tox * by - bym;\\r\\n            bym = by;\\r\\n            by = byp;\\r\\n        }\\r\\n        return by;\\r\\n    }\"]\n",
    "batch_ids, batch_attention_mask = vocab.encode_batch(batch)\n",
    "print(\"Encoded batch indices:\", batch_ids)\n",
    "print(\"Batch attention masks:\", batch_attention_mask)\n",
    "\n",
    "# Test decode\n",
    "decoded_sequence = vocab.decode(ids)\n",
    "print(\"Decoded sequence:\", decoded_sequence)\n",
    "\n",
    "# Test decode_batch\n",
    "decoded_batch = vocab.decode_batch(batch_ids)\n",
    "print(\"Decoded batch:\", decoded_batch)\n",
    "\n",
    "# Test get_pad_index\n",
    "print(\"PAD token index:\", vocab.get_pad_index())\n",
    "\n",
    "# Test get_sos_index\n",
    "print(\"SOS token index:\", vocab.get_sos_index())\n",
    "\n",
    "# Test get_eos_index\n",
    "print(\"EOS token index:\", vocab.get_eos_index())\n",
    "\n",
    "# Test get_unk_index\n",
    "print(\"UNK token index:\", vocab.get_unk_index())\n",
    "\n",
    "# Test get_mask_index\n",
    "print(\"MSK token index:\", vocab.get_mask_index())\n",
    "\n",
    "# Test num_special_token\n",
    "print(\"Number of special tokens:\", vocab.num_special_token())\n",
    "\n",
    "# Test __len__\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "\n",
    "# Test __contains__\n",
    "token_to_check = 'token'\n",
    "print(\"'{}' is in vocabulary:\".format(token_to_check), token_to_check in vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4c7953",
   "metadata": {},
   "source": [
    "This code will load the vocabulary instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae8c4b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(vocab_root, name) -> Vocab:\n",
    "    \"\"\"\n",
    "    Load vocabulary instance from pickle file, which should locates in the sub-directory named given vocabulary name.\n",
    "\n",
    "    Args:\n",
    "        vocab_root (BytesPath): Root of the vocabulary\n",
    "        name (str): Name of the vocabulary\n",
    "\n",
    "    Returns:\n",
    "        Vocab: Loaded vocab instance\n",
    "\n",
    "    \"\"\"\n",
    "    vocab_full_path = os.path.abspath(os.path.join(vocab_root, name, '{}.pk'.format(name)))\n",
    "    if not os.path.exists(vocab_full_path):\n",
    "        logger.info('-' * 100)\n",
    "        logger.info('Not Exist: %s', vocab_full_path)\n",
    "        logger.info('-' * 100)\n",
    "        sys.exit        \n",
    "\n",
    "    with open(os.path.join(vocab_root, name, '{}.pk'.format(name)), mode='rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    assert isinstance(obj, Vocab)\n",
    "    return obj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92903b65",
   "metadata": {},
   "source": [
    "This code will initialize the vocabulary and save the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3f8468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_vocab(vocab_save_dir,\n",
    "               name,\n",
    "               method='word',\n",
    "               vocab_size=None,\n",
    "               datasets: Union[List[str], List[List[str]]] = None,\n",
    "               additional_special_symbols=None,\n",
    "               ignore_case=False,\n",
    "               save_root=None,\n",
    "               index_offset=None,\n",
    "               load_if_saved=True) -> Vocab:\n",
    "    vocab_name = '.'.join(\n",
    "        [sub_name for sub_name in [name, method, str(vocab_size), str(index_offset)] if sub_name is not None])\n",
    "    path = os.path.join(vocab_save_dir, f'{vocab_name}.pk')\n",
    "    if load_if_saved:\n",
    "        if os.path.exists(path) and os.path.isfile(path):\n",
    "            logger.info(f'Trying to load saved binary pickle file from: {path}')\n",
    "            with open(path, mode='rb') as f:\n",
    "                obj = pickle.load(f)\n",
    "            assert isinstance(obj, Vocab)\n",
    "            if save_root:\n",
    "                obj.save(save_root)\n",
    "            return obj\n",
    "    vocab = Vocab(name=name,\n",
    "                  method=method,\n",
    "                  vocab_size=vocab_size,\n",
    "                  datasets=datasets,\n",
    "                  additional_special_symbols=additional_special_symbols,\n",
    "                  ignore_case=ignore_case,\n",
    "                  save_root=save_root,\n",
    "                  index_offset=index_offset)\n",
    "    vocab.save_pickle(path)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e12afba",
   "metadata": {},
   "source": [
    "Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70ada8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/spt-code/sources')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76753fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.antlr_parsers.java.Java8Lexer import Java8Lexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2571e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from antlr4 import InputStream\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07c3c08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "STRING_MATCHING_PATTERN = re.compile(r'([bruf]*)(\\\"\\\"\\\"|\\'\\'\\'|\\\"|\\')(?:(?!\\2)(?:\\\\.|[^\\\\]))*\\2')\n",
    "NON_SPACE_MATCHING_PATTERN = re.compile(r'\\S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "db214bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPPING_LANG_LEXER = {\n",
    "    LANG_JAVA: Java8Lexer\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53817ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "main_args = None\n",
    "\n",
    "def set_args(args):\n",
    "    global main_args\n",
    "    main_args = args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9cf15a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eval_lines(path):\n",
    "    \"\"\"\n",
    "    Load and eval lines from given path.\n",
    "\n",
    "    Args:\n",
    "        path (str): Dataset file path\n",
    "\n",
    "    Returns:\n",
    "        list: List of lines\n",
    "\n",
    "    \"\"\"\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        lines = [eval(line.strip()) for line in f]\n",
    "    return lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "614dcc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_eval_list_lines(path):\n",
    "    \"\"\"\n",
    "    Load and eval lines from given path, each line is a list that will be convert into a string.\n",
    "\n",
    "    Args:\n",
    "        path (str): Dataset file path\n",
    "\n",
    "    Returns:\n",
    "        list: List of lines\n",
    "\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            tokens = eval(line.strip())\n",
    "            string = ' '.join(tokens)\n",
    "            string = re.sub(r'\\s+', ' ', string)\n",
    "            lines.append(string)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "82400626",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_lines(path):\n",
    "    \"\"\"\n",
    "    Load lines from given path.\n",
    "\n",
    "    Args:\n",
    "        path (str): Dataset file path\n",
    "\n",
    "    Returns:\n",
    "        list: List of lines\n",
    "\n",
    "    \"\"\"\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        lines = [line.strip() for line in f]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f4a82f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_method_name(full_name):\n",
    "    \"\"\"\n",
    "    Extract method/function name from its full name,\n",
    "    e.g., RpcResponseResolver.resolveResponseObject -> resolveResponseObject\n",
    "\n",
    "    Args:\n",
    "        full_name (str): Full name\n",
    "\n",
    "    Returns:\n",
    "        str: Method/Function name\n",
    "\n",
    "    \"\"\"\n",
    "    point_pos = full_name.rfind('.')\n",
    "    if point_pos != -1:\n",
    "        return full_name[point_pos + 1:]\n",
    "    else:\n",
    "        return full_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1db5ea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def replace_string_literal(source):\n",
    "    \"\"\"\n",
    "    Replace the string literal in source code with ``<STR>``.\n",
    "\n",
    "    Args:\n",
    "        source (str): Source code in string\n",
    "\n",
    "    Returns:\n",
    "        str: Code after replaced\n",
    "\n",
    "    \"\"\"\n",
    "    return re.sub(pattern=STRING_MATCHING_PATTERN, repl='___STR', string=source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "892fe0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tokenize\n",
    "from io import StringIO\n",
    "\n",
    "def remove_comments_and_docstrings(source, lang):\n",
    "    \"\"\"\n",
    "    Remove docs and comments from source string.\n",
    "    Thanks to authors of GraphCodeBERT\n",
    "    from: https://github.com/microsoft/CodeBERT/blob/master/GraphCodeBERT/codesearch/parser/utils.py#L4\n",
    "\n",
    "    Args:\n",
    "        source (str): Source code string\n",
    "        lang (str): Source code language\n",
    "\n",
    "    Returns:\n",
    "        str: Source string\n",
    "\n",
    "    \"\"\"\n",
    "    if lang == LANG_PYTHON:\n",
    "        try:\n",
    "            io_obj = StringIO(source)\n",
    "            out = \"\"\n",
    "            prev_token_type = tokenize.INDENT\n",
    "            last_lineno = -1\n",
    "            last_col = 0\n",
    "            for tok in tokenize.generate_tokens(io_obj.readline):\n",
    "                token_type = tok[0]\n",
    "                token_string = tok[1]\n",
    "                start_line, start_col = tok[2]\n",
    "                end_line, end_col = tok[3]\n",
    "                # l_text = tok[4]\n",
    "                if start_line > last_lineno:\n",
    "                    last_col = 0\n",
    "                if start_col > last_col:\n",
    "                    out += (\" \" * (start_col - last_col))\n",
    "                # Remove comments:\n",
    "                if token_type == tokenize.COMMENT:\n",
    "                    pass\n",
    "                # This series of conditionals removes docstrings:\n",
    "                elif token_type == tokenize.STRING:\n",
    "                    if prev_token_type != tokenize.INDENT:\n",
    "                        # This is likely a docstring; double-check we're not inside an operator:\n",
    "                        if prev_token_type != tokenize.NEWLINE:\n",
    "                            if start_col > 0:\n",
    "                                out += token_string\n",
    "                else:\n",
    "                    out += token_string\n",
    "                prev_token_type = token_type\n",
    "                last_col = end_col\n",
    "                last_lineno = end_line\n",
    "            temp = []\n",
    "            for x in out.split('\\n'):\n",
    "                if x.strip() != \"\":\n",
    "                    temp.append(x)\n",
    "            return '\\n'.join(temp)\n",
    "        except Exception:\n",
    "            return source\n",
    "    elif lang in [LANG_RUBY]:\n",
    "        return source\n",
    "    else:\n",
    "        def replacer(match):\n",
    "            s = match.group(0)\n",
    "            if s.startswith('/'):\n",
    "                return \" \"  # note: a space and not an empty string\n",
    "            else:\n",
    "                return s\n",
    "\n",
    "        pattern = re.compile(\n",
    "            r'//.*?$|/\\*.*?\\*/|\\'(?:\\\\.|[^\\\\\\'])*\\'|\"(?:\\\\.|[^\\\\\"])*\"',\n",
    "            re.DOTALL | re.MULTILINE\n",
    "        )\n",
    "        temp = []\n",
    "        for x in re.sub(pattern, replacer, source).split('\\n'):\n",
    "            if x.strip() != \"\":\n",
    "                temp.append(x)\n",
    "        return '\\n'.join(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e695d85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def parse_json_file(file, lang):\n",
    "    \"\"\"\n",
    "    Parse a dataset file where each line is a json string representing a sample.\n",
    "\n",
    "    Args:\n",
    "        file (str): The file path\n",
    "        lang (str): Source code language\n",
    "\n",
    "    Returns:\n",
    "        (list[str], list[str], list[str], list[str], List[str]):\n",
    "            - List of source codes\n",
    "            - List of tokenized codes\n",
    "            - List of split method names\n",
    "            - List of tokenized codes with method name replaced with ``f``\n",
    "            - List of docstring strings, not every sample has it\n",
    "\n",
    "    \"\"\"\n",
    "    sources = []\n",
    "    codes = []\n",
    "    names = []\n",
    "    codes_wo_name = [] \n",
    "    docs = []\n",
    "\n",
    "    # #######################################################################\n",
    "    # Updated to reduce the time to parse, myoungkyu song, 03/23/2024\n",
    "    if main_args.parse_subset_ratio:\n",
    "        lines_to_extract = 0\n",
    "        line_counter = 0\n",
    "        total_lines = 0\n",
    "\n",
    "        with open(file, encoding='utf-8') as f:\n",
    "            total_lines = sum(1 for _ in f)\n",
    "            lines_to_extract = int(total_lines * main_args.parse_subset_ratio)\n",
    "\n",
    "        # if total_lines > 10_000:\n",
    "        #     lines_to_extract = int(lines_to_extract * main_args.parse_subset_ratio)\n",
    "        if total_lines > 100_000:\n",
    "            lines_to_extract = int(lines_to_extract * main_args.parse_subset_ratio)\n",
    "\n",
    "        logger.info('*' * 100)\n",
    "        logger.info(f'{lang} => The size of trimmed / original pre_train set to parse: {lines_to_extract} / {total_lines}')\n",
    "    # #######################################################################\n",
    "\n",
    "    with open(file, encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            if main_args.parse_subset_ratio:\n",
    "                if line_counter > lines_to_extract:\n",
    "                    break\n",
    "                line_counter += 1\n",
    "\n",
    "            data = json.loads(line.strip())\n",
    "            name = trim_method_name(data['func_name'])\n",
    "            source = data['code'].strip()\n",
    "            source = remove_comments_and_docstrings(source, lang)\n",
    "            source = replace_string_literal(source)\n",
    "            code = replace_string_literal(' '.join(data['code_tokens']))\n",
    "\n",
    "            sources.append(source)\n",
    "            codes.append(code)\n",
    "\n",
    "            code_wo_name = code.replace(name, 'f', 1)\n",
    "            codes_wo_name.append(code_wo_name)\n",
    "\n",
    "            name = ' '.join(split_identifier(name))\n",
    "            names.append(name)\n",
    "\n",
    "            if 'docstring' in data:\n",
    "                doc = clean_doc(data['docstring'])\n",
    "                if doc:\n",
    "                    docs.append(doc)\n",
    "\n",
    "    return sources, codes, names, codes_wo_name, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "344b5fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_all_files(base):\n",
    "    \"\"\"\n",
    "    Iterator for all file paths in the given base path.\n",
    "\n",
    "    Args:\n",
    "        base (str): Path like string\n",
    "\n",
    "    Returns:\n",
    "        str: Path of each file\n",
    "    \"\"\"\n",
    "    for root, ds, fs in os.walk(base):\n",
    "        for f in fs:\n",
    "            yield os.path.join(root, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1f5ac51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def iter_pre_train_dataset_files(lang_dir, lang):\n",
    "    \"\"\"\n",
    "    Get files for pre-training, all files with extension ``jsonl`` will be included.\n",
    "\n",
    "    Args:\n",
    "        lang_dir (str): Path of language dir\n",
    "        lang (str): Source code language\n",
    "\n",
    "    Returns:\n",
    "        list[str]: List of paths of files\n",
    "\n",
    "    \"\"\"\n",
    "    # if lang in [enums.LANG_PYTHON]:\n",
    "    #     for file in iter_all_files(base=lang_dir):\n",
    "    #         if file.endswith('.jsonl'):\n",
    "    #             return [file]\n",
    "    # if lang in [enums.LANG_PYTHON]:\n",
    "    #     return [file for file in iter_all_files(base=lang_dir) if file.endswith('.jsonl')]\n",
    "    if lang in [LANG_GO, LANG_JAVA, LANG_PYTHON, LANG_JAVASCRIPT, LANG_PHP,\n",
    "                LANG_RUBY]:\n",
    "        return [file for file in iter_all_files(base=lang_dir) if file.endswith('.jsonl')]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3c5c5286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pre_train_dataset(file, lang):\n",
    "    \"\"\"\n",
    "    Load json dataset from given file.\n",
    "\n",
    "    Args:\n",
    "        file (str): Path of dataset file\n",
    "        lang (str): Source code language\n",
    "\n",
    "    Returns:\n",
    "        (list[str], list[str], list[str], list[str], list[str]):\n",
    "            - List of source code strings\n",
    "            - List of tokenized code strings\n",
    "            - List of nl strings\n",
    "            - List of tokenized code strings with method names replaced\n",
    "            - List of doc strings, not every sample has it\n",
    "\n",
    "    \"\"\"\n",
    "    if lang in [LANG_JAVA, LANG_PYTHON, LANG_GO,\n",
    "                LANG_JAVASCRIPT, LANG_PHP, LANG_RUBY]:\n",
    "        sources, codes, names, codes_wo_name, docs = parse_json_file(file, lang=lang)\n",
    "        return sources, codes, names, codes_wo_name, docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2a083fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_dataset_from_dir(dataset_dir):\n",
    "    \"\"\"\n",
    "    Load all files in the given dir, only for pre-training.\n",
    "\n",
    "    Args:\n",
    "        dataset_dir (str): Root directory\n",
    "\n",
    "    Returns:\n",
    "        (dict, list[str], list[str], list[str], List[str], list[str], list[str], list[str], list[str], list[str]):\n",
    "            - Dict of paths: key is the dataset group, value is the path\n",
    "            - List of str: languages for each line\n",
    "            - List of str: source code\n",
    "            - List of str: tokenized code string\n",
    "            - List of ast: linearized ast string\n",
    "            - List of str: split method name string\n",
    "            - List of str:\n",
    "            - List of str:\n",
    "            - List of str:\n",
    "            - List of str: List of docs\n",
    "\n",
    "    \"\"\"\n",
    "    paths = {}\n",
    "    languages = []\n",
    "    all_sources = []\n",
    "    all_asts = []\n",
    "    all_codes = []\n",
    "    all_codes_wo_name = []\n",
    "    all_names = []\n",
    "    all_names_wo_name = []\n",
    "    all_only_names = []\n",
    "    all_docs = []\n",
    "\n",
    "    if not os.path.exists(dataset_dir):\n",
    "        logger.info('-' * 100)\n",
    "        full_path_dataset_dir = os.path.abspath(dataset_dir)\n",
    "        check_exist = os.path.exists(full_path_dataset_dir)\n",
    "        logger.info('Directory Not Exist: %s', dataset_dir)\n",
    "        logger.info('-' * 100)\n",
    "        sys.exit()\n",
    "\n",
    "    for file in os.listdir(dataset_dir):\n",
    "\n",
    "        path = os.path.join(dataset_dir, file)\n",
    "        if os.path.isfile(path):\n",
    "            continue\n",
    "\n",
    "        lang = file\n",
    "        dataset_files = iter_pre_train_dataset_files(path, lang=lang)\n",
    "        if len(dataset_files) > 0:\n",
    "            logger.info(f'  Language: {lang}')\n",
    "            paths[lang] = dataset_files\n",
    "            n_sample = 0\n",
    "            for dataset_file_path in dataset_files:\n",
    "                sources, codes, names, codes_wo_name, docs = load_pre_train_dataset(file=dataset_file_path,\n",
    "                                                                                    lang=lang)\n",
    "\n",
    "                new_sources = []\n",
    "                new_codes = []\n",
    "                new_codes_wo_name = []\n",
    "                new_names = []\n",
    "                new_names_wo_name = []\n",
    "                only_names = []\n",
    "                asts = []\n",
    "                for source, code, name, code_wo_name in tqdm(zip(sources, codes, names, codes_wo_name),\n",
    "                                                             desc=f'Parsing {os.path.basename(dataset_file_path)}',\n",
    "                                                             leave=False,\n",
    "                                                             total=len(sources)):\n",
    "                    try:\n",
    "                        ast, nl, nl_wo_name = generate_single_ast_nl(source=source,\n",
    "                                                                     lang=lang,\n",
    "                                                                     name=name,\n",
    "                                                                     replace_method_name=True)\n",
    "                        new_sources.append(source)\n",
    "                        new_codes.append(code)\n",
    "                        new_codes_wo_name.append(code_wo_name)\n",
    "                        new_names.append(nl)\n",
    "                        new_names_wo_name.append(nl_wo_name)\n",
    "                        asts.append(ast)\n",
    "                        only_names.append(name)\n",
    "                    except Exception:\n",
    "                        continue\n",
    "\n",
    "                all_sources += new_sources\n",
    "                all_codes += new_codes\n",
    "                all_codes_wo_name += new_codes_wo_name\n",
    "                all_names += new_names\n",
    "                all_names_wo_name += new_names_wo_name\n",
    "                all_only_names += only_names\n",
    "                all_asts += asts\n",
    "                all_docs += docs\n",
    "\n",
    "                n_line = len(new_sources)\n",
    "                languages += [lang for _ in range(n_line)]\n",
    "                n_sample += n_line\n",
    "\n",
    "                logger.info(f'    File: {dataset_file_path}, {n_line} samples')\n",
    "\n",
    "            logger.info(f'  {lang} dataset size: {n_sample}')\n",
    "\n",
    "    assert len(languages) == len(all_sources) == len(all_codes) == len(all_codes_wo_name) == len(all_asts) == \\\n",
    "           len(all_names) == len(all_names_wo_name) == len(all_only_names)\n",
    "    return paths, languages, all_sources, all_codes, all_asts, all_names, all_codes_wo_name, all_names_wo_name, \\\n",
    "           all_only_names, all_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4dce2ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded lines: ['public static double Sinc(double x) {\\\\r\\\\n        return Math.sin(Math.PI * x) / (Math.PI * x);\\\\r\\\\n    }', 'protected void modify(Transaction t) {\\\\n        try {\\\\n            this.lock.writeLock().lock();\\\\n            t.perform();\\\\n        } finally {\\\\n            this.lock.writeLock().unlock();\\\\n        }\\\\n    }', 'protected <E> E read(Supplier<E> sup) {\\\\n        try {\\\\n            this.lock.readLock().lock();\\\\n            return sup.get();\\\\n        } finally {\\\\n            this.lock.readLock().unlock();\\\\n        }\\\\n    }']\n",
      "Trimmed method name: ClassLoader getBootstrapClassLoader\n",
      "Replaced string literals: void pushBaseIndentifier(String baseID)\n",
      "  {\n",
      "\n",
      "    if (null != baseID)\n",
      "    {\n",
      "      int posOfHash = baseID.indexOf(___STR);\n",
      "\n",
      "      if (posOfHash > -1)\n",
      "      {\n",
      "        m_fragmentIDString = baseID.substring(posOfHash + 1);\n",
      "        m_shouldProcess = false;\n",
      "      }\n",
      "      else\n",
      "        m_shouldProcess = true;\n",
      "    }\n",
      "    else\n",
      "      m_shouldProcess = true;\n",
      "\n",
      "    m_baseIdentifiers.push(baseID);\n",
      "  }\n",
      "Cleaned source code: void pushBaseIndentifier(String baseID)\n",
      "  {\n",
      "    if (null != baseID)\n",
      "    {\n",
      "      int posOfHash = baseID.indexOf('#');\n",
      "      if (posOfHash > -1)\n",
      "      {\n",
      "        m_fragmentIDString = baseID.substring(posOfHash + 1);\n",
      "        m_shouldProcess = false;\n",
      "      }\n",
      "      else\n",
      "        m_shouldProcess = true;\n",
      "    }\n",
      "    else\n",
      "      m_shouldProcess = true;\n",
      "    m_baseIdentifiers.push(baseID);\n",
      "  }\n"
     ]
    }
   ],
   "source": [
    "# Load and print the lines from a file\n",
    "lines = load_lines(\"/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/pre_train/java/train/sourcetest1.code\")\n",
    "print(\"Loaded lines:\", lines)\n",
    "\n",
    "# Trim the method name and print\n",
    "full_name = \"ClassLoader getBootstrapClassLoader\"\n",
    "trimmed_name = trim_method_name(full_name)\n",
    "print(\"Trimmed method name:\", trimmed_name)\n",
    "\n",
    "# Replace string literals and print\n",
    "source_code = \"void pushBaseIndentifier(String baseID)\\n  {\\n\\n    if (null != baseID)\\n    {\\n      int posOfHash = baseID.indexOf('#');\\n\\n      if (posOfHash > -1)\\n      {\\n        m_fragmentIDString = baseID.substring(posOfHash + 1);\\n        m_shouldProcess = false;\\n      }\\n      else\\n        m_shouldProcess = true;\\n    }\\n    else\\n      m_shouldProcess = true;\\n\\n    m_baseIdentifiers.push(baseID);\\n  }\"\n",
    "replaced_source = replace_string_literal(source_code)\n",
    "print(\"Replaced string literals:\", replaced_source)\n",
    "\n",
    "# Remove comments and docstrings from source code and print\n",
    "cleaned_source = remove_comments_and_docstrings(source_code, lang=\"java\")\n",
    "print(\"Cleaned source code:\", cleaned_source)\n",
    "\n",
    "# # Parse a JSON file and print\n",
    "# parsed_data = parse_json_file(\"/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/pre_train/java/train.jsonl\", lang=\"java\")\n",
    "# print(\"Parsed data:\", parsed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "45812d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def trim_spaces(string):\n",
    "    \"\"\"\n",
    "    Replace consecutive spaces with a single whitespace.\n",
    "\n",
    "    Args:\n",
    "        string (str): String\n",
    "\n",
    "    Returns:\n",
    "        str: Replaced string\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\s+', ' ', string).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fcd96842",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_python(source):\n",
    "    \"\"\"\n",
    "    Python lib to tokenize python source code.\n",
    "\n",
    "    Args:\n",
    "        source (str): Source code string\n",
    "\n",
    "    Returns:\n",
    "        str: Tokenized code string\n",
    "\n",
    "    \"\"\"\n",
    "    tokens = tokenize.generate_tokens(StringIO(source).readline)\n",
    "    return ' '.join([token.string for token in tokens if token.string.strip() != ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1ad23dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_source(source, lang, use_regular=False):\n",
    "    \"\"\"\n",
    "    Tokenize the source code into tokens.\n",
    "\n",
    "    Args:\n",
    "        source (str): Source in string\n",
    "        lang (str): Language of source code\n",
    "        use_regular (bool): Whether to use regular tokenize method, default to False\n",
    "\n",
    "    Returns:\n",
    "        str: Tokenized code, delimited by whitespace, string literal will be replaced by ``___STR``\n",
    "\n",
    "    \"\"\"\n",
    "    if use_regular:\n",
    "        code = replace_string_literal(regular_tokenize(source))\n",
    "        return trim_spaces(code)\n",
    "    if lang == LANG_PYTHON:\n",
    "        tokens = tokenize.generate_tokens(StringIO(source).readline)\n",
    "        code = ' '.join([token.string for token in tokens])\n",
    "        code = replace_string_literal(code)\n",
    "        return trim_spaces(code)\n",
    "    if lang in [LANG_JAVA, LANG_JAVASCRIPT, LANG_PHP, LANG_GO]:\n",
    "        input_stream = InputStream(source)\n",
    "        lexer = MAPPING_LANG_LEXER[lang](input_stream)\n",
    "        tokens = [token.text for token in lexer.getAllTokens()]\n",
    "        code = replace_string_literal(' '.join(tokens))\n",
    "        return trim_spaces(code)\n",
    "    elif lang == LANG_RUBY:\n",
    "        tokens = MAPPING_LANG_LEXER[lang].get_pure_tokens(source)\n",
    "        code = replace_string_literal(' '.join([token[0] for token in tokens]))\n",
    "        return trim_spaces(code)\n",
    "    else:\n",
    "        # TODO: c# tokenize\n",
    "        code = replace_string_literal(regular_tokenize(source))\n",
    "        return trim_spaces(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8bd8c47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_non_space_chars(s):\n",
    "    \"\"\"\n",
    "    Count the non-space characters.\n",
    "\n",
    "    Args:\n",
    "        s (str): String to be counted\n",
    "\n",
    "    Returns:\n",
    "        int: Number of non-space characters\n",
    "\n",
    "    \"\"\"\n",
    "    matches = re.findall(NON_SPACE_MATCHING_PATTERN, s)\n",
    "    return len(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8d60250f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def align_source_code(former_source, code):\n",
    "    \"\"\"\n",
    "    Align former source to code token string and split code into former one and latter one.\n",
    "\n",
    "    Args:\n",
    "        former_source (str): Former part of the source\n",
    "        code (str): Tokenized source code\n",
    "\n",
    "    Returns:\n",
    "        (str, str):\n",
    "            - Former part of tokenized code\n",
    "            - Latter part of tokenized code\n",
    "\n",
    "    \"\"\"\n",
    "    former_count = count_non_space_chars(former_source)\n",
    "    total = 0\n",
    "    code_tokens = code.split(' ')\n",
    "    token_index = 0\n",
    "    while total < former_count:\n",
    "        total += count_non_space_chars(code_tokens[token_index])\n",
    "        token_index += 1\n",
    "    former_code = ' '.join(code_tokens[:token_index])\n",
    "    latter_code = ' '.join(code_tokens[token_index:])\n",
    "    return former_code, latter_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f8645615",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def regular_tokenize(source: str):\n",
    "    \"\"\"\n",
    "    NLTK word tokenize with simple adoptions for source code.\n",
    "\n",
    "    Args:\n",
    "        source (str): Source code string.\n",
    "\n",
    "    Returns:\n",
    "        str: Tokenized code string\n",
    "    \"\"\"\n",
    "    source = re.sub(r'(\\S)[.=](\\S)', r'\\1 . \\2', source)\n",
    "    return ' '.join(nltk.word_tokenize(source))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0b563ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_doc(s):\n",
    "    \"\"\"\n",
    "    Clean docstring.\n",
    "\n",
    "    Args:\n",
    "        s (str): Raw docstring\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned docstring\n",
    "\n",
    "    \"\"\"\n",
    "    # // Create an instance of  {@link RepresentationBaseType } and {@link RepresentationBaseType }.\n",
    "    # // Create an instance of RepresentationBaseType and RepresentationBaseType\n",
    "    # // Public setter for the  {@code rowMapper}.\n",
    "    # // Public setter for the rowMapper\n",
    "    # comment = comment.replaceAll(\"\\\\{@link|code(.*?)}\", \"$1\");\n",
    "    # comment = comment.replaceAll(\"@see\", \"\");\n",
    "\n",
    "    s = re.sub(r'{@link|code(.*?)}', r'\\1', s)\n",
    "    s = re.sub(r'@see', '', s)\n",
    "\n",
    "    # // Implementation of the <a href=\"http://www.tarsnap.com/scrypt/scrypt.pdf\"/>scrypt KDF</a>.\n",
    "    # // Implementation of the scrypt KDF\n",
    "    # comment = comment.replaceAll(\"<a.*?>(.*?)a>\", \"$1\");\n",
    "    s = re.sub(r'<a.*?>(.*?)a>', r'\\1', s)\n",
    "\n",
    "    # // remove all tags like <p>, </b>\n",
    "    # comment = comment.replaceAll(\"</?[A-Za-z0-9]+>\", \"\");\n",
    "    s = re.sub(r'</?[A-Za-z0-9]+>', '', s)\n",
    "\n",
    "    # // Set the list of the watchable objects (meta data).\n",
    "    # // Set the list of the watchable objects\n",
    "    # comment = comment.replaceAll(\"\\\\(.*?\\\\)\", \"\");\n",
    "    s = re.sub(r'\\(.*?\\)', '', s)\n",
    "\n",
    "    # // #dispatchMessage dispatchMessage\n",
    "    # // dispatchMessage\n",
    "    # comment = comment.replaceAll(\"#([\\\\w]+)\\\\s+\\\\1\", \"$1\");\n",
    "    s = re.sub(r'#([\\w]+)\\s+\\1', r'\\1', s)\n",
    "\n",
    "    # // remove http url\n",
    "    # comment = comment.replaceAll(\"http\\\\S*\", \"\");\n",
    "    s = re.sub(r'http\\S*', '', s)\n",
    "\n",
    "    # // characters except english and number are ignored.\n",
    "    # comment = comment.replaceAll(\"[^a-zA-Z0-9_]\", \" \");\n",
    "    s = re.sub(r'[^a-zA-Z0-9_]', ' ', s)\n",
    "\n",
    "    # // delete empty symbols\n",
    "    # comment = comment.replaceAll(\"[ \\f\\n\\r\\t]\", \" \").trim();\n",
    "    # comment = comment.replaceAll(\" +\", \" \");\n",
    "    s = re.sub(r'[ \\f\\n\\r\\t]', ' ', s).strip()\n",
    "    s = re.sub(r' +', ' ', s).strip()\n",
    "\n",
    "    if len(s) == 0 or len(s.split()) < 3:\n",
    "        return None\n",
    "    else:\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "555cd01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_python_source_classical_summarization(source: str):\n",
    "    source = re.sub(r' *DCNL *', '\\n', source)\n",
    "    source = re.sub(r' *DCSP *', '\\t', source)\n",
    "    return source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1572574c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed string: public void put(String hostname, int netId, InetAddress[] addresses ) { cache.put(new AddressCacheKey(hostname, netId), new AddressCacheEntry(addresses)); }\n",
      "Tokenized Python code: public void put ( String hostname , int netId , InetAddress [ ] addresses ) { cache . put ( new AddressCacheKey ( hostname , netId ) , new AddressCacheEntry ( addresses ) ) ; }\n",
      "Tokenized code: public void put ( String hostname , int netId , InetAddress [ ] addresses ) { cache . put ( new AddressCacheKey ( hostname , netId ) , new AddressCacheEntry ( addresses ) ) ; }\n",
      "Count of non-space characters: 141\n",
      "Aligned former code: public static double Sinc ( double x ) { return Math . sin ( Math . PI * x ) / ( Math . PI * x ) ; }\n",
      "Latter code: \n",
      "Regular tokenized code: public void put ( String hostname , int netId , InetAddress [ ] addresses ) { cache . put ( new AddressCacheKey ( hostname , netId ) , new AddressCacheEntry ( addresses ) ) ; }\n",
      "Cleaned docstring: Gets the proper modulus operation param x Integer param m Modulo return Modulus\n",
      "Converted source code: \n",
      "def hello_world():\t\n",
      "print('Hello, world!')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Trim spaces in a string and print\n",
    "trimmed_string = trim_spaces(\"public void put(String hostname, int netId, InetAddress[] addresses ) {\\n        cache.put(new AddressCacheKey(hostname, netId), new AddressCacheEntry(addresses));\\n    }\")\n",
    "print(\"Trimmed string:\", trimmed_string)\n",
    "\n",
    "# Tokenize Python source code and print\n",
    "source_code = \"public void put(String hostname, int netId, InetAddress[] addresses ) {\\n        cache.put(new AddressCacheKey(hostname, netId), new AddressCacheEntry(addresses));\\n    }\"\n",
    "tokenized_code = tokenize_python(source_code)\n",
    "print(\"Tokenized Python code:\", tokenized_code)\n",
    "\n",
    "# Tokenize source code and print\n",
    "source_code = \"public void put(String hostname, int netId, InetAddress[] addresses ) {\\n        cache.put(new AddressCacheKey(hostname, netId), new AddressCacheEntry(addresses));\\n    }\"\n",
    "tokenized_code = tokenize_source(source_code, lang=\"python\")\n",
    "print(\"Tokenized code:\", tokenized_code)\n",
    "\n",
    "# Count non-space characters in a string and print\n",
    "count = count_non_space_chars(\"public void put(String hostname, int netId, InetAddress[] addresses ) {\\n        cache.put(new AddressCacheKey(hostname, netId), new AddressCacheEntry(addresses));\\n    }\")\n",
    "print(\"Count of non-space characters:\", count)\n",
    "\n",
    "# Align source code and print\n",
    "former_source = \"public static double Sinc(double x) {\\r\\n        return Math.sin(Math.PI * x) / (Math.PI * x);\\r\\n    }\"\n",
    "code = \"public static double Sinc ( double x ) { return Math . sin ( Math . PI * x ) / ( Math . PI * x ) ; }\"\n",
    "aligned_former_code, latter_code = align_source_code(former_source, code)\n",
    "print(\"Aligned former code:\", aligned_former_code)\n",
    "print(\"Latter code:\", latter_code)\n",
    "\n",
    "# Tokenize source code using NLTK and print\n",
    "source_code = \"public void put(String hostname, int netId, InetAddress[] addresses ) {\\n        cache.put(new AddressCacheKey(hostname, netId), new AddressCacheEntry(addresses));\\n    }\"\n",
    "tokenized_code = regular_tokenize(source_code)\n",
    "print(\"Regular tokenized code:\", tokenized_code)\n",
    "\n",
    "# Clean docstring and print\n",
    "docstring = \"Gets the proper modulus operation.\\n\\n@param x Integer.\\n@param m Modulo.\\n@return Modulus.\"\n",
    "cleaned_docstring = clean_doc(docstring)\n",
    "print(\"Cleaned docstring:\", cleaned_docstring)\n",
    "\n",
    "# Convert Python source code for classical summarization and print\n",
    "source_code = \"DCNLdef hello_world():DCSPDCNLprint('Hello, world!')DCNL\"\n",
    "converted_source = convert_python_source_classical_summarization(source_code)\n",
    "print(\"Converted source code:\", converted_source)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5e1a1df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_for_summarization(source_path, code_path, nl_path, lang):\n",
    "    \"\"\"\n",
    "    Load and parse dataset for code summarization.\n",
    "\n",
    "    Args:\n",
    "        source_path (str): Path of source code dataset\n",
    "        code_path (str): Path of tokenized code dataset, if not file not exist, tokenize on the fly\n",
    "        nl_path (str): Path of comment dataset\n",
    "        lang (str): Source code language\n",
    "\n",
    "    Returns:\n",
    "        (Dict, list[str], list[str], list[str], list[str]):\n",
    "            - Dict mapping dataset groups to paths\n",
    "            - List of tokenized code strings\n",
    "            - List of linearized AST strings\n",
    "            - List of name and API sequence strings\n",
    "            - List of comment strings\n",
    "\n",
    "    \"\"\"\n",
    "    paths = {'source': source_path}\n",
    "    logger.info(f'    Source code file: {source_path}')\n",
    "    sources = load_lines(source_path)\n",
    "    # if lang == enums.LANG_PYTHON:\n",
    "    #     sources = [convert_python_source_classical_summarization(source) for source in sources]\n",
    "\n",
    "    if not os.path.isfile(code_path):\n",
    "        paths['code'] = source_path\n",
    "        logger.info('    Tokenize source code')\n",
    "        codes = [tokenize_source(source, lang=lang) for source in sources]\n",
    "    else:\n",
    "        paths['code'] = code_path\n",
    "        logger.info(f'    Tokenized code file: {code_path}')\n",
    "        codes = load_lines(code_path)\n",
    "    paths['nl'] = nl_path\n",
    "    logger.info(f'    Summarization file: {nl_path}')\n",
    "    nls = load_lines(nl_path)\n",
    "    # sources, codes, nls = sources[:1000], codes[:1000], nls[:1000]\n",
    "    assert len(sources) == len(codes) == len(nls)\n",
    "\n",
    "    new_codes = []\n",
    "    new_nls = []\n",
    "    names = []\n",
    "    asts = []\n",
    "    for source, code, nl in tqdm(zip(sources, codes, nls), desc='Parsing', leave=False, total=len(sources)):\n",
    "        try:\n",
    "            source = remove_comments_and_docstrings(source, lang=lang)\n",
    "            ast, name = generate_single_ast_nl(source=source, lang=lang) #It generates the AST and method name for each source code using the generate_single_ast_nl function.\n",
    "            new_codes.append(code)\n",
    "            new_nls.append(nl)\n",
    "            names.append(name)\n",
    "            asts.append(ast)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return paths, new_codes, asts, names, new_nls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5edef6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed paths: {'source': '/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/pre_train/java/train/sourcetest1.code', 'code': '/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/pre_train/java/train/token1.code', 'nl': '/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/pre_train/java/train/raw1.docstring'}\n",
      "Parsed codes: ['public static double Sinc ( double x ) { return Math . sin ( Math . PI * x ) / ( Math . PI * x ) ; }', 'protected void modify ( Transaction t ) { try { this . lock . writeLock ( ) . lock ( ) ; t . perform ( ) ; } finally { this . lock . writeLock ( ) . unlock ( ) ; } }', 'protected < E > E read ( Supplier < E > sup ) { try { this . lock . readLock ( ) . lock ( ) ; return sup . get ( ) ; } finally { this . lock . readLock ( ) . unlock ( ) ; } }']\n",
      "Parsed asts: ['return_statement__ binary_expression__ binary_expression parenthesized_expression__ binary_expression __parenthesized_expression __binary_expression __return_statement', 'try_statement__ expression_statement expression_statement expression_statement __try_statement', 'try_statement__ expression_statement return_statement expression_statement __try_statement']\n",
      "Parsed names: ['sinc h s', 'modify k write lo lo t perf lock writ ck u', 'read k read lo lo up lock rea ck u']\n",
      "Parsed nls: ['Executes the given transaction within the context of a write lock.\\\\n\\\\n@param t The transaction to execute.', 'Executes the given supplier within the context of a read lock.\\\\n\\\\n@param <E> The result type.\\\\n@param sup The supplier.\\\\n@return The result of {@link Supplier#get()}.', 'Sinc function.\\\\n\\\\n@param x Value.\\\\n@return Sinc of the value.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Parse for summarization and print\n",
    "paths, codes, asts, names, nls = parse_for_summarization(\"/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/pre_train/java/train/sourcetest1.code\", \"/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/pre_train/java/train/token1.code\", \"/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/pre_train/java/train/raw1.docstring\", lang=\"java\")\n",
    "print(\"Parsed paths:\", paths)\n",
    "print(\"Parsed codes:\", codes)\n",
    "print(\"Parsed asts:\", asts)\n",
    "print(\"Parsed names:\", names)\n",
    "print(\"Parsed nls:\", nls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dd385d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_for_translation(source_path, source_lang, target_path, target_lang):\n",
    "    \"\"\"\n",
    "    Load and parse for code translation.\n",
    "\n",
    "    Args:\n",
    "        source_path (str): Path of source dataset\n",
    "        source_lang (str): Source language\n",
    "        target_path (str): Path of target dataset\n",
    "        target_lang (str): Target language\n",
    "\n",
    "    Returns:\n",
    "        (list[str], list[str], list[str], list[str]):\n",
    "            - List of tokenized code strings\n",
    "            - List of linearized AST strings\n",
    "            - List of name and API sequence strings\n",
    "            - List of tokenized target code strings\n",
    "\n",
    "    \"\"\"\n",
    "    logger.info(f'    Source file: {source_path}')\n",
    "    sources = load_lines(source_path)\n",
    "    print(sources)\n",
    "    logger.info(f'    Target file: {target_path}')\n",
    "    targets = load_lines(target_path)\n",
    "    print(targets)\n",
    "\n",
    "    new_sources = []\n",
    "    new_targets = []\n",
    "    asts = []\n",
    "    names = []\n",
    "    for source, target in tqdm(zip(sources, targets), desc='Parsing', leave=False, total=len(sources)):\n",
    "        try:\n",
    "            source = remove_comments_and_docstrings(source, lang=source_lang)\n",
    "            source = replace_string_literal(source)\n",
    "            target = remove_comments_and_docstrings(target, lang=target_lang)\n",
    "            target = replace_string_literal(target)\n",
    "\n",
    "            ast, name = generate_single_ast_nl(source=source, lang=source_lang)\n",
    "            code = tokenize_source(source=source, lang=source_lang, use_regular=True)\n",
    "            tokenized_target = tokenize_source(source=target, lang=target_lang, use_regular=True)\n",
    "\n",
    "            new_sources.append(code)\n",
    "            asts.append(ast)\n",
    "            names.append(name)\n",
    "            new_targets.append(tokenized_target)\n",
    "        except Exception:\n",
    "            continue\n",
    "    print(asts)\n",
    "    return new_sources, asts, names, new_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['public static double Sinc(double x) {\\\\r\\\\n        return Math.sin(Math.PI * x) / (Math.PI * x);\\\\r\\\\n    }', 'protected void modify(Transaction t) {\\\\n        try {\\\\n            this.lock.writeLock().lock();\\\\n            t.perform();\\\\n        } finally {\\\\n            this.lock.writeLock().unlock();\\\\n        }\\\\n    }', 'protected <E> E read(Supplier<E> sup) {\\\\n        try {\\\\n            this.lock.readLock().lock();\\\\n            return sup.get();\\\\n        } finally {\\\\n            this.lock.readLock().unlock();\\\\n        }\\\\n    }']\n",
      "['import math \\\\n \\\\nSinc = lambda x: 1.0 if x == 0 else math.sin(math.pi * x) / (math.pi * x)']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['return_statement__ binary_expression__ binary_expression parenthesized_expression__ binary_expression __parenthesized_expression __binary_expression __return_statement']\n",
      "Parsed translation sources: ['public static double Sinc ( double x ) { \\\\r\\\\n return Math . sin ( Math . PI * x ) / ( Math . PI * x ) ; \\\\r\\\\n }']\n",
      "Parsed translation ASTs: ['return_statement__ binary_expression__ binary_expression parenthesized_expression__ binary_expression __parenthesized_expression __binary_expression __return_statement']\n",
      "Parsed translation names: ['sinc h s']\n",
      "Parsed translation targets: ['import math \\\\n \\\\nSinc = lambda x : 1 . 0 if x == 0 else math . sin ( math . pi * x ) / ( math . pi * x )']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Parse for translation and print\n",
    "source_path = \"/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/pre_train/java/train/sourcetest1.code\"\n",
    "source_lang = \"java\"\n",
    "target_path = \"/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/targetpath.code\"\n",
    "target_lang = \"python\"\n",
    "sources, asts, names, targets = parse_for_translation(source_path, source_lang, target_path, target_lang)\n",
    "print(\"Parsed translation sources:\", sources)\n",
    "print(\"Parsed translation ASTs:\", asts)\n",
    "print(\"Parsed translation names:\", names)\n",
    "print(\"Parsed translation targets:\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7ffcacba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def parse_for_search(dataset_dir, lang, split):\n",
    "    \"\"\"\n",
    "    Load and parse for code search.\n",
    "\n",
    "    Args:\n",
    "        dataset_dir (str): Directory of the dataset\n",
    "        lang (str): Source code language\n",
    "        split (str): Split set of the dataset, support `train`, `valid`, `test`, `codebase`\n",
    "\n",
    "    Returns:\n",
    "        (list[str], list[str], list[str], list[str])\n",
    "            - List of tokenized code strings\n",
    "            - List of AST sequences\n",
    "            - List of name strings\n",
    "            - List of nl strings\n",
    "\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    codes = []\n",
    "    asts = []\n",
    "    names = []\n",
    "    nls = []\n",
    "\n",
    "    print(f\"Parsing {split} dataset\")\n",
    "    path = os.path.join(dataset_dir, f'{split}.jsonl')\n",
    "\n",
    "    # #######################################################################\n",
    "    # Updated to reduce the time to parse, myoungkyu song, 03/23/2024\n",
    "    if main_args.parse_subset_ratio:\n",
    "        lines_to_extract = 0\n",
    "        line_counter = 0\n",
    "        total_lines = 0\n",
    "\n",
    "        with open(path, encoding='utf-8') as f:\n",
    "            total_lines = sum(1 for _ in f)\n",
    "            lines_to_extract = int(total_lines * main_args.parse_subset_ratio)\n",
    "\n",
    "        if total_lines > 10_000:\n",
    "            lines_to_extract = int(lines_to_extract * main_args.parse_subset_ratio)\n",
    "        if total_lines > 100_000:\n",
    "            lines_to_extract = int(lines_to_extract * main_args.parse_subset_ratio)\n",
    "\n",
    "        logger.info('*' * 100)\n",
    "        logger.info(f'{lang} => The size of trimmed / original fine tunning {split} set to parse: {lines_to_extract} / {total_lines}')\n",
    "    # #######################################################################\n",
    "\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        logger.info(f'  File: {path}')\n",
    "        for line in tqdm(f.readlines()):\n",
    "            if main_args.parse_subset_ratio: # myoungkyu song, 03/23/2024\n",
    "                if line_counter > lines_to_extract:\n",
    "                    break\n",
    "                line_counter += 1\n",
    "\n",
    "            data = json.loads(line.strip())\n",
    "            if split in ['train', 'valid', 'test']:\n",
    "                if 'docstring' not in data:\n",
    "                    continue\n",
    "                nl = clean_doc(data['docstring'])\n",
    "                if nl is None:\n",
    "                    continue\n",
    "            try:\n",
    "                if split in ['codebase', 'train']:\n",
    "                    code = replace_string_literal(' '.join(data['code_tokens']))\n",
    "                    name = trim_method_name(data['func_name'])\n",
    "\n",
    "                    source = data['code'].strip()\n",
    "                    source = remove_comments_and_docstrings(source, lang)\n",
    "                    source = replace_string_literal(source)\n",
    "                    ast, name = generate_single_ast_nl(source=source, lang=lang, name=name)\n",
    "\n",
    "                    codes.append(code)\n",
    "                    asts.append(ast)\n",
    "                    names.append(name)\n",
    "\n",
    "                if split in ['train', 'valid', 'test']:\n",
    "                    nls.append(nl)\n",
    "\n",
    "                if split != 'train':\n",
    "                    url = data['url']\n",
    "                    urls.append(url)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    if split == 'codebase':\n",
    "        return urls, codes, asts, names\n",
    "    elif split == 'train':\n",
    "        return codes, asts, names, nls\n",
    "    elif split in ['valid', 'test']:\n",
    "        return urls, nls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "17953433",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_clone_mapping(dataset_root):\n",
    "    \"\"\"\n",
    "    Load json file and transfer to a mapping from code id to source code.\n",
    "\n",
    "    Args:\n",
    "        dataset_root (str): Root of the dataset\n",
    "\n",
    "    Returns:\n",
    "        dict: Mapping from code id to source code\n",
    "\n",
    "    \"\"\"\n",
    "    path = os.path.join(dataset_root, 'fine_tune', TASK_CLONE_DETECTION, 'data.jsonl')\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    mapping = dict()\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            data = json.loads(line.strip())\n",
    "            code_id = data['idx']\n",
    "            source = data['func'].strip()\n",
    "            mapping[code_id] = source\n",
    "    return mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cf58ea54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_for_clone(path, mapping):\n",
    "    \"\"\"\n",
    "    Load and parse for code clone detection.\n",
    "\n",
    "    Args:\n",
    "        path (str): Dataset path\n",
    "        mapping (dict[int, str]): Mapping from code id to source code\n",
    "\n",
    "    Returns:\n",
    "        list[str], list[str], list[str], list[str], list[str], list[str], list[int]:\n",
    "            - List of source code 1 strings\n",
    "            - List of ast 1 strings\n",
    "            - List of name 1 strings\n",
    "            - List of source code 2 strings\n",
    "            - List of ast 2 strings\n",
    "            - List of name 2 strings\n",
    "            - List of label integers\n",
    "\n",
    "    \"\"\"\n",
    "    codes_1 = []\n",
    "    asts_1 = []\n",
    "    names_1 = []\n",
    "    codes_2 = []\n",
    "    asts_2 = []\n",
    "    names_2 = []\n",
    "    labels = []\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        for line in tqdm(f.readlines()):\n",
    "            id_1, id_2, label = line.split('\\t')\n",
    "            try:\n",
    "                source_1 = mapping[id_1]\n",
    "                source_1 = remove_comments_and_docstrings(source_1, lang=LANG_JAVA)\n",
    "                source_1 = replace_string_literal(source_1)\n",
    "                ast_1, name_1 = generate_single_ast_nl(source=source_1, lang=LANG_JAVA)\n",
    "                code_1 = tokenize_source(source=source_1, lang=LANG_JAVA)\n",
    "\n",
    "                source_2 = mapping[id_2]\n",
    "                source_2 = remove_comments_and_docstrings(source_2, lang=LANG_JAVA)\n",
    "                source_2 = replace_string_literal(source_2)\n",
    "                ast_2, name_2 = generate_single_ast_nl(source=source_2, lang=LANG_JAVA)\n",
    "                code_2 = tokenize_source(source=source_2, lang=LANG_JAVA)\n",
    "\n",
    "                label = int(label)\n",
    "\n",
    "                codes_1.append(code_1)\n",
    "                asts_1.append(ast_1)\n",
    "                names_1.append(name_1)\n",
    "                codes_2.append(code_2)\n",
    "                asts_2.append(ast_2)\n",
    "                names_2.append(name_2)\n",
    "                labels.append(label)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return codes_1, asts_1, names_1, codes_2, asts_2, names_2, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0e359e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def parse_for_completion(source_path, target_path):\n",
    "#     \"\"\"\n",
    "#     Load and parse for code completion.\n",
    "\n",
    "#     Args:\n",
    "#         source_path (str): Path of source\n",
    "#         target_path (str): Path of target\n",
    "\n",
    "#     Returns:\n",
    "#         (list[str], list[str], list[str], list[str]):\n",
    "#             - List of strings: source code\n",
    "#             - List of strings: AST sequence\n",
    "#             - List of strings: name sequence\n",
    "#             - List of strings: target code\n",
    "\n",
    "#     \"\"\"\n",
    "#     def restore_source(sub_source):\n",
    "#         \"\"\"\n",
    "#         Transfer split source to source code, which can be parsed into AST.\n",
    "\n",
    "#         Args:\n",
    "#             sub_source (str): Split code\n",
    "\n",
    "#         Returns:\n",
    "#             str: Source code that can be parsed\n",
    "\n",
    "#         \"\"\"\n",
    "#         tokens = sub_source.split()\n",
    "#         is_subtoken = False\n",
    "#         restored_source = ''\n",
    "#         for token in tokens:\n",
    "#             if token == '_':\n",
    "#                 is_subtoken = True\n",
    "#                 continue\n",
    "#             if token == 'PRED':\n",
    "#                 token = Vocab.MSK_TOKEN\n",
    "#             if is_subtoken:\n",
    "#                 restored_source += token.capitalize()\n",
    "#             else:\n",
    "#                 restored_source += f' {token}'\n",
    "#             is_subtoken = False\n",
    "#         return restored_source.strip()\n",
    "\n",
    "#     source_lines = load_lines(source_path)\n",
    "#     target_lines = load_lines(target_path)\n",
    "#     assert len(source_lines) == len(target_lines)\n",
    "\n",
    "#     # #######################################################################\n",
    "#     # Updated to reduce the time to parse, myoungkyu song, 03/31/2024\n",
    "#     if main_args.parse_subset_ratio:\n",
    "#         print(main_args.parse_subset_ratio)\n",
    "#         line_counter = 0\n",
    "#         lines_to_extract = int(len(source_lines) * main_args.parse_subset_ratio)\n",
    "\n",
    "#         if len(source_lines) > 10_000:\n",
    "#             lines_to_extract = int(lines_to_extract * main_args.parse_subset_ratio)\n",
    "#         if len(source_lines) > 100_000:\n",
    "#             lines_to_extract = int(lines_to_extract * main_args.parse_subset_ratio)\n",
    "\n",
    "#         logger.info('*' * 100)\n",
    "#         logger.info(f'The size of trimmed / original fine tunning completion set to parse: {lines_to_extract} / {len(source_lines)}')\n",
    "#     # #######################################################################\n",
    "\n",
    "#     codes = []\n",
    "#     asts = []\n",
    "#     names = []\n",
    "#     targets = []\n",
    "#     for source, target in tqdm(zip(source_lines, target_lines), desc='Parsing', total=len(source_lines)):\n",
    "#         try:\n",
    "#             if main_args.parse_subset_ratio: # myoungkyu song, 03/31/2024\n",
    "#                 if line_counter > lines_to_extract:\n",
    "#                     break\n",
    "#                 line_counter += 1\n",
    "\n",
    "#             source = restore_source(source)\n",
    "#             target = restore_source(target)\n",
    "#             ast, name = generate_single_ast_nl(source=source, lang=LANG_JAVA)\n",
    "#             codes.append(source)\n",
    "#             asts.append(ast)\n",
    "#             names.append(name)\n",
    "#             targets.append(target)\n",
    "#         except Exception:\n",
    "#             continue\n",
    "#     return codes, asts, names, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7decd4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_for_completion(source_code, target_code):\n",
    "    \"\"\"\n",
    "    Parse source and target code for code completion.\n",
    "\n",
    "    Args:\n",
    "        source_code (str): Source code\n",
    "        target_code (str): Target code\n",
    "\n",
    "    Returns:\n",
    "        (list[str], list[str], list[str], list[str]):\n",
    "            - List of strings: source code\n",
    "            - List of strings: AST sequence\n",
    "            - List of strings: name sequence\n",
    "            - List of strings: target code\n",
    "\n",
    "    \"\"\"\n",
    "    def restore_source(sub_source):\n",
    "        tokens = sub_source.split()\n",
    "        is_subtoken = False\n",
    "        restored_source = ''\n",
    "        for token in tokens:\n",
    "            if token == '_':\n",
    "                is_subtoken = True\n",
    "                continue\n",
    "            if token == 'PRED':\n",
    "                token = Vocab.MSK_TOKEN\n",
    "            if is_subtoken:\n",
    "                restored_source += token.capitalize()\n",
    "            else:\n",
    "                restored_source += f' {token}'\n",
    "            is_subtoken = False\n",
    "        return restored_source.strip()\n",
    "\n",
    "    try:\n",
    "        ast, name = generate_single_ast_nl(source=source_code, lang=LANG_JAVA)\n",
    "        return [source_code], [ast], [name], [target_code]\n",
    "    except Exception as e:\n",
    "        # If there's any exception, return empty lists\n",
    "        return [], [], [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8b2f50c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed completion codes: ['public static double Sinc(double x) {\\\\r\\\\n        return Math.sin(Math.PI * x) /']\n",
      "Parsed completion ASTs: ['binary_expression']\n",
      "Parsed completion names: ['sinc h s']\n",
      "Parsed completion targets: ['public static double Sinc(double x) {\\\\r\\\\n        return Math.sin(Math.PI * x) / (Math.PI * x);\\\\r\\\\n    }']\n"
     ]
    }
   ],
   "source": [
    "# Parse for completion and print\n",
    "source_path = \"public static double Sinc(double x) {\\\\r\\\\n        return Math.sin(Math.PI * x) /\"\n",
    "target_path = \"public static double Sinc(double x) {\\\\r\\\\n        return Math.sin(Math.PI * x) / (Math.PI * x);\\\\r\\\\n    }\"\n",
    "codes, asts, names, targets = parse_for_completion(source_path, target_path)\n",
    "print(\"Parsed completion codes:\", codes)\n",
    "print(\"Parsed completion ASTs:\", asts)\n",
    "print(\"Parsed completion names:\", names)\n",
    "print(\"Parsed completion targets:\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "af069421",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_for_bug_fix(buggy_path, fixed_path):\n",
    "    \"\"\"\n",
    "    Load and parse for bug fix.\n",
    "\n",
    "    Args:\n",
    "        buggy_path (str): Path of buggy code\n",
    "        fixed_path (str): Path of fixed code\n",
    "\n",
    "    Returns:\n",
    "        (list[str], list[str], list[str], list[str]):\n",
    "            - List of strings: source code\n",
    "            - List of strings: AST sequence\n",
    "            - List of strings: name sequence\n",
    "            - List of strings: target code\n",
    "\n",
    "    \"\"\"\n",
    "    buggy_lines = load_lines(buggy_path)\n",
    "    fixed_lines = load_lines(fixed_path)\n",
    "    assert len(buggy_lines) == len(fixed_lines)\n",
    "    codes = []\n",
    "    asts = []\n",
    "    names = []\n",
    "    targets = []\n",
    "    for buggy, fixed in tqdm(zip(buggy_lines, fixed_lines), desc='Parsing', total=len(buggy_lines)):\n",
    "        try:\n",
    "            ast, name = generate_single_ast_nl(source=buggy, lang=LANG_JAVA)\n",
    "            codes.append(buggy.lower())\n",
    "            asts.append(ast)\n",
    "            names.append(name.lower())\n",
    "            targets.append(fixed.lower())\n",
    "        except Exception:\n",
    "            continue\n",
    "    return codes, asts, names, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d3eb9c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parse for search and print\n",
    "# dataset_dir = \"/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/pre_train/java\"\n",
    "# lang = \"java\"\n",
    "# split = \"train\"\n",
    "# codes, asts, names, nls = parse_for_search(dataset_dir, lang, split)\n",
    "# print(\"Parsed search codes:\", codes)\n",
    "# print(\"Parsed search ASTs:\", asts)\n",
    "# print(\"Parsed search names:\", names)\n",
    "# print(\"Parsed search nls:\", nls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b7a879c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded clone mapping: None\n"
     ]
    }
   ],
   "source": [
    "# # Load clone mapping and print\n",
    "# dataset_root = \"/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset\"\n",
    "# mapping = load_clone_mapping(dataset_root)\n",
    "# print(\"Loaded clone mapping:\", mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1ed2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parse for clone and print\n",
    "# path = \"clone_dataset_path\"\n",
    "# codes_1, asts_1, names_1, codes_2, asts_2, names_2, labels = parse_for_clone(path, mapping)\n",
    "# print(\"Parsed clone codes_1:\", codes_1)\n",
    "# print(\"Parsed clone ASTs_1:\", asts_1)\n",
    "# print(\"Parsed clone names_1:\", names_1)\n",
    "# print(\"Parsed clone codes_2:\", codes_2)\n",
    "# print(\"Parsed clone ASTs_2:\", asts_2)\n",
    "# print(\"Parsed clone names_2:\", names_2)\n",
    "# print(\"Parsed clone labels:\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd0ebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Parse for bug fix and print\n",
    "# buggy_path = \"buggy_path\"\n",
    "# fixed_path = \"fixed_path\"\n",
    "# codes, asts, names, targets = parse_for_bug_fix(buggy_path, fixed_path)\n",
    "# print(\"Parsed bug fix codes:\", codes)\n",
    "# print(\"Parsed bug fix ASTs:\", asts)\n",
    "# print(\"Parsed bug fix names:\", names)\n",
    "# print(\"Parsed bug fix targets:\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "72c0acdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "class CodeDataset(Dataset):\n",
    "\n",
    "    def __init__(self, args, dataset_name, mode, task=None, language=None, split=None, clone_mapping=None):\n",
    "        \"\"\"\n",
    "        Initialization definition.\n",
    "\n",
    "        Args:\n",
    "            args (argparse.Namespace): Arguments\n",
    "            dataset_name (str): Name of the dataset\n",
    "            mode (str): Training mode, ``pre_train`` or ``fine_tune``\n",
    "            task (str): Dataset mode, support pre-training tasks: ['cap', 'mass', 'mnp'],\n",
    "                and downstream fine-tuning task: ['summarization', 'translation'],\n",
    "                future support ['completion', 'search', 'clone', 'summarization', 'translation']\n",
    "            language (str): Only for downstream fine-tuning\n",
    "            split (str): Only for downstream fine-tuning, support ['train', 'valid', 'test', 'codebase']\n",
    "            clone_mapping (dict[int, str]): Mapping from code id to source code string, use only for clone detection\n",
    "\n",
    "        \"\"\"\n",
    "        super(CodeDataset, self).__init__()\n",
    "        self.args = args\n",
    "        self.dataset_name = dataset_name\n",
    "        self.task = task\n",
    "        self.mode = mode\n",
    "        self.split = split\n",
    "        self.paths = {}\n",
    "\n",
    "        # dataset dir for files, all files in this dir meeting the filename will be used as dataset files\n",
    "        self.dataset_dir = os.path.join(args.dataset_root, self.mode)\n",
    "\n",
    "        # load pre-training dataset\n",
    "        if self.mode == 'pre_train':\n",
    "            set_args(args=args)\n",
    "            self.paths, self.languages, self.sources, self.codes, self.asts, self.names, self.codes_wo_name, \\\n",
    "                self.names_wo_name, self.only_names, self.docs = load_dataset_from_dir(dataset_dir=self.dataset_dir)\n",
    "            self.size = len(self.codes)\n",
    "        # load fine-tuning dataset\n",
    "        else:\n",
    "            assert split\n",
    "            logger.info(f'  Loading {split} set')\n",
    "            self.dataset_dir = os.path.join(self.dataset_dir, task)\n",
    "            # code summarization\n",
    "            if task == TASK_SUMMARIZATION:\n",
    "                assert language, '\\'Language\\' must be specific if downstream task is code summarization'\n",
    "                assert split in ['train', 'valid', 'test']\n",
    "                self.dataset_dir = os.path.join(self.dataset_dir, language, split)\n",
    "\n",
    "                self.source_path = os.path.join(self.dataset_dir, 'source.code')\n",
    "                self.code_path = os.path.join(self.dataset_dir, 'token.code')\n",
    "                self.nl_path = os.path.join(self.dataset_dir, 'token.docstring')\n",
    "\n",
    "                self.paths, self.codes, self.asts, self.names, self.nls = parse_for_summarization(\n",
    "                    source_path=self.source_path,\n",
    "                    code_path=self.code_path,\n",
    "                    nl_path=self.nl_path,\n",
    "                    lang=language)\n",
    "                assert len(self.codes) == len(self.asts) == len(self.names) == len(self.nls)\n",
    "                self.size = len(self.codes)\n",
    "            # code translation\n",
    "            elif task == TASK_TRANSLATION:\n",
    "                assert split in ['train', 'valid', 'test']\n",
    "                assert language in ['java-c_sharp', 'c_sharp-java']\n",
    "                source_lang, target_lang = language.split('-')\n",
    "                java_path = f'{split}.java-cs.txt.java'\n",
    "                c_sharp_path = f'{split}.java-cs.txt.cs'\n",
    "                source_path = os.path.join(self.dataset_dir,\n",
    "                                           c_sharp_path if source_lang == 'c_sharp' else java_path)\n",
    "                target_path = os.path.join(self.dataset_dir,\n",
    "                                           c_sharp_path if target_lang == 'c_sharp' else java_path)\n",
    "                self.paths['source'] = source_path\n",
    "                self.paths['target'] = target_path\n",
    "                self.codes, self.asts, self.names, self.targets = parse_for_translation(\n",
    "                    source_path=source_path,\n",
    "                    source_lang=args.translation_source_language,\n",
    "                    target_path=target_path,\n",
    "                    target_lang=args.translation_target_language)\n",
    "\n",
    "                assert len(self.codes) == len(self.asts) == len(self.names) == len(self.targets)\n",
    "                self.size = len(self.codes)\n",
    "            # code search\n",
    "            elif task == TASK_SEARCH:\n",
    "                assert language, '``Language`` must be specific if downstream task is code search'\n",
    "                assert split in ['codebase', 'train', 'valid', 'test']\n",
    "                self.dataset_dir = os.path.join(self.dataset_dir, language)\n",
    "                self.paths['file'] = os.path.join(self.dataset_dir, f'{split}.jsonl')\n",
    "\n",
    "                if split == 'codebase':\n",
    "                    set_args(args=args) # Added to pass args, myoungkyu song, 03/24/2024\n",
    "                    self.urls, self.codes, self.asts, self.names = parse_for_search(dataset_dir=self.dataset_dir,\n",
    "                                                                                    lang=language,\n",
    "                                                                                    split=split)\n",
    "                    assert len(self.urls), len(self.codes) == len(self.asts) == len(self.names)\n",
    "                    self.size = len(self.urls)\n",
    "                elif split == 'train':\n",
    "                    self.codes, self.asts, self.names, self.nls = parse_for_search(dataset_dir=self.dataset_dir,\n",
    "                                                                                   lang=language,\n",
    "                                                                                   split=split)\n",
    "                    assert len(self.codes) == len(self.asts) == len(self.names) == len(self.nls)\n",
    "                    self.size = len(self.codes)\n",
    "                else:\n",
    "                    self.urls, self.nls = parse_for_search(dataset_dir=self.dataset_dir, lang=language, split=split)\n",
    "                    self.size = len(self.urls)\n",
    "            # code clone detection\n",
    "            elif task == TASK_CLONE_DETECTION:\n",
    "                assert split in ['train', 'valid', 'test']\n",
    "                assert clone_mapping\n",
    "                path = os.path.join(self.dataset_dir, f'{split}.txt')\n",
    "                self.paths['file'] = path\n",
    "                self.codes_1, self.asts_1, self.names_1, \\\n",
    "                    self.codes_2, self.asts_2, self.names_2, self.labels = parse_for_clone(path=path,\n",
    "                                                                                           mapping=clone_mapping)\n",
    "                assert len(self.codes_1) == len(self.asts_1) == len(self.names_1) \\\n",
    "                       == len(self.codes_2) == len(self.asts_2) == len(self.names_2) == len(self.labels)\n",
    "                self.size = len(self.codes_1)\n",
    "            # completion\n",
    "            elif task == TASK_COMPLETION:\n",
    "                assert split in ['train', 'valid', 'test']\n",
    "                source_path = os.path.join(self.dataset_dir, f'data.TargetType.seq.{split}.source.txt')\n",
    "                target_path = os.path.join(self.dataset_dir, f'data.TargetType.seq.{split}.target.txt')\n",
    "                self.paths['source'] = source_path\n",
    "                self.paths['target'] = target_path\n",
    "                set_args(args=args) # Added to pass args, myoungkyu song, 03/31/2024\n",
    "                self.codes, self.asts, self.names, self.targets = parse_for_completion(source_path=source_path,\n",
    "                                                                                       target_path=target_path)\n",
    "                assert len(self.codes) == len(self.asts) == len(self.names) == len(self.targets)\n",
    "                self.size = len(self.codes)\n",
    "            # bug fix\n",
    "            elif task == TASK_BUG_FIX:\n",
    "                assert split in ['train', 'valid', 'test']\n",
    "                # language here stands for dataset scale\n",
    "                assert language in ['small', 'medium']\n",
    "                self.dataset_dir = os.path.join(self.dataset_dir, language)\n",
    "                buggy_path = os.path.join(self.dataset_dir, f'{split}.buggy-fixed.buggy')\n",
    "                fixed_path = os.path.join(self.dataset_dir, f'{split}.buggy-fixed.fixed')\n",
    "                self.paths['buggy'] = buggy_path\n",
    "                self.paths['fixed'] = fixed_path\n",
    "                self.codes, self.asts, self.names, self.targets = parse_for_bug_fix(buggy_path=buggy_path,\n",
    "                                                                                    fixed_path=fixed_path)\n",
    "                assert len(self.codes) == len(self.asts) == len(self.names) == len(self.targets)\n",
    "                self.size = len(self.codes)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # cap\n",
    "        \n",
    "#Randomly decides whether to return an original AST or a different one along with the corresponding code and name.\n",
    "#If an original AST is chosen, it returns the code, the original AST, the name, and a label indicating 1 (original).\n",
    "#If a different AST is chosen, it returns the code, a randomly selected AST (excluding the original one), the name, and a label indicating 0 (different).\n",
    "        if self.task == TASK_CODE_AST_PREDICTION:\n",
    "            # print(f'[DBG] index: {index}')\n",
    "            # org_ast = self.asts[index]\n",
    "            # if index == 184215:\n",
    "            #     self.keep_ast_index = 184215\n",
    "            # elif index == 59442:\n",
    "            #     print(f'[DBG] returned index: {self.keep_ast_index}')\n",
    "            #     return self.codes[self.keep_ast_index], self.asts[self.keep_ast_index], self.names[self.keep_ast_index], 1\n",
    "\n",
    "            is_ast = random.random() < 0.5\n",
    "            if is_ast:\n",
    "                return self.codes[index], self.asts[index], self.names[index], 1\n",
    "            else:\n",
    "                other_ast = self.asts[random.randint(0, self.size - 1)]\n",
    "                while other_ast == self.asts[index]:\n",
    "                    other_ast = self.asts[random.randint(0, self.size - 1)]\n",
    "                return self.codes[index], other_ast, self.names[index], 0\n",
    "        # mass\n",
    "#Randomly selects a portion of the code tokens to mask.\n",
    "#Constructs input tokens with a masked section.\n",
    "#Returns the modified code, the AST, the name, and the masked tokens.\n",
    "        elif self.task == TASK_MASS:\n",
    "            # print(f'[DBG] index: {index}')\n",
    "\n",
    "            code_tokens = self.codes[index].split()\n",
    "            mask_len = int(self.args.mass_mask_ratio * len(code_tokens))\n",
    "            mask_start = random.randint(0, len(code_tokens) - mask_len)\n",
    "            mask_tokens = code_tokens[mask_start: mask_start + mask_len]\n",
    "            input_tokens = code_tokens[:mask_start] + [Vocab.MSK_TOKEN] + code_tokens[mask_start + mask_len:]\n",
    "            # print(f'[DBG] code {code_tokens}')\n",
    "            # print(f'[DBG] input {input_tokens}')\n",
    "            # print(f'[DBG] mask {mask_tokens}')\n",
    "            return ' '.join(input_tokens), self.asts[index], self.names[index], ' '.join(mask_tokens)\n",
    "        # mnp\n",
    "#Returns the code without method names, the AST, the name without method names, and the original name\n",
    "        elif self.task == TASK_METHOD_NAME_PREDICTION:\n",
    "            return self.codes_wo_name[index], self.asts[index], self.names_wo_name[index], self.names[index]\n",
    "        # summarization\n",
    "        elif self.task == TASK_SUMMARIZATION:\n",
    "            return self.codes[index], self.asts[index], self.names[index], self.nls[index]\n",
    "            # return self.codes[index], None, None, self.nls[index]\n",
    "        # translation\n",
    "        elif self.task == TASK_TRANSLATION:\n",
    "            return self.codes[index], self.asts[index], self.names[index], self.targets[index]\n",
    "        # search\n",
    "        elif self.task == TASK_SEARCH:\n",
    "            if self.split == 'codebase':\n",
    "                return self.split, self.urls[index], self.codes[index], self.asts[index], self.names[index]\n",
    "            elif self.split == 'train':\n",
    "                pos_nl = self.nls[index]\n",
    "                # while True:\n",
    "                #     neg_index = random.randint(0, self.size - 1)\n",
    "                #     neg_nl = self.nls[neg_index]\n",
    "                #     if avg_bleu(references=[pos_nl.split()], candidates=[neg_nl.split()]) < 0.5:\n",
    "                #         break\n",
    "                # return self.split, self.codes[index], self.asts[index], self.names[index], pos_nl, neg_nl\n",
    "                return self.split, self.codes[index], self.asts[index], self.names[index], pos_nl\n",
    "            else:\n",
    "                return self.split, self.urls[index], self.nls[index]\n",
    "        # clone detection\n",
    "        elif self.task == TASK_CLONE_DETECTION:\n",
    "            return self.codes_1[index], self.asts_1[index], self.names_1[index], \\\n",
    "                   self.codes_2[index], self.asts_2[index], self.names_2[index], self.labels[index]\n",
    "        # code completion\n",
    "        elif self.task == TASK_COMPLETION:\n",
    "            return self.codes[index], self.asts[index], self.names[index], self.targets[index]\n",
    "        # bug fix\n",
    "        elif self.task == TASK_BUG_FIX:\n",
    "            return self.codes[index], self.asts[index], self.names[index], self.targets[index]\n",
    "#For tasks like code summarization, translation, search, etc., it returns source code, AST, name/API sequence, and target code.\n",
    "#For pre-training tasks like CAP (Code-AST Prediction), MASS (Masked Sequence Prediction), and MNP (Method Name Prediction), it returns different combinations of source code, AST, name/API sequence, and masked tokens.\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def set_task(self, task):\n",
    "        self.task = task\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"Save to binary pickle file\"\"\"\n",
    "        path = os.path.join(self.args.dataset_save_dir, f'{self.dataset_name}.pk')\n",
    "        with open(path, mode='wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "        logger.info(f'Dataset saved to {path}')\n",
    "\n",
    "    def subset(self, ratio):\n",
    "        \"\"\"\n",
    "        Return a subset of self.\n",
    "\n",
    "        Args:\n",
    "            ratio (float): The ratio of size, must greater than 0 and less than/equal to 1\n",
    "\n",
    "        Returns:\n",
    "            Dataset: the subset\n",
    "\n",
    "        \"\"\"\n",
    "        assert 0 < ratio <= 1, f'The subset ratio supposed to be 0 < ratio <= 1, but got ratio={ratio}'\n",
    "        if ratio == 1:\n",
    "            return self\n",
    "        indices = random.sample(range(self.size), int(self.size * ratio))\n",
    "        return torch.utils.data.Subset(self, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "66d12269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If a saved instance exists and load_if_saved is True, it loads and returns the saved dataset.\n",
    "#If no saved instance exists or load_if_saved is False, it initializes a new dataset using CodeDataset class and saves it.\n",
    "def init_dataset(args, mode, task=None, language=None, split=None, clone_mapping=None,\n",
    "                 load_if_saved=True) -> CodeDataset:\n",
    "    \"\"\"\n",
    "    Find dataset, if the dataset is saved, load and return, else initialize and return.\n",
    "\n",
    "    Args:\n",
    "        args (argparse.Namespace): Arguments\n",
    "        mode (str): Training mode, ``pre_train`` or ``fine_tune``\n",
    "        task (str): Dataset mode, support pre-training tasks: ['cap', 'mass', 'mnp'],\n",
    "            and downstream fine-tuning task: ['summarization', 'translation'],\n",
    "            future support ['completion', 'search', 'clone', 'summarization', 'translation']\n",
    "        language (str): Only for downstream fine-tuning\n",
    "        split (str): Only for downstream fine-tuning, support ['train', 'valid', 'test', 'codebase(only for search)']\n",
    "        clone_mapping (dict[int, str]): Mapping from code id to source code string, use only for clone detection\n",
    "        load_if_saved (bool): Whether to load the saved instance if it exists, default to True\n",
    "\n",
    "    Returns:\n",
    "        CodeDataset: Loaded or initialized dataset\n",
    "\n",
    "    \"\"\"\n",
    "    name = '.'.join([sub_name for sub_name in [mode, task, language, split] if sub_name is not None])\n",
    "    if load_if_saved:\n",
    "        path = os.path.join(args.dataset_save_dir, f'{name}.pk') # '../../dataset/dataset_saved/pre_train.pk'\n",
    "        path_org = os.path.join(args.dataset_save_dir, f'{name}_org.pk') # '../../dataset/dataset_saved/pre_train_org.pk'\n",
    "\n",
    "        # #######################################################################\n",
    "        # Updated it with an argument `remove_existing_saved_file`, myoungkyu song, 03/23/2024\n",
    "        if os.path.exists(path) and \\\n",
    "                (args.remove_existing_saved_file is not None and 'fine_tune' in args.remove_existing_saved_file) and \\\n",
    "                ('fine_tune' in path):\n",
    "            logger.info(f'Removing the existing file: {path}')\n",
    "            os.remove(path)\n",
    "        if os.path.exists(path) and \\\n",
    "                (args.remove_existing_saved_file is not None and 'pre_train' in args.remove_existing_saved_file) and \\\n",
    "                ('pre_train' in path):\n",
    "            logger.info(f'Removing the existing file: {path}')\n",
    "            os.remove(path)\n",
    "        if os.path.exists(path_org) and (args.copy_existing_saved_file is not None and 'pre_train_org' in args.copy_existing_saved_file) and ('pre_train' in path):\n",
    "            logger.info(f'Copying the existing file: {path_org}')\n",
    "            shutil.copy(path_org, path)\n",
    "            \n",
    "        # #######################################################################\n",
    "\n",
    "        if os.path.exists(path) and os.path.isfile(path):\n",
    "            logger.info(f'Trying to load saved binary pickle file from: {path}')\n",
    "            with open(path, mode='rb') as f:\n",
    "                obj = pickle.load(f)\n",
    "            assert isinstance(obj, CodeDataset)\n",
    "            obj.args = args\n",
    "            logger.info(f'Dataset instance loaded from: {path}')\n",
    "            print_paths(obj.paths)\n",
    "            return obj\n",
    "    dataset = CodeDataset(args=args,\n",
    "                          dataset_name=name,\n",
    "                          mode=mode,\n",
    "                          task=task,\n",
    "                          language=language,\n",
    "                          split=split,\n",
    "                          clone_mapping=clone_mapping)\n",
    "    dataset.save()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "30abe29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_paths(paths):\n",
    "    \"\"\"\n",
    "    Print paths.\n",
    "\n",
    "    Args:\n",
    "        paths (dict): Dict mapping path group to path string or list of path strings.\n",
    "\n",
    "    \"\"\"\n",
    "    logger.info('Dataset loaded from these files:')\n",
    "    for key, value in paths.items():\n",
    "        if isinstance(value, list):\n",
    "            for v in value:\n",
    "                logger.info(f'  {key}: {v}')\n",
    "        else:\n",
    "            logger.info(f'  {key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60e6e122",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_all_datasets(args):\n",
    "    # logger.info('*' * 100)\n",
    "    # logger.info('Pre-training dataset')\n",
    "    # _ = init_dataset(args=args,\n",
    "    #                  mode=enums.TRAINING_MODE_PRE_TRAIN,\n",
    "    #                  load_if_saved=False)\n",
    "    # summarization\n",
    "    for lang in [LANG_JAVA, LANG_GO, LANG_PHP, LANG_PYTHON, LANG_RUBY,\n",
    "                 LANG_JAVASCRIPT]:\n",
    "        for split in ['train', 'valid', 'test']:\n",
    "            logger.info('*' * 100)\n",
    "            logger.info(f'Summarization - {lang} - {split}')\n",
    "            _ = init_dataset(args=args,\n",
    "                             mode= TRAINING_MODE_FINE_TUNE,\n",
    "                             task= TASK_SUMMARIZATION,\n",
    "                             language=lang,\n",
    "                             split=split,\n",
    "                             load_if_saved=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc02196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/spt-code/sources')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b924bd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Configuration:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.bart.modeling_bart because of the following error (look up to see its traceback):\nObject of type type is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/myenv_python3_11/lib/python3.11/site-packages/transformers/utils/import_utils.py:1382\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv_python3_11/lib/python3.11/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:940\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv_python3_11/lib/python3.11/site-packages/transformers/models/bart/modeling_bart.py:54\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     45\u001b[0m     add_code_sample_docstrings,\n\u001b[1;32m     46\u001b[0m     add_end_docstrings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m     replace_return_docstrings,\n\u001b[1;32m     53\u001b[0m )\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_bart\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BartConfig\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_flash_attn_2_available():\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv_python3_11/lib/python3.11/site-packages/transformers/models/bart/configuration_bart.py:422\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitialized Configuration:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 422\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBartConfig class properties:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv_python3_11/lib/python3.11/site-packages/transformers/configuration_utils.py:834\u001b[0m, in \u001b[0;36mPretrainedConfig.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_json_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv_python3_11/lib/python3.11/site-packages/transformers/configuration_utils.py:951\u001b[0m, in \u001b[0;36mPretrainedConfig.to_json_string\u001b[0;34m(self, use_diff)\u001b[0m\n\u001b[1;32m    950\u001b[0m     config_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[0;32m--> 951\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv_python3_11/lib/python3.11/json/__init__.py:238\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseparators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m--> 238\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv_python3_11/lib/python3.11/json/encoder.py:202\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 202\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(chunks)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv_python3_11/lib/python3.11/json/encoder.py:432\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 432\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv_python3_11/lib/python3.11/json/encoder.py:406\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    405\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 406\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv_python3_11/lib/python3.11/json/encoder.py:439\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    438\u001b[0m     markers[markerid] \u001b[38;5;241m=\u001b[39m o\n\u001b[0;32m--> 439\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv_python3_11/lib/python3.11/json/encoder.py:180\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03ma serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m(to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    181\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type type is not JSON serializable",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEntropyLoss, MSELoss\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mf\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BartForConditionalGeneration, BartConfig\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbart\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_bart\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BartClassificationHead, shift_tokens_right\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Seq2SeqLMOutput, Seq2SeqSequenceClassifierOutput\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1229\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv_python3_11/lib/python3.11/site-packages/transformers/utils/import_utils.py:1373\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1372\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m-> 1373\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv_python3_11/lib/python3.11/site-packages/transformers/utils/import_utils.py:1372\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1370\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1372\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv_python3_11/lib/python3.11/site-packages/transformers/utils/import_utils.py:1384\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1383\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1384\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1385\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1386\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1387\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.bart.modeling_bart because of the following error (look up to see its traceback):\nObject of type type is not JSON serializable"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "import torch.nn.functional as f\n",
    "\n",
    "from transformers import BartForConditionalGeneration, BartConfig\n",
    "from transformers.models.bart.modeling_bart import BartClassificationHead, shift_tokens_right\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput, Seq2SeqSequenceClassifierOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af249f69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c774ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f8b3b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6c117a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d4f6a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6883323",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "myenv_python3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
