{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import logging\n",
    "import os\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "STRING_MATCHING_PATTERN = re.compile(r'([bruf]*)(\\\"\\\"\\\"|\\'\\'\\'|\\\"|\\')(?:(?!\\2)(?:\\\\.|[^\\\\]))*\\2')\n",
    "NON_SPACE_MATCHING_PATTERN = re.compile(r'\\S')\n",
    "\n",
    "lang_dir = '/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/pre_train/java'\n",
    "\n",
    "\n",
    "def trim_method_name(full_name):\n",
    "    \"\"\"\n",
    "    Extract method/function name from its full name,\n",
    "    e.g., RpcResponseResolver.resolveResponseObject -> resolveResponseObject\n",
    "\n",
    "    Args:\n",
    "        full_name (str): Full name\n",
    "\n",
    "    Returns:\n",
    "        str: Method/Function name\n",
    "\n",
    "    \"\"\"\n",
    "    point_pos = full_name.rfind('.')\n",
    "    if point_pos != -1:\n",
    "        return full_name[point_pos + 1:]\n",
    "    else:\n",
    "        return full_name\n",
    "\n",
    "def remove_comments_and_docstrings(source):\n",
    "    \"\"\"\n",
    "    Remove docs and comments from source string.\n",
    "    Thanks to authors of GraphCodeBERT\n",
    "    from: https://github.com/microsoft/CodeBERT/blob/master/GraphCodeBERT/codesearch/parser/utils.py#L4\n",
    "\n",
    "    Args:\n",
    "        source (str): Source code string\n",
    "        lang (str): Source code language\n",
    "\n",
    "    Returns:\n",
    "        str: Source string\n",
    "\n",
    "    \"\"\"\n",
    "    def replacer(match):\n",
    "        s = match.group(0)\n",
    "        if s.startswith('/'):\n",
    "            return \" \"  # note: a space and not an empty string\n",
    "        else:\n",
    "            return s\n",
    "\n",
    "    pattern = re.compile(\n",
    "        r'//.*?$|/\\*.*?\\*/|\\'(?:\\\\.|[^\\\\\\'])*\\'|\"(?:\\\\.|[^\\\\\"])*\"',\n",
    "        re.DOTALL | re.MULTILINE\n",
    "    )\n",
    "    temp = []\n",
    "    for x in re.sub(pattern, replacer, source).split('\\n'):\n",
    "        if x.strip() != \"\":\n",
    "            temp.append(x)\n",
    "    return '\\n'.join(temp)\n",
    "\n",
    "\n",
    "def remove_unwanted_characters(source):\n",
    "    return source.replace('\\n', '').replace('\\t', '').replace('\\r', '')\n",
    "\n",
    "def parse_java_json_file(file_path):\n",
    "    # sources, codes, names = [], [], []\n",
    "    sources = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line.strip())\n",
    "            name = trim_method_name(data['func_name'])\n",
    "            source = data['code'].strip()\n",
    "            source = remove_comments_and_docstrings(source)\n",
    "            clean_code = remove_unwanted_characters(source)\n",
    "            source = replace_string_literal(clean_code)\n",
    "\n",
    "            sources.append(source)\n",
    "            \n",
    "\n",
    "    return sources\n",
    "\n",
    "\n",
    "def replace_string_literal(source):\n",
    "    return re.sub(pattern=STRING_MATCHING_PATTERN, repl='___STR', string=source)\n",
    "\n",
    "def trim_method_name(full_name):\n",
    "    return full_name.split('.')[-1]\n",
    "\n",
    "def iter_all_files(base):\n",
    "    for root, dirs, files in os.walk(base):\n",
    "        for f in files:\n",
    "            yield os.path.join(root, f)\n",
    "\n",
    "def iter_pre_train_dataset_files(lang_dir, lang=\"Java\"):\n",
    "    return [file for file in iter_all_files(base=lang_dir) if file.endswith('.jsonl')]\n",
    "\n",
    "# Iterate over files and print the outputs\n",
    "file_paths = iter_pre_train_dataset_files(lang_dir)\n",
    "for file_path in file_paths:\n",
    "    # sources, codes, names = parse_java_json_file(file_path)\n",
    "    sources = parse_java_json_file(file_path)\n",
    "    # for source, code, name in zip(sources, codes, names):\n",
    "    for source in zip(sources):\n",
    "    #     print(f\"File: {file_path}\")\n",
    "    #     print(f\"Method Name: {name}\")\n",
    "          print(f\"Source Code: {source}\")\n",
    "        # print(f\"Tokenized Code: {code}\\n\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lang_dir = '/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/pre_train/java'\n",
    "\n",
    "output_path = '/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/spt-code/sources/data/asts/ast_jdt/longmethod.txt'\n",
    "\n",
    "file_paths = iter_pre_train_dataset_files(lang_dir, 'Java')\n",
    "with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "    for file_path in file_paths:\n",
    "        sources = parse_java_json_file(file_path)\n",
    "        for source in sources:\n",
    "            output_file.write(json.dumps(source) + \"\\n\")\n",
    "        #break  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Finetuning part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import logging\n",
    "import os\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "STRING_MATCHING_PATTERN = re.compile(r'([bruf]*)(\\\"\\\"\\\"|\\'\\'\\'|\\\"|\\')(?:(?!\\2)(?:\\\\.|[^\\\\]))*\\2')\n",
    "NON_SPACE_MATCHING_PATTERN = re.compile(r'\\S')\n",
    "\n",
    "lang_dir = '/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/finetune_raw/java-small-json'\n",
    "\n",
    "\n",
    "def trim_method_name(full_name):\n",
    "    \"\"\"\n",
    "    Extract method/function name from its full name,\n",
    "    e.g., RpcResponseResolver.resolveResponseObject -> resolveResponseObject\n",
    "\n",
    "    Args:\n",
    "        full_name (str): Full name\n",
    "\n",
    "    Returns:\n",
    "        str: Method/Function name\n",
    "\n",
    "    \"\"\"\n",
    "    point_pos = full_name.rfind('.')\n",
    "    if point_pos != -1:\n",
    "        return full_name[point_pos + 1:]\n",
    "    else:\n",
    "        return full_name\n",
    "\n",
    "def remove_comments_and_docstrings(source):\n",
    "    \"\"\"\n",
    "    Remove docs and comments from source string.\n",
    "    Thanks to authors of GraphCodeBERT\n",
    "    from: https://github.com/microsoft/CodeBERT/blob/master/GraphCodeBERT/codesearch/parser/utils.py#L4\n",
    "\n",
    "    Args:\n",
    "        source (str): Source code string\n",
    "        lang (str): Source code language\n",
    "\n",
    "    Returns:\n",
    "        str: Source string\n",
    "\n",
    "    \"\"\"\n",
    "    def replacer(match):\n",
    "        s = match.group(0)\n",
    "        if s.startswith('/'):\n",
    "            return \" \"  # note: a space and not an empty string\n",
    "        else:\n",
    "            return s\n",
    "\n",
    "    pattern = re.compile(\n",
    "        r'//.*?$|/\\*.*?\\*/|\\'(?:\\\\.|[^\\\\\\'])*\\'|\"(?:\\\\.|[^\\\\\"])*\"',\n",
    "        re.DOTALL | re.MULTILINE\n",
    "    )\n",
    "    temp = []\n",
    "    for x in re.sub(pattern, replacer, source).split('\\n'):\n",
    "        if x.strip() != \"\":\n",
    "            temp.append(x)\n",
    "    return '\\n'.join(temp)\n",
    "\n",
    "\n",
    "def remove_unwanted_characters(source):\n",
    "    return source.replace('\\n', '').replace('\\t', '').replace('\\r', '')\n",
    "\n",
    "def parse_java_json_file(file_path):\n",
    "    # sources, codes, names = [], [], []\n",
    "    sources = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line.strip())\n",
    "            left_context = replace_string_literal(data['left_context']).replace('\\n', '\\\\n').replace('=', '\\\\u003d').replace(\"\\\\\", \"\\\\\\\\\").replace(\"\\\"\", \"\\\\\\\"\").replace(\"\\r\", \"\\\\r\").replace(\"\\t\", \"\\\\t\")\n",
    "            right_context = replace_string_literal(data['right_context']).replace('\\n', '\\\\n').replace('=', '\\\\u003d').replace(\"\\\\\", \"\\\\\\\\\").replace(\"\\\"\", \"\\\\\\\"\").replace(\"\\r\", \"\\\\r\").replace(\"\\t\", \"\\\\t\")\n",
    "            target_seq = replace_string_literal(data['target_seq']).replace('\\n', '\\\\n').replace('=', '\\\\u003d').replace(\"\\\\\", \"\\\\\\\\\").replace(\"\\\"\", \"\\\\\\\"\").replace(\"\\r\", \"\\\\r\").replace(\"\\t\", \"\\\\t\")\n",
    "            formatted_string = f\"{left_context} {target_seq} {right_context}\"\n",
    "            clean_code = remove_comments_and_docstrings(formatted_string)\n",
    "            source = remove_unwanted_characters(clean_code)\n",
    "\n",
    "            sources.append(source)\n",
    "            \n",
    "\n",
    "    return sources\n",
    "\n",
    "\n",
    "def replace_string_literal(source):\n",
    "    return re.sub(pattern=STRING_MATCHING_PATTERN, repl='___STR', string=source)\n",
    "\n",
    "def trim_method_name(full_name):\n",
    "    return full_name.split('.')[-1]\n",
    "\n",
    "def iter_all_files(base):\n",
    "    for root, dirs, files in os.walk(base):\n",
    "        for f in files:\n",
    "            yield os.path.join(root, f)\n",
    "\n",
    "def iter_pre_train_dataset_files(lang_dir):\n",
    "    return [file for file in iter_all_files(base=lang_dir) if file.endswith('.json')]\n",
    "\n",
    "# Iterate over files and print the outputs\n",
    "file_paths = iter_pre_train_dataset_files(lang_dir)\n",
    "for file_path in file_paths:\n",
    "    # sources, codes, names = parse_java_json_file(file_path)\n",
    "    sources = parse_java_json_file(file_path)\n",
    "    # for source, code, name in zip(sources, codes, names):\n",
    "    for source in zip(sources):\n",
    "    #     print(f\"File: {file_path}\")\n",
    "    #     print(f\"Method Name: {name}\")\n",
    "          print(f\"Source Code: {source}\")\n",
    "        # print(f\"Tokenized Code: {code}\\n\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/finetune_raw/java-small-json/finetune_methods.txt'\n",
    "\n",
    "file_paths = iter_pre_train_dataset_files(lang_dir)\n",
    "with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "    for file_path in file_paths:\n",
    "        sources = parse_java_json_file(file_path)\n",
    "        for source in sources:\n",
    "            output_file.write(json.dumps(source) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_python3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
