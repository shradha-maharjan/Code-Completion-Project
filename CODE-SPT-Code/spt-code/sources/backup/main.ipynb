{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1-selab3/miniconda3/envs/myenv_python3_11/lib/python3.11/site-packages/tree_sitter/__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data\n",
    "from transformers import BartConfig, Seq2SeqTrainingArguments, IntervalStrategy, SchedulerType, TrainingArguments\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from typing import Union, Tuple\n",
    "\n",
    "import enums\n",
    "from data.dataset import init_dataset\n",
    "from data.vocab import Vocab, init_vocab, load_vocab\n",
    "from utils.general import count_params, human_format, layer_wise_parameters\n",
    "from utils.trainer import CodeTrainer, CodeCLSTrainer\n",
    "from utils.callbacks import LogStateCallBack\n",
    "from models.bart import BartForClassificationAndGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tokenize\n",
    "from io import StringIO\n",
    "import json\n",
    "import os, sys\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from antlr4 import InputStream\n",
    "import nltk\n",
    "\n",
    "from .asts.ast_parser import generate_single_ast_nl, split_identifier\n",
    "import enums\n",
    "from data.vocab import Vocab\n",
    "from data.antlr_parsers.go.GoLexer import GoLexer\n",
    "from data.antlr_parsers.java.Java8Lexer import Java8Lexer\n",
    "from data.antlr_parsers.python3.Python3Lexer import Python3Lexer\n",
    "from data.antlr_parsers.php.PhpLexer import PhpLexer\n",
    "from data.antlr_parsers.javascript.JavaScriptLexer import JavaScriptLexer\n",
    "from data.code_tokenizers.ruby.ruby_tokenizer import RubyTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01menums\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset_from_dir, set_args, \\\n\u001b[1;32m     12\u001b[0m     parse_for_summarization, parse_for_translation, parse_for_search, parse_for_clone, parse_for_completion, \\\n\u001b[1;32m     13\u001b[0m     parse_for_bug_fix\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleu\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgoogle_bleu\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m avg_bleu\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Vocab\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import torch.utils.data\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "import enums\n",
    "from .data_utils import load_dataset_from_dir, set_args, \\\n",
    "    parse_for_summarization, parse_for_translation, parse_for_search, parse_for_clone, parse_for_completion, \\\n",
    "    parse_for_bug_fix\n",
    "from eval.bleu.google_bleu import avg_bleu\n",
    "from data.vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_dataset(args, mode, task=None, language=None, split=None, clone_mapping=None,\n",
    "                 load_if_saved=True) -> CodeDataset:\n",
    "    \"\"\"\n",
    "    Find dataset, if the dataset is saved, load and return, else initialize and return.\n",
    "\n",
    "    Args:\n",
    "        args (argparse.Namespace): Arguments\n",
    "        mode (str): Training mode, ``pre_train`` or ``fine_tune``\n",
    "        task (str): Dataset mode, support pre-training tasks: ['cap', 'mass', 'mnp'],\n",
    "            and downstream fine-tuning task: ['summarization', 'translation'],\n",
    "            future support ['completion', 'search', 'clone', 'summarization', 'translation']\n",
    "        language (str): Only for downstream fine-tuning\n",
    "        split (str): Only for downstream fine-tuning, support ['train', 'valid', 'test', 'codebase(only for search)']\n",
    "        clone_mapping (dict[int, str]): Mapping from code id to source code string, use only for clone detection\n",
    "        load_if_saved (bool): Whether to load the saved instance if it exists, default to True\n",
    "\n",
    "    Returns:\n",
    "        CodeDataset: Loaded or initialized dataset\n",
    "\n",
    "    \"\"\"\n",
    "    name = '.'.join([sub_name for sub_name in [mode, task, language, split] if sub_name is not None])\n",
    "    if load_if_saved:\n",
    "        path = os.path.join(args.dataset_save_dir, f'{name}.pk') # '../../dataset/dataset_saved/pre_train.pk'\n",
    "        path_org = os.path.join(args.dataset_save_dir, f'{name}_org.pk') # '../../dataset/dataset_saved/pre_train_org.pk'\n",
    "\n",
    "        # #######################################################################\n",
    "        # Updated it with an argument `remove_existing_saved_file`, myoungkyu song, 03/23/2024\n",
    "        if os.path.exists(path) and \\\n",
    "                (args.remove_existing_saved_file is not None and 'fine_tune' in args.remove_existing_saved_file) and \\\n",
    "                ('fine_tune' in path):\n",
    "            logger.info(f'Removing the existing file: {path}')\n",
    "            os.remove(path)\n",
    "        if os.path.exists(path) and \\\n",
    "                (args.remove_existing_saved_file is not None and 'pre_train' in args.remove_existing_saved_file) and \\\n",
    "                ('pre_train' in path):\n",
    "            logger.info(f'Removing the existing file: {path}')\n",
    "            os.remove(path)\n",
    "        if os.path.exists(path_org) and (args.copy_existing_saved_file is not None and 'pre_train_org' in args.copy_existing_saved_file) and ('pre_train' in path):\n",
    "            logger.info(f'Copying the existing file: {path_org}')\n",
    "            shutil.copy(path_org, path)\n",
    "            \n",
    "        # #######################################################################\n",
    "\n",
    "        if os.path.exists(path) and os.path.isfile(path):\n",
    "            logger.info(f'Trying to load saved binary pickle file from: {path}')\n",
    "            with open(path, mode='rb') as f:\n",
    "                obj = pickle.load(f)\n",
    "            assert isinstance(obj, CodeDataset)\n",
    "            obj.args = args\n",
    "            logger.info(f'Dataset instance loaded from: {path}')\n",
    "            print_paths(obj.paths)\n",
    "            return obj\n",
    "    dataset = CodeDataset(args=args,\n",
    "                          dataset_name=name,\n",
    "                          mode=mode,\n",
    "                          task=task,\n",
    "                          language=language,\n",
    "                          split=split,\n",
    "                          clone_mapping=clone_mapping)\n",
    "    dataset.save()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pre_train(args,\n",
    "              trained_model: Union[BartForClassificationAndGeneration, str] = None,\n",
    "              trained_vocab: Union[Tuple[Vocab, Vocab, Vocab], str] = None):\n",
    "    tasks = args.pre_train_tasks\n",
    "    if tasks is None:\n",
    "        logger.warning('Was specified for pre-training, but got pre-training tasks to None, '\n",
    "                       'will default to {}'.format(','.join(enums.PRE_TRAIN_TASKS)))\n",
    "        tasks = enums.PRE_TRAIN_TASKS\n",
    "    else:\n",
    "        supported_tasks = []\n",
    "        for task in tasks.split(','):\n",
    "            task = task.strip().lower()\n",
    "            if task in enums.PRE_TRAIN_TASKS:\n",
    "                supported_tasks.append(task)\n",
    "            else:\n",
    "                logger.warning(f'Pre-training task {task} is not supported and will be ignored.')\n",
    "        tasks = supported_tasks\n",
    "\n",
    "    assert not trained_model or \\\n",
    "        isinstance(trained_model, str) or \\\n",
    "        isinstance(trained_model, BartForClassificationAndGeneration), \\\n",
    "        f'The model type is not supported, expect Bart model or string of model dir, got {type(trained_model)}'\n",
    "\n",
    "    if trained_vocab is None and args.trained_vocab is not None:\n",
    "        trained_vocab = args.trained_vocab\n",
    "    assert not trained_vocab or isinstance(trained_vocab, str), \\\n",
    "        f'The vocab type is not supported, expect string of vocab dir, got {type(trained_vocab)}'\n",
    "\n",
    "    logger.info('*' * 100)\n",
    "    logger.info('Initializing pre-training environments')\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # datasets\n",
    "    # --------------------------------------------------\n",
    "    logger.info('-' * 100)\n",
    "    logger.info('Loading and parsing datasets')\n",
    "    dataset = init_dataset(args=args, mode=enums.TRAINING_MODE_PRE_TRAIN)\n",
    "    logger.info(f'The size of pre_training set: {len(dataset)}')\n",
    "    if args.pre_train_subset_ratio:\n",
    "        logger.info(f'The pre-train dataset is trimmed to subset due to the argument: '\n",
    "                    f'pre_train_subset_ratio={args.pre_train_subset_ratio}')\n",
    "        dataset = dataset.subset(args.pre_train_subset_ratio)\n",
    "        logger.info('The size of trimmed pre-train set: {}'.format(len(dataset)))\n",
    "        # ######################################################\n",
    "        # Updated for bug-fix, myoungkyu song, 03/23/2024\n",
    "        \"\"\"\n",
    "        if isinstance(dataset, torch.utils.data.Subset):\n",
    "            logger.info('The size of trimmed pre-train set (dataset.dataset): {}'.format(len(dataset.dataset)))\n",
    "            dataset = dataset.dataset\n",
    "        \"\"\"\n",
    "        logger.info('The size of trimmed pre-train set: {}'.format(len(dataset)))\n",
    "\n",
    "    logger.info('Datasets loaded and parsed successfully')\n",
    "\n",
    "    # ##################################################\n",
    "    # Edited to force to initialize vocab, myoungkyu song, 03/23/2024\n",
    "    if trained_vocab == 'None':\n",
    "        trained_vocab = None\n",
    "    # ##################################################\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # vocabs\n",
    "    # --------------------------------------------------\n",
    "    logger.info('-' * 100)\n",
    "    if trained_vocab:\n",
    "        logger.info('Loading vocabularies from files')\n",
    "        code_vocab = load_vocab(vocab_root=trained_vocab, name=args.code_vocab_name)\n",
    "        ast_vocab = load_vocab(vocab_root=trained_vocab, name=args.ast_vocab_name)\n",
    "        nl_vocab = load_vocab(vocab_root=trained_vocab, name=args.nl_vocab_name)\n",
    "    else:\n",
    "        logger.info('Building vocabularies')\n",
    "        # code vocab\n",
    "        code_vocab = init_vocab(vocab_save_dir=args.vocab_save_dir,\n",
    "                                name=args.code_vocab_name,\n",
    "                                method=args.code_tokenize_method,\n",
    "                                vocab_size=args.code_vocab_size,\n",
    "                                datasets=[dataset.codes],\n",
    "                                ignore_case=True,\n",
    "                                save_root=args.vocab_root)\n",
    "        # nl vocab\n",
    "        nl_vocab = init_vocab(vocab_save_dir=args.vocab_save_dir,\n",
    "                              name=args.nl_vocab_name,\n",
    "                              method=args.nl_tokenize_method,\n",
    "                              vocab_size=args.nl_vocab_size,\n",
    "                              datasets=[dataset.names, dataset.docs] if hasattr(dataset, 'docs') else [dataset.names],\n",
    "                              ignore_case=True,\n",
    "                              save_root=args.vocab_root,\n",
    "                              index_offset=len(code_vocab))\n",
    "        # ast vocab\n",
    "        ast_vocab = init_vocab(vocab_save_dir=args.vocab_save_dir,\n",
    "                               name=args.ast_vocab_name,\n",
    "                               method='word',\n",
    "                               datasets=[dataset.asts],\n",
    "                               save_root=args.vocab_root,\n",
    "                               index_offset=len(code_vocab) + len(nl_vocab))\n",
    "    logger.info(f'The size of code vocabulary: {len(code_vocab)}')\n",
    "    logger.info(f'The size of nl vocabulary: {len(nl_vocab)}')\n",
    "    logger.info(f'The size of ast vocabulary: {len(ast_vocab)}')\n",
    "    logger.info('Vocabularies built successfully')\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Model\n",
    "    # --------------------------------------------------\n",
    "    logger.info('-' * 100)\n",
    "    logger.info('Building model')\n",
    "    config = BartConfig(vocab_size=len(code_vocab) + len(ast_vocab) + len(nl_vocab),\n",
    "                        max_position_embeddings=512,\n",
    "                        encoder_layers=args.n_layer,\n",
    "                        encoder_ffn_dim=args.d_ff,\n",
    "                        encoder_attention_heads=args.n_head,\n",
    "                        decoder_layers=args.n_layer,\n",
    "                        decoder_ffn_dim=args.d_ff,\n",
    "                        decoder_attention_heads=args.n_head,\n",
    "                        activation_function='gelu',\n",
    "                        d_model=args.d_model,\n",
    "                        dropout=args.dropout,\n",
    "                        use_cache=True,\n",
    "                        pad_token_id=Vocab.START_VOCAB.index(Vocab.PAD_TOKEN),\n",
    "                        bos_token_id=Vocab.START_VOCAB.index(Vocab.SOS_TOKEN),\n",
    "                        eos_token_id=Vocab.START_VOCAB.index(Vocab.EOS_TOKEN),\n",
    "                        is_encoder_decoder=True,\n",
    "                        decoder_start_token_id=Vocab.START_VOCAB.index(Vocab.SOS_TOKEN),\n",
    "                        forced_eos_token_id=Vocab.START_VOCAB.index(Vocab.EOS_TOKEN),\n",
    "                        max_length=100,\n",
    "                        min_length=1,\n",
    "                        num_beams=args.beam_width,\n",
    "                        num_labels=2)\n",
    "    model = BartForClassificationAndGeneration(config)\n",
    "    # log model statistic\n",
    "    logger.info('Model trainable parameters: {}'.format(human_format(count_params(model))))\n",
    "    table = layer_wise_parameters(model)\n",
    "    logger.debug('Layer-wised trainable parameters:\\n{}'.format(table))\n",
    "    logger.info('Model built successfully')\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # pre-train\n",
    "    # --------------------------------------------------\n",
    "    for task in tasks:\n",
    "        logger.info('-' * 100)\n",
    "        logger.info(f'Pre-training task: {task.upper()}')\n",
    "\n",
    "        if isinstance(dataset, torch.utils.data.Subset):\n",
    "            dataset.dataset.set_task(task)\n",
    "        else:\n",
    "            dataset.set_task(task)\n",
    "\n",
    "        if task == enums.TASK_CODE_AST_PREDICTION:\n",
    "            logger.info('-' * 100)\n",
    "            if args.n_epoch_pre_train != 30:\n",
    "                print(f'n_epoch_pre_train is updated to {args.n_epoch_pre_train}')\n",
    "            # set model mode\n",
    "            logger.info('-' * 100)\n",
    "            # Updated to use 'GEN' instead of 'CLS'\n",
    "            model.set_model_mode(enums.MODEL_MODE_GEN) # model.set_model_mode(enums.MODEL_MODE_CLS)\n",
    "            # --------------------------------------------------\n",
    "            # trainer\n",
    "            # --------------------------------------------------\n",
    "            logger.info('-' * 100)\n",
    "            logger.info('Initializing the running configurations')\n",
    "            # Updated n_epoch to reduce training time\n",
    "            #   File \"/.../envs/spt-code/lib/python3.8/site-packages/transformers/trainer_pt_utils.py\", line 510, in __call__\n",
    "            #     nll_loss = log_probs.gather(dim=-1, index=labels)\n",
    "            # RuntimeError: Index tensor must have the same number of dimensions as input tensor\n",
    "            training_args = TrainingArguments(output_dir=os.path.join(args.pre_train_output_root, task),\n",
    "                                              overwrite_output_dir=True,\n",
    "                                              do_train=True,\n",
    "                                              per_device_train_batch_size=args.batch_size,\n",
    "                                              gradient_accumulation_steps=1,\n",
    "                                              learning_rate=args.learning_rate,\n",
    "                                              weight_decay=args.lr_decay_rate,\n",
    "                                              max_grad_norm=args.grad_clipping_norm,\n",
    "                                              num_train_epochs=args.n_epoch_pre_train, # updated\n",
    "                                              lr_scheduler_type=SchedulerType.LINEAR,\n",
    "                                              warmup_steps=args.warmup_steps,\n",
    "                                              logging_dir=os.path.join(args.tensor_board_root, task),\n",
    "                                              logging_strategy=IntervalStrategy.STEPS,\n",
    "                                              logging_steps=args.logging_steps,\n",
    "                                              save_strategy=IntervalStrategy.NO,\n",
    "                                              seed=args.random_seed,\n",
    "                                              fp16=args.fp16,\n",
    "                                              dataloader_drop_last=False,\n",
    "                                              run_name=args.model_name,\n",
    "                                              load_best_model_at_end=True,\n",
    "                                              ignore_data_skip=False,\n",
    "                                              label_smoothing_factor=args.label_smoothing,\n",
    "                                              report_to=['tensorboard'],\n",
    "                                              dataloader_pin_memory=True)\n",
    "            trainer = CodeCLSTrainer(main_args=args,\n",
    "                                     code_vocab=code_vocab,\n",
    "                                     ast_vocab=ast_vocab,\n",
    "                                     nl_vocab=nl_vocab,\n",
    "                                     task=task,\n",
    "                                     model=model,\n",
    "                                     args=training_args,\n",
    "                                     data_collator=None,\n",
    "                                     train_dataset=dataset,\n",
    "                                     tokenizer=nl_vocab,\n",
    "                                     model_init=None,\n",
    "                                     compute_metrics=None,\n",
    "                                     callbacks=[LogStateCallBack()])\n",
    "            logger.info('Running configurations initialized successfully')\n",
    "\n",
    "            # --------------------------------------------------\n",
    "            # train\n",
    "            # --------------------------------------------------\n",
    "            logger.info('-' * 100)\n",
    "            logger.info(f'Start pre-training task: {task}')\n",
    "            cap_result = trainer.train()\n",
    "            logger.info(f'Pre-training task {task} finished')\n",
    "            trainer.save_model(os.path.join(args.model_root, task))\n",
    "\n",
    "        elif task == enums.TASK_MASS:\n",
    "            logger.info('-' * 100)\n",
    "            if args.n_epoch_pre_train != 30:\n",
    "                print(f'n_epoch_pre_train is updated to {args.n_epoch_pre_train}')\n",
    "            # set model mode\n",
    "            logger.info('-' * 100)\n",
    "            model.set_model_mode(enums.MODEL_MODE_GEN)\n",
    "            # --------------------------------------------------\n",
    "            # trainer\n",
    "            # --------------------------------------------------\n",
    "            # Updated n_epoch to reduce training time, myoungkyu song, 3/20/2024\n",
    "            logger.info('-' * 100)\n",
    "            logger.info('Initializing the running configurations')\n",
    "            training_args = Seq2SeqTrainingArguments(output_dir=os.path.join(args.pre_train_output_root, task),\n",
    "                                                     overwrite_output_dir=True,\n",
    "                                                     do_train=True,\n",
    "                                                     per_device_train_batch_size=args.batch_size,\n",
    "                                                     gradient_accumulation_steps=1,\n",
    "                                                     learning_rate=args.learning_rate,\n",
    "                                                     weight_decay=args.lr_decay_rate,\n",
    "                                                     max_grad_norm=args.grad_clipping_norm,\n",
    "                                                     num_train_epochs=args.n_epoch_pre_train, # updated\n",
    "                                                     lr_scheduler_type=SchedulerType.LINEAR,\n",
    "                                                     warmup_steps=args.warmup_steps,\n",
    "                                                     logging_dir=os.path.join(args.tensor_board_root, task),\n",
    "                                                     logging_strategy=IntervalStrategy.STEPS,\n",
    "                                                     logging_steps=args.logging_steps,\n",
    "                                                     save_strategy=IntervalStrategy.NO,\n",
    "                                                     seed=args.random_seed,\n",
    "                                                     fp16=args.fp16,\n",
    "                                                     dataloader_drop_last=False,\n",
    "                                                     run_name=args.model_name,\n",
    "                                                     load_best_model_at_end=True,\n",
    "                                                     ignore_data_skip=False,\n",
    "                                                     label_smoothing_factor=args.label_smoothing,\n",
    "                                                     report_to=['tensorboard'],\n",
    "                                                     dataloader_pin_memory=True)\n",
    "            trainer = CodeTrainer(main_args=args,\n",
    "                                  code_vocab=code_vocab,\n",
    "                                  ast_vocab=ast_vocab,\n",
    "                                  nl_vocab=nl_vocab,\n",
    "                                  task=task,\n",
    "                                  model=model,\n",
    "                                  args=training_args,\n",
    "                                  data_collator=None,\n",
    "                                  train_dataset=dataset,\n",
    "                                  tokenizer=nl_vocab,\n",
    "                                  model_init=None,\n",
    "                                  compute_metrics=None,\n",
    "                                  callbacks=[LogStateCallBack()])\n",
    "            logger.info('Running configurations initialized successfully')\n",
    "\n",
    "            # --------------------------------------------------\n",
    "            # train\n",
    "            # --------------------------------------------------\n",
    "            logger.info('-' * 100)\n",
    "            logger.info(f'Start pre-training task: {task}')\n",
    "            # model device\n",
    "            logger.info('Device: {}'.format(next(model.parameters()).device))\n",
    "            mass_result = trainer.train()\n",
    "            logger.info(f'Pre-training task {task} finished')\n",
    "            trainer.save_model(os.path.join(args.model_root, task))\n",
    "\n",
    "        elif task == enums.TASK_METHOD_NAME_PREDICTION:\n",
    "            logger.info('-' * 100)\n",
    "            if args.n_epoch_pre_train != 30:\n",
    "                print(f'n_epoch_pre_train is updated to {args.n_epoch_pre_train}')\n",
    "            # set model mode\n",
    "            logger.info('-' * 100)\n",
    "            model.set_model_mode(enums.MODEL_MODE_GEN)\n",
    "            # --------------------------------------------------\n",
    "            # trainer\n",
    "            # --------------------------------------------------\n",
    "            logger.info('-' * 100)\n",
    "            logger.info('Initializing the running configurations')\n",
    "            training_args = Seq2SeqTrainingArguments(output_dir=os.path.join(args.pre_train_output_root, task),\n",
    "                                                     overwrite_output_dir=True,\n",
    "                                                     do_train=True,\n",
    "                                                     per_device_train_batch_size=args.batch_size,\n",
    "                                                     gradient_accumulation_steps=1,\n",
    "                                                     learning_rate=args.learning_rate,\n",
    "                                                     weight_decay=args.lr_decay_rate,\n",
    "                                                     max_grad_norm=args.grad_clipping_norm,\n",
    "                                                     num_train_epochs=args.n_epoch_pre_train, # updated\n",
    "                                                     lr_scheduler_type=SchedulerType.LINEAR,\n",
    "                                                     warmup_steps=args.warmup_steps,\n",
    "                                                     logging_dir=os.path.join(args.tensor_board_root, task),\n",
    "                                                     logging_strategy=IntervalStrategy.STEPS,\n",
    "                                                     logging_steps=args.logging_steps,\n",
    "                                                     save_strategy=IntervalStrategy.NO,\n",
    "                                                     seed=args.random_seed,\n",
    "                                                     fp16=args.fp16,\n",
    "                                                     dataloader_drop_last=False,\n",
    "                                                     run_name=args.model_name,\n",
    "                                                     load_best_model_at_end=True,\n",
    "                                                     ignore_data_skip=False,\n",
    "                                                     label_smoothing_factor=args.label_smoothing,\n",
    "                                                     report_to=['tensorboard'],\n",
    "                                                     dataloader_pin_memory=True)\n",
    "            trainer = CodeTrainer(main_args=args,\n",
    "                                  code_vocab=code_vocab,\n",
    "                                  ast_vocab=ast_vocab,\n",
    "                                  nl_vocab=nl_vocab,\n",
    "                                  task=task,\n",
    "                                  model=model,\n",
    "                                  args=training_args,\n",
    "                                  data_collator=None,\n",
    "                                  train_dataset=dataset,\n",
    "                                  tokenizer=nl_vocab,\n",
    "                                  model_init=None,\n",
    "                                  compute_metrics=None,\n",
    "                                  callbacks=[LogStateCallBack()])\n",
    "            logger.info('Running configurations initialized successfully')\n",
    "\n",
    "            # --------------------------------------------------\n",
    "            # train\n",
    "            # --------------------------------------------------\n",
    "            logger.info('-' * 100)\n",
    "            logger.info(f'Start pre-training task: {task}')\n",
    "            mnp_result = trainer.train()\n",
    "            logger.info(f'Pre-training task {task} finished')\n",
    "            trainer.save_model(os.path.join(args.model_root, task))\n",
    "\n",
    "    logger.info('Pre-training finished')\n",
    "\n",
    "    return model, (code_vocab, ast_vocab, nl_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_python3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
