{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1-selab3/miniconda3/envs/myenv_python3_11/lib/python3.11/site-packages/tree_sitter/__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data\n",
    "from transformers import BartConfig, Seq2SeqTrainingArguments, IntervalStrategy, SchedulerType, TrainingArguments\n",
    "\n",
    "\n",
    "from data.dataset import init_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the attributes of pre_train pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes in dataset: ['__add__', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_is_protocol', 'args', 'asts', 'codes', 'codes_wo_name', 'dataset_dir', 'dataset_name', 'docs', 'languages', 'mode', 'names', 'names_wo_name', 'only_names', 'paths', 'save', 'set_task', 'size', 'sources', 'split', 'subset', 'task']\n",
      "Data-related attributes in dataset: ['_is_protocol', 'args', 'asts', 'codes', 'codes_wo_name', 'dataset_dir', 'dataset_name', 'docs', 'languages', 'mode', 'names', 'names_wo_name', 'only_names', 'paths', 'size', 'sources', 'split', 'task']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "filename = '/home/user1-selab3/Documents/pre_train.pk'\n",
    "\n",
    "with open(filename, 'rb') as file:\n",
    "    dataset = pickle.load(file)\n",
    "\n",
    "attributes = dir(dataset)\n",
    "print(\"Attributes in dataset:\", attributes)\n",
    "\n",
    "data_attributes = [attr for attr in attributes if not attr.startswith('__') and not callable(getattr(dataset, attr))]\n",
    "print(\"Data-related attributes in dataset:\", data_attributes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Check and print information about Codes and Asts if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset length: 654393\n",
      "Length of sliced AST data: 181061\n",
      "Sample AST from sliced data: local_variable_declaration local_variable_declaration if_statement__ parenthesized_expression__ binary_expression__ binary_expression __binary_expression __parenthesized_expression if_statement__ parenthesized_expression expression_statement if_statement__ parenthesized_expression__ binary_expression__ unary_expression __binary_expression __parenthesized_expression return_statement __if_statement expression_statement __if_statement expression_statement if_statement__ parenthesized_expression__ unary_expression __parenthesized_expression return_statement __if_statement __if_statement expression_statement\n",
      "Length of sliced Code data: 181061\n",
      "Sample Code from sliced data: protected final void fastPathOrderedEmit ( U value , boolean delayError , Disposable disposable ) { final Observer < ? super V > observer = downstream ; final SimplePlainQueue < U > q = queue ; if ( wip . get ( ) == 0 && wip . compareAndSet ( 0 , 1 ) ) { if ( q . isEmpty ( ) ) { accept ( observer , value ) ; if ( leave ( - 1 ) == 0 ) { return ; } } else { q . offer ( value ) ; } } else { q . offer ( value ) ; if ( ! enter ( ) ) { return ; } } QueueDrainHelper . drainLoop ( q , observer , delayError , disposable , this ) ; }\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Total dataset length:\", len(dataset))\n",
    "\n",
    "#Java Code lines index\n",
    "index_start = 45423\n",
    "index_end = 226484\n",
    "\n",
    "if hasattr(dataset, 'asts') and isinstance(dataset.asts, list):\n",
    "    sliced_asts = dataset.asts[index_start:index_end]  # Adjust indices as needed\n",
    "    print(\"Length of sliced AST data:\", len(sliced_asts))\n",
    "    if sliced_asts:\n",
    "        print(\"Sample AST from sliced data:\", sliced_asts[0])\n",
    "    else:\n",
    "        print(\"Sliced AST data is empty.\")\n",
    "else:\n",
    "    print(\"The 'asts' attribute does not exist or is not a list.\")\n",
    "\n",
    "\n",
    "if hasattr(dataset, 'codes'):\n",
    "    if isinstance(dataset.codes, list):\n",
    "        sliced_codes = dataset.codes[index_start:index_end]  # Adjust indices as needed\n",
    "        print(\"Length of sliced Code data:\", len(sliced_codes))\n",
    "        if sliced_codes:\n",
    "            print(\"Sample Code from sliced data:\", sliced_codes[0])\n",
    "        else:\n",
    "            print(\"Sliced Code data is empty.\")\n",
    "    else:\n",
    "        print(\"The 'codes' attribute exists but is not a list, it is:\", type(dataset.codes))\n",
    "else:\n",
    "    print(\"The 'codes' attribute does not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the Asts and corresponding codes to jsonl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique AST and Code pairs have been written to 'unique_ast_code_pairs.txt'\n"
     ]
    }
   ],
   "source": [
    "if hasattr(dataset, 'asts') and hasattr(dataset, 'codes') and dataset.asts and dataset.codes:\n",
    "    if len(dataset.asts) != len(dataset.codes):\n",
    "        print(\"The lengths of 'asts' and 'codes' do not match.\")\n",
    "    else:\n",
    "        ast_code_pairs = list(zip(dataset.asts, dataset.codes))\n",
    "        \n",
    "        unique_ast_code_pairs = set(ast_code_pairs)\n",
    "        \n",
    "        with open('unique_ast_code_pairs.jsonl', 'w') as file:\n",
    "            file.write(\"Unique ASTs with corresponding Codes:\\n\")\n",
    "            for ast, code in unique_ast_code_pairs:\n",
    "                file.write(f\"AST: {ast}\\nCode: {code}\\n\\n\")\n",
    "\n",
    "        print(\"Unique AST and Code pairs have been written to 'unique_ast_code_pairs.jsonl'\")\n",
    "else:\n",
    "    print(\"Either 'asts' or 'codes' attribute is empty or does not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Unique components in finetuning files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = '/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/dataset_saved/fine_tune.summarization.java.test.pk'\n",
    "# filename = '/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/dataset_saved/pre_train.pk'\n",
    "\n",
    "with open(filename, 'rb') as file:\n",
    "    dataset = pickle.load(file)\n",
    "    print(len(dataset))\n",
    "    \n",
    "if hasattr(dataset, 'asts'):\n",
    "    print(\"Type of 'asts':\", type(dataset.asts))\n",
    "    if isinstance(dataset.asts, list) and len(dataset.asts) > 0:\n",
    "        print(\"Sample of 'asts':\", dataset.asts[0])\n",
    "    else:\n",
    "        print(\"The 'asts' list is empty.\")\n",
    "else:\n",
    "    print(\"The 'asts' attribute does not exist.\")\n",
    "\n",
    "if hasattr(dataset, 'asts') and dataset.asts:\n",
    "    extracted_data = ' '.join(str(ast) for ast in dataset.asts)\n",
    "    components = extracted_data.split()  \n",
    "    unique_components = list(set(components))  \n",
    "    unique_components.sort() \n",
    "\n",
    "    print(\"Unique components in 'asts':\")\n",
    "    for component in unique_components:\n",
    "        print(component)\n",
    "else:\n",
    "    print(\"The 'asts' attribute is empty or does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if hasattr(dataset, 'asts') and hasattr(dataset, 'codes') and dataset.asts and dataset.codes:\n",
    "#     if len(dataset.asts) != len(dataset.codes):\n",
    "#         print(\"The lengths of 'asts' and 'codes' do not match.\")\n",
    "#     else:\n",
    "#         ast_code_pairs = list(zip(dataset.asts, dataset.codes))\n",
    "        \n",
    "#         unique_ast_code_pairs = set(ast_code_pairs)\n",
    "        \n",
    "#         print(\"Unique ASTs with corresponding Codes:\")\n",
    "#         for ast, code in unique_ast_code_pairs:\n",
    "#             print(\"AST:\", ast, \"Code:\", code)\n",
    "# else:\n",
    "#     print(\"Either 'asts' or 'codes' attribute is empty or does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# filename = '/home/user1-selab3/Documents/pre_train.pk'\n",
    "# # filename = '/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/dataset_saved/pre_train.pk'\n",
    "\n",
    "# with open(filename, 'rb') as file:\n",
    "#     dataset = pickle.load(file)\n",
    "#     print(len(dataset))\n",
    "\n",
    "# if hasattr(dataset, 'asts') and isinstance(dataset.asts, list) and len(dataset.asts) >= 226483:\n",
    "#     sliced_data = dataset.asts[45423:226484]  \n",
    "\n",
    "#     print(\"Length of sliced data:\", len(sliced_data))\n",
    "#     if sliced_data:\n",
    "#         print(\"Sample of 'asts' from sliced data:\", sliced_data[0])\n",
    "#     else:\n",
    "#         print(\"Sliced data is empty.\")\n",
    "# else:\n",
    "#     print(\"The 'asts' attribute does not exist or does not contain enough data.\")\n",
    "\n",
    "# if hasattr(dataset, 'asts') and dataset.asts:\n",
    "#     extracted_data = ' '.join(str(ast) for ast in dataset.asts)\n",
    "#     components = extracted_data.split() \n",
    "#     unique_components = list(set(components))  \n",
    "#     unique_components.sort()  \n",
    "\n",
    "#     print(\"Unique components in 'asts':\")\n",
    "#     for component in unique_components:\n",
    "#         print(component)\n",
    "# else:\n",
    "#     print(\"The 'asts' attribute is empty or does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_python3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
