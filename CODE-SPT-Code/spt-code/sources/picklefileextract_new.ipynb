{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1-selab3/miniconda3/envs/myenv_python3_11/lib/python3.11/site-packages/tree_sitter/__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data\n",
    "from transformers import BartConfig, Seq2SeqTrainingArguments, IntervalStrategy, SchedulerType, TrainingArguments\n",
    "\n",
    "\n",
    "from data.dataset import init_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the attributes of pre_train pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes in dataset: ['__add__', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_is_protocol', 'args', 'asts', 'codes', 'codes_wo_name', 'dataset_dir', 'dataset_name', 'docs', 'languages', 'mode', 'names', 'names_wo_name', 'only_names', 'paths', 'save', 'set_task', 'size', 'sources', 'split', 'subset', 'task']\n",
      "Data-related attributes in dataset: ['_is_protocol', 'args', 'asts', 'codes', 'codes_wo_name', 'dataset_dir', 'dataset_name', 'docs', 'languages', 'mode', 'names', 'names_wo_name', 'only_names', 'paths', 'size', 'sources', 'split', 'task']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "filename = '/home/user1-selab3/Documents/pre_train.pk'\n",
    "\n",
    "with open(filename, 'rb') as file:\n",
    "    dataset = pickle.load(file)\n",
    "\n",
    "attributes = dir(dataset)\n",
    "print(\"Attributes in dataset:\", attributes)\n",
    "\n",
    "data_attributes = [attr for attr in attributes if not attr.startswith('__') and not callable(getattr(dataset, attr))]\n",
    "print(\"Data-related attributes in dataset:\", data_attributes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['func NewSTM ( c * v3 . Client , apply func ( STM ) error , so ... stmOption ) ( * v3 . TxnResponse , error ) { opts := & stmOptions { ctx : c . Ctx ( ) } \\n for _ , f := range so { f ( opts ) \\n } \\n if len ( opts . prefetch ) != 0 { f := apply \\n apply = func ( s STM ) error { s . Get ( opts . prefetch ... ) \\n return f ( s ) \\n } \\n } \\n return runSTM ( mkSTM ( c , opts ) , apply ) \\n }']\n",
      "654393\n",
      "['short_var_declaration__ unary_expression__ call_expression__ selector_expression __call_expression __unary_expression __short_var_declaration for_statement__ call_expression __for_statement if_statement__ binary_expression__ call_expression__ selector_expression __call_expression __binary_expression short_var_declaration assignment_statement__ parameter_declaration call_expression__ selector_expression selector_expression __call_expression return_statement__ call_expression __return_statement __assignment_statement __if_statement return_statement__ call_expression__ call_expression __call_expression __return_statement']\n",
      "654393\n"
     ]
    }
   ],
   "source": [
    "pickle_file_path = '/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset_backup/dataset_saved/pre_train.pk'#fine_tune.completion.java.test.pk'\n",
    "\n",
    "with open(pickle_file_path, 'rb') as file:\n",
    "    dataset = pickle.load(file)\n",
    "\n",
    "# print(dataset.dataset_dir)\n",
    "# print(dataset.args)\n",
    "# print(dataset.asts)\n",
    "\n",
    "print(dataset.codes[:1])\n",
    "print(len(dataset.codes))\n",
    "\n",
    "\n",
    "print(dataset.asts[:1])\n",
    "print(len(dataset.asts))\n",
    "\n",
    "# print(dataset)\n",
    "\n",
    "# first_five_items = dataset[:2]\n",
    "\n",
    "# for item in first_five_items:\n",
    "#     print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Check and print information about Codes and Asts if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset length: 19413\n",
      "Length of sliced AST data: 1000\n",
      "Sample AST from sliced data: local_variable_declaration local_variable_declaration for_statement__ local_variable_declaration binary_expression update_expression local_variable_declaration__ binary_expression__ parenthesized_expression__ binary_expression__ binary_expression __binary_expression __parenthesized_expression __binary_expression __local_variable_declaration local_variable_declaration__ object_creation_expression __local_variable_declaration local_variable_declaration if_statement__ parenthesized_expression__ binary_expression __parenthesized_expression return_statement __if_statement __for_statement throw_statement__ object_creation_expression__ binary_expression __object_creation_expression __throw_statement\n",
      "Length of sliced Code data: 1000\n",
      "Sample Code from sliced data: file getFile ( string dirsProp , string path ) throws ioException { string [ ] dirs = getTrimmedStrings ( dirsProp ) ; int hashCode = path . hashCode ( ) ; for ( int i = 0 ; i < dirs . length ; i ++ ) { int index = ( hashCode + i & [MSK] ) % dirs . length ; file file = new file ( dirs [ index ] , path ) ; file dir = file . getParentFile ( ) ; if ( dir . exists ( ) || dir . mkdirs ( ) ) { return file ; } } throw new ioException ( \"NoValid local directories in property: \" + dirsProp ) ; }\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Total dataset length:\", len(dataset))\n",
    "\n",
    "#Java Code lines index\n",
    "index_start = 0\n",
    "index_end = 1000\n",
    "\n",
    "if hasattr(dataset, 'asts') and isinstance(dataset.asts, list):\n",
    "    sliced_asts = dataset.asts[index_start:index_end]  # Adjust indices as needed\n",
    "    print(\"Length of sliced AST data:\", len(sliced_asts))\n",
    "    if sliced_asts:\n",
    "        print(\"Sample AST from sliced data:\", sliced_asts[0])\n",
    "    else:\n",
    "        print(\"Sliced AST data is empty.\")\n",
    "else:\n",
    "    print(\"The 'asts' attribute does not exist or is not a list.\")\n",
    "\n",
    "\n",
    "if hasattr(dataset, 'codes'):\n",
    "    if isinstance(dataset.codes, list):\n",
    "        sliced_codes = dataset.codes[index_start:index_end]  # Adjust indices as needed\n",
    "        print(\"Length of sliced Code data:\", len(sliced_codes))\n",
    "        if sliced_codes:\n",
    "            print(\"Sample Code from sliced data:\", sliced_codes[0])\n",
    "        else:\n",
    "            print(\"Sliced Code data is empty.\")\n",
    "    else:\n",
    "        print(\"The 'codes' attribute exists but is not a list, it is:\", type(dataset.codes))\n",
    "else:\n",
    "    print(\"The 'codes' attribute does not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the Asts and corresponding codes to jsonl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique AST and Code pairs have been written to 'unique_ast_code_pairs.txt'\n"
     ]
    }
   ],
   "source": [
    "if hasattr(dataset, 'asts') and hasattr(dataset, 'codes') and dataset.asts and dataset.codes:\n",
    "    if len(dataset.asts) != len(dataset.codes):\n",
    "        print(\"The lengths of 'asts' and 'codes' do not match.\")\n",
    "    else:\n",
    "        ast_code_pairs = list(zip(dataset.asts, dataset.codes))\n",
    "        \n",
    "        unique_ast_code_pairs = set(ast_code_pairs)\n",
    "        \n",
    "        with open('unique_ast_code_pairs.jsonl', 'w') as file:\n",
    "            file.write(\"Unique ASTs with corresponding Codes:\\n\")\n",
    "            for ast, code in unique_ast_code_pairs:\n",
    "                file.write(f\"AST: {ast}\\nCode: {code}\\n\\n\")\n",
    "\n",
    "        print(\"Unique AST and Code pairs have been written to 'unique_ast_code_pairs.jsonl'\")\n",
    "else:\n",
    "    print(\"Either 'asts' or 'codes' attribute is empty or does not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Unique components in finetuning files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9820\n",
      "Type of 'asts': <class 'list'>\n",
      "Sample of 'asts': local_variable_declaration return_statement__ binary_expression__ binary_expression__ binary_expression__ binary_expression__ binary_expression binary_expression __binary_expression binary_expression __binary_expression binary_expression __binary_expression binary_expression __binary_expression binary_expression__ binary_expression__ binary_expression binary_expression __binary_expression binary_expression __binary_expression __return_statement\n",
      "Unique components in 'asts':\n",
      "__array_creation_expression\n",
      "__assert_statement\n",
      "__assignment_expression\n",
      "__binary_expression\n",
      "__cast_expression\n",
      "__class_declaration\n",
      "__do_statement\n",
      "__enhanced_for_statement\n",
      "__expression_statement\n",
      "__for_statement\n",
      "__if_statement\n",
      "__labeled_statement\n",
      "__local_variable_declaration\n",
      "__object_creation_expression\n",
      "__parenthesized_expression\n",
      "__return_statement\n",
      "__switch_expression\n",
      "__synchronized_statement\n",
      "__ternary_expression\n",
      "__throw_statement\n",
      "__try_statement\n",
      "__unary_expression\n",
      "__while_statement\n",
      "array_creation_expression\n",
      "array_creation_expression__\n",
      "assert_statement\n",
      "assert_statement__\n",
      "assignment_expression\n",
      "assignment_expression__\n",
      "binary_expression\n",
      "binary_expression__\n",
      "break_statement\n",
      "cast_expression\n",
      "cast_expression__\n",
      "class_declaration__\n",
      "continue_statement\n",
      "do_statement__\n",
      "enhanced_for_statement__\n",
      "expression_statement\n",
      "expression_statement__\n",
      "field_declaration\n",
      "for_statement__\n",
      "if_statement__\n",
      "instanceof_expression\n",
      "labeled_statement__\n",
      "local_variable_declaration\n",
      "local_variable_declaration__\n",
      "object_creation_expression\n",
      "object_creation_expression__\n",
      "parenthesized_expression\n",
      "parenthesized_expression__\n",
      "return_statement\n",
      "return_statement__\n",
      "switch_expression__\n",
      "synchronized_statement__\n",
      "ternary_expression\n",
      "ternary_expression__\n",
      "throw_statement\n",
      "throw_statement__\n",
      "try_statement__\n",
      "unary_expression\n",
      "unary_expression__\n",
      "update_expression\n",
      "while_statement__\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "filename = '/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset_backup/dataset_saved/fine_tune.completion.valid.pk'\n",
    "# filename = '/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/dataset_saved/pre_train.pk'\n",
    "\n",
    "with open(filename, 'rb') as file:\n",
    "    dataset = pickle.load(file)\n",
    "    print(len(dataset))\n",
    "    \n",
    "if hasattr(dataset, 'asts'):\n",
    "    print(\"Type of 'asts':\", type(dataset.asts))\n",
    "    if isinstance(dataset.asts, list) and len(dataset.asts) > 0:\n",
    "        print(\"Sample of 'asts':\", dataset.asts[0])\n",
    "    else:\n",
    "        print(\"The 'asts' list is empty.\")\n",
    "else:\n",
    "    print(\"The 'asts' attribute does not exist.\")\n",
    "\n",
    "if hasattr(dataset, 'asts') and dataset.asts:\n",
    "    extracted_data = ' '.join(str(ast) for ast in dataset.asts)\n",
    "    components = extracted_data.split()  \n",
    "    unique_components = list(set(components))  \n",
    "    unique_components.sort() \n",
    "\n",
    "    print(\"Unique components in 'asts':\")\n",
    "    for component in unique_components:\n",
    "        print(component)\n",
    "else:\n",
    "    print(\"The 'asts' attribute is empty or does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concat cleaned method and their respective ast into a file since java is not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The files were concatenated successfully and saved to finetune_methods_test.txt.\n"
     ]
    }
   ],
   "source": [
    "# Open the two source files and the output file\n",
    "with open('/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/finetune_raw/java-small-json/finetune_methods_train_final.txt', 'r') as source_file, \\\n",
    "     open('/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/asts/finetune/asts_train.txt', 'r') as asts_file, \\\n",
    "     open('/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/asts/finetune/finetune_methods_train.txt', 'w') as output_file:\n",
    "\n",
    "    # Read lines from both files\n",
    "    source_lines = source_file.readlines()\n",
    "    asts_lines = asts_file.readlines()\n",
    "\n",
    "    # Iterate over the lines from both files\n",
    "    for source_line, asts_line in zip(source_lines, asts_lines):\n",
    "        # Strip newline characters and concatenate the lines with the desired format\n",
    "        source_line_clean = source_line.replace('\"', '').strip()\n",
    "        asts_line_clean = asts_line.strip()\n",
    "\n",
    "        formatted_line = f'Source: \"{source_line_clean}\", Asts: \"{asts_line_clean}\"\\n'\n",
    "        \n",
    "        # Write the formatted line to the output file\n",
    "        output_file.write(formatted_line)\n",
    "\n",
    "print(\"The files were concatenated successfully and saved to finetune_methods_test.txt.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare a finetuning dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been processed and saved to finetune_raw_valid.jsonl.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Path to your JSON file\n",
    "file_path = '/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/finetune_raw/java-small-json/java-small.val.json'\n",
    "\n",
    "# Open and read the file line by line (assuming each line is a separate JSON object)\n",
    "results = []\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            # Parse each line as a JSON object\n",
    "            json_object = json.loads(line)\n",
    "            # Format the desired fields\n",
    "            formatted_string = f\"{json_object['left_context']} {json_object['target_seq']} {json_object['right_context']}\"\n",
    "            results.append(formatted_string)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error decoding JSON from line:\", line)\n",
    "            continue\n",
    "\n",
    "# Write all formatted strings to a new file\n",
    "with open('finetune_raw_valid.txt', 'w') as output_file:\n",
    "    for item in results:\n",
    "        output_file.write(item + '\\n')\n",
    "\n",
    "print(\"Data has been processed and saved to finetune_raw_valid.txt.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total input lines read: 1309842\n",
      "Total output lines prepared: 1309842\n",
      "Data has been processed and saved to /home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/finetune_raw/java-small-json/finetune_raw_train.txt.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Regular expression pattern to match string literals\n",
    "STRING_MATCHING_PATTERN = re.compile(r'([bruf]*)(\\\"\\\"\\\"|\\'\\'\\'|\\\"|\\')(?:(?!\\2)(?:\\\\.|[^\\\\]))*\\2')\n",
    "\n",
    "def replace_string_literal(source):\n",
    "    \"\"\"Replace string literals in the source code with '___STR'.\"\"\"\n",
    "    return re.sub(pattern=STRING_MATCHING_PATTERN, repl='___STR', string=source)\n",
    "\n",
    "# Path to your JSON file\n",
    "file_path = '/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/finetune_raw/java-small-json/java-small.train.json'\n",
    "output_file_path = '/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/finetune_raw/java-small-json/finetune_raw_train.txt'\n",
    "\n",
    "results = []  # Store the output lines here\n",
    "\n",
    "# Open and read the file line by line (assuming each line is a separate JSON object)\n",
    "with open(file_path, 'r') as file:\n",
    "    line_count = 0  # Counter for input lines\n",
    "    for line in file:\n",
    "        line_count += 1\n",
    "        try:\n",
    "            # Parse each line as a JSON object\n",
    "            json_object = json.loads(line)\n",
    "            # Replace string literals and concatenate the desired fields\n",
    "            left_context = replace_string_literal(json_object['left_context']).replace('\\n', '\\\\n').replace('=', '\\\\u003d').replace(\"\\\\\", \"\\\\\\\\\").replace(\"\\\"\", \"\\\\\\\"\").replace(\"\\r\", \"\\\\r\").replace(\"\\t\", \"\\\\t\")\n",
    "            right_context = replace_string_literal(json_object['right_context']).replace('\\n', '\\\\n').replace('=', '\\\\u003d').replace(\"\\\\\", \"\\\\\\\\\").replace(\"\\\"\", \"\\\\\\\"\").replace(\"\\r\", \"\\\\r\").replace(\"\\t\", \"\\\\t\")\n",
    "            target_seq = replace_string_literal(json_object['target_seq']).replace('\\n', '\\\\n').replace('=', '\\\\u003d').replace(\"\\\\\", \"\\\\\\\\\").replace(\"\\\"\", \"\\\\\\\"\").replace(\"\\r\", \"\\\\r\").replace(\"\\t\", \"\\\\t\")\n",
    "            formatted_string = f\"{left_context} {target_seq} {right_context}\"\n",
    "            # Enclose the concatenated string in quotes\n",
    "            enclosed_string = f\"\\\"{formatted_string}\\\"\"\n",
    "            results.append(enclosed_string)\n",
    "        except json.JSONDecodeError:\n",
    "            # Log the error and append a placeholder line\n",
    "            print(f\"Error decoding JSON on input line {line_count}\")\n",
    "            results.append(f\"\\\"Error in JSON format on line {line_count}\\\"\")\n",
    "\n",
    "# Print the line counts\n",
    "print(f\"Total input lines read: {line_count}\")\n",
    "print(f\"Total output lines prepared: {len(results)}\")\n",
    "\n",
    "# Write all prepared lines to the output file\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    for item in results:\n",
    "        output_file.write(item + '\\n')\n",
    "\n",
    "print(f\"Data has been processed and saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has 1309842 lines.\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the JSON file\n",
    "file_path = '/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/finetune_raw/java-small-json/java-small.train.json'\n",
    "\n",
    "# Initialize a counter for the lines\n",
    "line_count = 0\n",
    "\n",
    "# Open the file and count the lines\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        line_count += 1\n",
    "\n",
    "# Print the number of lines in the file\n",
    "print(f'The file has {line_count} lines.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if hasattr(dataset, 'asts') and hasattr(dataset, 'codes') and dataset.asts and dataset.codes:\n",
    "#     if len(dataset.asts) != len(dataset.codes):\n",
    "#         print(\"The lengths of 'asts' and 'codes' do not match.\")\n",
    "#     else:\n",
    "#         ast_code_pairs = list(zip(dataset.asts, dataset.codes))\n",
    "        \n",
    "#         unique_ast_code_pairs = set(ast_code_pairs)\n",
    "        \n",
    "#         print(\"Unique ASTs with corresponding Codes:\")\n",
    "#         for ast, code in unique_ast_code_pairs:\n",
    "#             print(\"AST:\", ast, \"Code:\", code)\n",
    "# else:\n",
    "#     print(\"Either 'asts' or 'codes' attribute is empty or does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# filename = '/home/user1-selab3/Documents/pre_train.pk'\n",
    "# # filename = '/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/dataset_saved/pre_train.pk'\n",
    "\n",
    "# with open(filename, 'rb') as file:\n",
    "#     dataset = pickle.load(file)\n",
    "#     print(len(dataset))\n",
    "\n",
    "# if hasattr(dataset, 'asts') and isinstance(dataset.asts, list) and len(dataset.asts) >= 226483:\n",
    "#     sliced_data = dataset.asts[45423:226484]  \n",
    "\n",
    "#     print(\"Length of sliced data:\", len(sliced_data))\n",
    "#     if sliced_data:\n",
    "#         print(\"Sample of 'asts' from sliced data:\", sliced_data[0])\n",
    "#     else:\n",
    "#         print(\"Sliced data is empty.\")\n",
    "# else:\n",
    "#     print(\"The 'asts' attribute does not exist or does not contain enough data.\")\n",
    "\n",
    "# if hasattr(dataset, 'asts') and dataset.asts:\n",
    "#     extracted_data = ' '.join(str(ast) for ast in dataset.asts)\n",
    "#     components = extracted_data.split() \n",
    "#     unique_components = list(set(components))  \n",
    "#     unique_components.sort()  \n",
    "\n",
    "#     print(\"Unique components in 'asts':\")\n",
    "#     for component in unique_components:\n",
    "#         print(component)\n",
    "# else:\n",
    "#     print(\"The 'asts' attribute is empty or does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_python3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
