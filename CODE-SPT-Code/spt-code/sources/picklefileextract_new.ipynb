{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1-selab3/miniconda3/envs/myenv_python3_11/lib/python3.11/site-packages/tree_sitter/__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data\n",
    "from transformers import BartConfig, Seq2SeqTrainingArguments, IntervalStrategy, SchedulerType, TrainingArguments\n",
    "\n",
    "\n",
    "from data.dataset import init_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the attributes of pre_train pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes in dataset: ['EOS_TOKEN', 'MSK_TOKEN', 'PAD_TOKEN', 'SEP_TOKEN', 'SOS_TOKEN', 'START_VOCAB', 'UNK_TOKEN', '_Vocab__special_symbols', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'add_special_symbols', 'decode', 'decode_batch', 'encode_batch', 'encode_sequence', 'eos_processor', 'get_eos_index', 'get_index', 'get_mask_index', 'get_pad_index', 'get_sos_index', 'get_token', 'get_unk_index', 'ignore_case', 'index_offset', 'method', 'name', 'num_special_token', 'pad_token_id', 'restore_index', 'save', 'save_pickle', 'save_pretrained', 'sep_processor', 'sos_processor', 'tokenizer', 'transfer_index']\n",
      "Data-related attributes in dataset: ['EOS_TOKEN', 'MSK_TOKEN', 'PAD_TOKEN', 'SEP_TOKEN', 'SOS_TOKEN', 'START_VOCAB', 'UNK_TOKEN', '_Vocab__special_symbols', 'eos_processor', 'ignore_case', 'index_offset', 'method', 'name', 'pad_token_id', 'sep_processor', 'sos_processor', 'tokenizer']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "filename = '/home/user1-selab3/Documents/pre_train.pk'\n",
    "\n",
    "with open(filename, 'rb') as file:\n",
    "    dataset = pickle.load(file)\n",
    "\n",
    "attributes = dir(dataset)\n",
    "print(\"Attributes in dataset:\", attributes)\n",
    "\n",
    "data_attributes = [attr for attr in attributes if not attr.startswith('__') and not callable(getattr(dataset, attr))]\n",
    "print(\"Data-related attributes in dataset:\", data_attributes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@ Override public ImageSource apply ( ImageSource input ) { final int [ ] [ ] pixelMatrix = new int [ 3 ] [ 3 ] ; int w = input . getWidth ( ) ; int h = input . getHeight ( ) ; int [ ] [ ] output = new int [ h ] [ w ] ; for ( int j = 1 ; j < h - 1 ; j ++ ) { for ( int i = 1 ; i < w - 1 ; i ++ ) { pixelMatrix [ 0 ] [ 0 ] = input . getR ( i - 1 , j - 1 ) ; pixelMatrix [ 0 ] [ 1 ] = input . getRGB ( i - 1 , j ) ; pixelMatrix [ 0 ] [ 2 ] = input . getRGB ( i - 1 , j + 1 ) ; pixelMatrix [ 1 ] [ 0 ] = input . getRGB ( i , j - 1 ) ; pixelMatrix [ 1 ] [ 2 ] = input . getRGB ( i , j + 1 ) ; pixelMatrix [ 2 ] [ 0 ] = input . getRGB ( i + 1 , j - 1 ) ; pixelMatrix [ 2 ] [ 1 ] = input . getRGB ( i + 1 , j ) ; pixelMatrix [ 2 ] [ 2 ] = input . getRGB ( i + 1 , j + 1 ) ; int edge = ( int ) convolution ( pixelMatrix ) ; int rgb = ( edge << 16 | edge << 8 | edge ) ; output [ j ] [ i ] = rgb ; } } MatrixSource source = new MatrixSource ( output ) ; return source ; }']\n",
      "164273\n",
      "['local_variable_declaration__ array_creation_expression __local_variable_declaration local_variable_declaration local_variable_declaration local_variable_declaration__ array_creation_expression __local_variable_declaration for_statement__ local_variable_declaration binary_expression__ binary_expression __binary_expression update_expression for_statement__ local_variable_declaration binary_expression__ binary_expression __binary_expression update_expression expression_statement__ assignment_expression__ binary_expression binary_expression __assignment_expression __expression_statement expression_statement__ assignment_expression__ binary_expression __assignment_expression __expression_statement expression_statement__ assignment_expression__ binary_expression binary_expression __assignment_expression __expression_statement expression_statement__ assignment_expression__ binary_expression __assignment_expression __expression_statement expression_statement__ assignment_expression__ binary_expression __assignment_expression __expression_statement expression_statement__ assignment_expression__ binary_expression binary_expression __assignment_expression __expression_statement expression_statement__ assignment_expression__ binary_expression __assignment_expression __expression_statement expression_statement__ assignment_expression__ binary_expression binary_expression __assignment_expression __expression_statement local_variable_declaration__ cast_expression __local_variable_declaration local_variable_declaration__ parenthesized_expression__ binary_expression__ binary_expression__ binary_expression binary_expression __binary_expression __binary_expression __parenthesized_expression __local_variable_declaration expression_statement__ assignment_expression __expression_statement __for_statement __for_statement local_variable_declaration__ object_creation_expression __local_variable_declaration return_statement']\n",
      "164273\n"
     ]
    }
   ],
   "source": [
    "pickle_file_path = '/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset_backup/dataset_saved/fine_tune.search.java.train.pk'#fine_tune.completion.java.test.pk'\n",
    "\n",
    "with open(pickle_file_path, 'rb') as file:\n",
    "    dataset = pickle.load(file)\n",
    "\n",
    "# print(dataset.dataset_dir)\n",
    "# print(dataset.args)\n",
    "# print(dataset.asts)\n",
    "\n",
    "print(dataset.codes[:1])\n",
    "print(len(dataset.codes))\n",
    "\n",
    "\n",
    "print(dataset.asts[:1])\n",
    "print(len(dataset.asts))\n",
    "\n",
    "# print(dataset)\n",
    "\n",
    "# first_five_items = dataset[:2]\n",
    "\n",
    "# for item in first_five_items:\n",
    "#     print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Check and print information about Codes and Asts if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset length: 19413\n",
      "Length of sliced AST data: 1000\n",
      "Sample AST from sliced data: local_variable_declaration local_variable_declaration for_statement__ local_variable_declaration binary_expression update_expression local_variable_declaration__ binary_expression__ parenthesized_expression__ binary_expression__ binary_expression __binary_expression __parenthesized_expression __binary_expression __local_variable_declaration local_variable_declaration__ object_creation_expression __local_variable_declaration local_variable_declaration if_statement__ parenthesized_expression__ binary_expression __parenthesized_expression return_statement __if_statement __for_statement throw_statement__ object_creation_expression__ binary_expression __object_creation_expression __throw_statement\n",
      "Length of sliced Code data: 1000\n",
      "Sample Code from sliced data: file getFile ( string dirsProp , string path ) throws ioException { string [ ] dirs = getTrimmedStrings ( dirsProp ) ; int hashCode = path . hashCode ( ) ; for ( int i = 0 ; i < dirs . length ; i ++ ) { int index = ( hashCode + i & [MSK] ) % dirs . length ; file file = new file ( dirs [ index ] , path ) ; file dir = file . getParentFile ( ) ; if ( dir . exists ( ) || dir . mkdirs ( ) ) { return file ; } } throw new ioException ( \"NoValid local directories in property: \" + dirsProp ) ; }\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Total dataset length:\", len(dataset))\n",
    "\n",
    "#Java Code lines index\n",
    "index_start = 0\n",
    "index_end = 1000\n",
    "\n",
    "if hasattr(dataset, 'asts') and isinstance(dataset.asts, list):\n",
    "    sliced_asts = dataset.asts[index_start:index_end]  # Adjust indices as needed\n",
    "    print(\"Length of sliced AST data:\", len(sliced_asts))\n",
    "    if sliced_asts:\n",
    "        print(\"Sample AST from sliced data:\", sliced_asts[0])\n",
    "    else:\n",
    "        print(\"Sliced AST data is empty.\")\n",
    "else:\n",
    "    print(\"The 'asts' attribute does not exist or is not a list.\")\n",
    "\n",
    "\n",
    "if hasattr(dataset, 'codes'):\n",
    "    if isinstance(dataset.codes, list):\n",
    "        sliced_codes = dataset.codes[index_start:index_end]  # Adjust indices as needed\n",
    "        print(\"Length of sliced Code data:\", len(sliced_codes))\n",
    "        if sliced_codes:\n",
    "            print(\"Sample Code from sliced data:\", sliced_codes[0])\n",
    "        else:\n",
    "            print(\"Sliced Code data is empty.\")\n",
    "    else:\n",
    "        print(\"The 'codes' attribute exists but is not a list, it is:\", type(dataset.codes))\n",
    "else:\n",
    "    print(\"The 'codes' attribute does not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the Asts and corresponding codes to jsonl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique AST and Code pairs have been written to 'unique_ast_code_pairs.txt'\n"
     ]
    }
   ],
   "source": [
    "if hasattr(dataset, 'asts') and hasattr(dataset, 'codes') and dataset.asts and dataset.codes:\n",
    "    if len(dataset.asts) != len(dataset.codes):\n",
    "        print(\"The lengths of 'asts' and 'codes' do not match.\")\n",
    "    else:\n",
    "        ast_code_pairs = list(zip(dataset.asts, dataset.codes))\n",
    "        \n",
    "        unique_ast_code_pairs = set(ast_code_pairs)\n",
    "        \n",
    "        with open('unique_ast_code_pairs.jsonl', 'w') as file:\n",
    "            file.write(\"Unique ASTs with corresponding Codes:\\n\")\n",
    "            for ast, code in unique_ast_code_pairs:\n",
    "                file.write(f\"AST: {ast}\\nCode: {code}\\n\\n\")\n",
    "\n",
    "        print(\"Unique AST and Code pairs have been written to 'unique_ast_code_pairs.jsonl'\")\n",
    "else:\n",
    "    print(\"Either 'asts' or 'codes' attribute is empty or does not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Unique components in finetuning files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "654393\n",
      "Type of 'asts': <class 'list'>\n",
      "Sample of 'asts': short_var_declaration__ unary_expression__ call_expression__ selector_expression __call_expression __unary_expression __short_var_declaration for_statement__ call_expression __for_statement if_statement__ binary_expression__ call_expression__ selector_expression __call_expression __binary_expression short_var_declaration assignment_statement__ parameter_declaration call_expression__ selector_expression selector_expression __call_expression return_statement__ call_expression __return_statement __assignment_statement __if_statement return_statement__ call_expression__ call_expression __call_expression __return_statement\n",
      "Unique components in 'asts':\n",
      "__anonymous_function_creation_expression\n",
      "__array_creation_expression\n",
      "__assert_statement\n",
      "__assignment\n",
      "__assignment_expression\n",
      "__assignment_statement\n",
      "__augmented_assignment\n",
      "__augmented_assignment_expression\n",
      "__await_expression\n",
      "__binary_expression\n",
      "__call_expression\n",
      "__call_statement\n",
      "__case_statement\n",
      "__cast_expression\n",
      "__class_constant_access_expression\n",
      "__class_declaration\n",
      "__clone_expression\n",
      "__compound_statement\n",
      "__conditional_expression\n",
      "__const_declaration\n",
      "__constructor_declaration\n",
      "__continue_statement\n",
      "__dec_statement\n",
      "__default_statement\n",
      "__defer_statement\n",
      "__do_statement\n",
      "__echo_statement\n",
      "__else_statement\n",
      "__enhanced_for_statement\n",
      "__exponentiation_expression\n",
      "__expression_statement\n",
      "__expression_switch_statement\n",
      "__field_declaration\n",
      "__for_in_statement\n",
      "__for_statement\n",
      "__foreach_statement\n",
      "__function_call_expression\n",
      "__go_statement\n",
      "__if_statement\n",
      "__inc_statement\n",
      "__include_expression\n",
      "__include_once_expression\n",
      "__index_expression\n",
      "__instanceof_expression\n",
      "__jsx_expression\n",
      "__labeled_statement\n",
      "__lambda_expression\n",
      "__local_variable_declaration\n",
      "__member_access_expression\n",
      "__member_call_expression\n",
      "__member_expression\n",
      "__method_declaration\n",
      "__new_expression\n",
      "__object_creation_expression\n",
      "__operator_assignment_statement\n",
      "__parameter_declaration\n",
      "__parenthesized_expression\n",
      "__receive_statement\n",
      "__require_expression\n",
      "__require_once_expression\n",
      "__rescue_statement\n",
      "__return_statement\n",
      "__scoped_call_expression\n",
      "__scoped_property_access_expression\n",
      "__select_statement\n",
      "__selector_expression\n",
      "__send_statement\n",
      "__sequence_expression\n",
      "__short_var_declaration\n",
      "__slice_expression\n",
      "__subscript_expression\n",
      "__switch_expression\n",
      "__switch_statement\n",
      "__synchronized_statement\n",
      "__ternary_expression\n",
      "__throw_expression\n",
      "__throw_statement\n",
      "__try_statement\n",
      "__try_with_resources_statement\n",
      "__type_assertion_expression\n",
      "__type_conversion_expression\n",
      "__type_declaration\n",
      "__type_switch_statement\n",
      "__unary_expression\n",
      "__unary_op_expression\n",
      "__unless_statement\n",
      "__unset_statement\n",
      "__until_statement\n",
      "__update_expression\n",
      "__var_declaration\n",
      "__when_statement\n",
      "__while_statement\n",
      "__with_statement\n",
      "__yield_expression\n",
      "__yield_statement\n",
      "anonymous_function_creation_expression__\n",
      "array_creation_expression\n",
      "array_creation_expression__\n",
      "assert_statement\n",
      "assert_statement__\n",
      "assignment\n",
      "assignment__\n",
      "assignment_expression\n",
      "assignment_expression__\n",
      "assignment_statement\n",
      "assignment_statement__\n",
      "augmented_assignment\n",
      "augmented_assignment__\n",
      "augmented_assignment_expression\n",
      "augmented_assignment_expression__\n",
      "await_expression\n",
      "await_expression__\n",
      "binary_expression\n",
      "binary_expression__\n",
      "break_statement\n",
      "call_expression\n",
      "call_expression__\n",
      "call_statement\n",
      "call_statement__\n",
      "case_statement\n",
      "case_statement__\n",
      "cast_expression\n",
      "cast_expression__\n",
      "class_constant_access_expression\n",
      "class_constant_access_expression__\n",
      "class_declaration__\n",
      "clone_expression\n",
      "clone_expression__\n",
      "compound_statement\n",
      "compound_statement__\n",
      "conditional_expression\n",
      "conditional_expression__\n",
      "const_declaration\n",
      "const_declaration__\n",
      "constructor_declaration\n",
      "constructor_declaration__\n",
      "continue_statement\n",
      "continue_statement__\n",
      "debugger_statement\n",
      "dec_statement\n",
      "dec_statement__\n",
      "declare_statement\n",
      "default_statement\n",
      "default_statement__\n",
      "defer_statement__\n",
      "delete_statement\n",
      "destructured_left_assignment_statement\n",
      "do_statement__\n",
      "echo_statement\n",
      "echo_statement__\n",
      "else_statement\n",
      "else_statement__\n",
      "empty_statement\n",
      "enhanced_for_statement\n",
      "enhanced_for_statement__\n",
      "exponentiation_expression\n",
      "exponentiation_expression__\n",
      "expression_statement\n",
      "expression_statement__\n",
      "expression_switch_statement\n",
      "expression_switch_statement__\n",
      "fallthrough_statement\n",
      "field_declaration\n",
      "field_declaration__\n",
      "for_in_statement\n",
      "for_in_statement__\n",
      "for_statement\n",
      "for_statement__\n",
      "foreach_statement__\n",
      "function_call_expression\n",
      "function_call_expression__\n",
      "global_statement\n",
      "go_statement__\n",
      "goto_statement\n",
      "if_statement\n",
      "if_statement__\n",
      "import_from_statement\n",
      "import_statement\n",
      "inc_statement\n",
      "inc_statement__\n",
      "include_expression\n",
      "include_expression__\n",
      "include_once_expression\n",
      "include_once_expression__\n",
      "index_expression\n",
      "index_expression__\n",
      "instanceof_expression\n",
      "instanceof_expression__\n",
      "jsx_expression\n",
      "jsx_expression__\n",
      "labeled_statement\n",
      "labeled_statement__\n",
      "lambda_expression\n",
      "lambda_expression__\n",
      "local_variable_declaration\n",
      "local_variable_declaration__\n",
      "member_access_expression\n",
      "member_access_expression__\n",
      "member_call_expression\n",
      "member_call_expression__\n",
      "member_expression\n",
      "member_expression__\n",
      "method_declaration\n",
      "method_declaration__\n",
      "named_label_statement\n",
      "new_expression\n",
      "new_expression__\n",
      "nonlocal_statement\n",
      "object_creation_expression\n",
      "object_creation_expression__\n",
      "operator_assignment_statement\n",
      "operator_assignment_statement__\n",
      "parameter_declaration\n",
      "parameter_declaration__\n",
      "parenthesized_expression\n",
      "parenthesized_expression__\n",
      "pass_statement\n",
      "print_statement\n",
      "raise_statement\n",
      "receive_statement__\n",
      "require_expression\n",
      "require_expression__\n",
      "require_once_expression\n",
      "require_once_expression__\n",
      "rescue_statement\n",
      "rescue_statement__\n",
      "rest_assignment_statement\n",
      "return_statement\n",
      "return_statement__\n",
      "scoped_call_expression\n",
      "scoped_call_expression__\n",
      "scoped_property_access_expression\n",
      "scoped_property_access_expression__\n",
      "select_statement\n",
      "select_statement__\n",
      "selector_expression\n",
      "selector_expression__\n",
      "send_statement\n",
      "send_statement__\n",
      "sequence_expression\n",
      "sequence_expression__\n",
      "shell_command_expression\n",
      "short_var_declaration\n",
      "short_var_declaration__\n",
      "slice_expression\n",
      "slice_expression__\n",
      "subscript_expression\n",
      "subscript_expression__\n",
      "switch_expression__\n",
      "switch_statement__\n",
      "synchronized_statement__\n",
      "ternary_expression\n",
      "ternary_expression__\n",
      "throw_expression\n",
      "throw_expression__\n",
      "throw_statement\n",
      "throw_statement__\n",
      "try_statement\n",
      "try_statement__\n",
      "try_with_resources_statement\n",
      "try_with_resources_statement__\n",
      "type_assertion_expression\n",
      "type_assertion_expression__\n",
      "type_conversion_expression\n",
      "type_conversion_expression__\n",
      "type_declaration\n",
      "type_declaration__\n",
      "type_switch_statement__\n",
      "unary_expression\n",
      "unary_expression__\n",
      "unary_op_expression\n",
      "unary_op_expression__\n",
      "unless_statement\n",
      "unless_statement__\n",
      "unset_statement\n",
      "unset_statement__\n",
      "until_statement\n",
      "until_statement__\n",
      "update_expression\n",
      "update_expression__\n",
      "var_declaration\n",
      "var_declaration__\n",
      "variadic_parameter_declaration\n",
      "when_statement\n",
      "when_statement__\n",
      "while_statement__\n",
      "with_statement\n",
      "with_statement__\n",
      "yield_expression\n",
      "yield_expression__\n",
      "yield_statement__\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "#filename = '/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset_backup/dataset_saved/fine_tune.search.java.valid.pk'\n",
    "filename = '/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset_backup/dataset_saved/pre_train.pk'\n",
    "\n",
    "with open(filename, 'rb') as file:\n",
    "    dataset = pickle.load(file)\n",
    "    print(len(dataset))\n",
    "    \n",
    "if hasattr(dataset, 'asts'):\n",
    "    print(\"Type of 'asts':\", type(dataset.asts))\n",
    "    if isinstance(dataset.asts, list) and len(dataset.asts) > 0:\n",
    "        print(\"Sample of 'asts':\", dataset.asts[0])\n",
    "    else:\n",
    "        print(\"The 'asts' list is empty.\")\n",
    "else:\n",
    "    print(\"The 'asts' attribute does not exist.\")\n",
    "\n",
    "if hasattr(dataset, 'asts') and dataset.asts:\n",
    "    extracted_data = ' '.join(str(ast) for ast in dataset.asts)\n",
    "    components = extracted_data.split()  \n",
    "    unique_components = list(set(components))  \n",
    "    unique_components.sort() \n",
    "\n",
    "    print(\"Unique components in 'asts':\")\n",
    "    for component in unique_components:\n",
    "        print(component)\n",
    "else:\n",
    "    print(\"The 'asts' attribute is empty or does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The files were concatenated successfully and saved to ast_jdt_nongen_output_file.\n"
     ]
    }
   ],
   "source": [
    "# Open the two source files and the output file\n",
    "with open('/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/spt-code/sources/data/asts/ast_jdt/longmethod.txt', 'r') as source_file, \\\n",
    "     open('/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/spt-code/sources/data/asts/ast_jdt/ast_outputs_pretrain.txt', 'r') as asts_file, \\\n",
    "     open('/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/asts/ast_jdt_nongen_output_file.txt', 'w') as output_file:\n",
    "\n",
    "    # Read lines from both files\n",
    "    source_lines = source_file.readlines()\n",
    "    asts_lines = asts_file.readlines()\n",
    "\n",
    "    # Iterate over the lines from both files\n",
    "    for source_line, asts_line in zip(source_lines, asts_lines):\n",
    "        # Strip newline characters and concatenate the lines with the desired format\n",
    "        source_line_clean = source_line.replace('\"', '').strip()\n",
    "        asts_line_clean = asts_line.strip()\n",
    "\n",
    "        formatted_line = f'Source: \"{source_line_clean}\", Asts: \"{asts_line_clean}\"\\n'\n",
    "        \n",
    "        # Write the formatted line to the output file\n",
    "        output_file.write(formatted_line)\n",
    "\n",
    "print(\"The files were concatenated successfully and saved to ast_jdt_nongen_output_file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concat cleaned method and their respective ast into a file since java is not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The files were concatenated successfully and saved to finetune_methods_test.txt.\n"
     ]
    }
   ],
   "source": [
    "# Open the two source files and the output file\n",
    "with open('/home/user1-system11/Documents/data_shradha/asts/pretrain/pretrain_source.txt', 'r') as source_file, \\\n",
    "     open('/home/user1-system11/Documents/data_shradha/spt-code-new-data/pretrain_methods_asts_blockexpr.txt', 'r') as asts_file, \\\n",
    "     open('/home/user1-system11/Documents/data_shradha/asts/pretrain_ast_jdt_output', 'w') as output_file:\n",
    "\n",
    "    # Read lines from both files\n",
    "    source_lines = source_file.readlines()\n",
    "    asts_lines = asts_file.readlines()\n",
    "\n",
    "    # Iterate over the lines from both files\n",
    "    for source_line, asts_line in zip(source_lines, asts_lines):\n",
    "        # Strip newline characters and concatenate the lines with the desired format\n",
    "        source_line_clean = source_line.replace('\"', '').strip()\n",
    "        asts_line_clean = asts_line.strip()\n",
    "\n",
    "        formatted_line = f'Source: \"{source_line_clean}\", Asts: \"{asts_line_clean}\"\\n'\n",
    "        \n",
    "        # Write the formatted line to the output file\n",
    "        output_file.write(formatted_line\n",
    "                          )\n",
    "\n",
    "print(\"The files were concatenated successfully and saved to finetune_methods_test.txt.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare a finetuning dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been processed and saved to finetune_raw_valid.jsonl.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Path to your JSON file\n",
    "file_path = '/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/finetune_raw/java-small-json/java-small.val.json'\n",
    "\n",
    "# Open and read the file line by line (assuming each line is a separate JSON object)\n",
    "results = []\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            # Parse each line as a JSON object\n",
    "            json_object = json.loads(line)\n",
    "            # Format the desired fields\n",
    "            formatted_string = f\"{json_object['left_context']} {json_object['target_seq']} {json_object['right_context']}\"\n",
    "            results.append(formatted_string)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error decoding JSON from line:\", line)\n",
    "            continue\n",
    "\n",
    "# Write all formatted strings to a new file\n",
    "with open('finetune_raw_valid.txt', 'w') as output_file:\n",
    "    for item in results:\n",
    "        output_file.write(item + '\\n')\n",
    "\n",
    "print(\"Data has been processed and saved to finetune_raw_valid.txt.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total input lines read: 10000\n",
      "Total output lines prepared: 10000\n",
      "Data has been processed and saved to /home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/finetune_raw/java-small-json/finetune_methods_valid_pred_final.txt.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Regular expression pattern to match string literals\n",
    "STRING_MATCHING_PATTERN = re.compile(r'([bruf]*)(\\\"\\\"\\\"|\\'\\'\\'|\\\"|\\')(?:(?!\\2)(?:\\\\.|[^\\\\]))*\\2')\n",
    "\n",
    "def replace_string_literal(source):\n",
    "    \"\"\"Replace string literals in the source code with '___STR'.\"\"\"\n",
    "    return re.sub(pattern=STRING_MATCHING_PATTERN, repl='___STR', string=source)\n",
    "\n",
    "# Path to your JSON file\n",
    "file_path = '/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/finetune_raw/java-small-json/java-small.val.json'\n",
    "output_file_path = '/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/finetune_raw/java-small-json/finetune_methods_valid_final.txt'\n",
    "\n",
    "results = []  # Store the output lines here\n",
    "\n",
    "# Open and read the file line by line (assuming each line is a separate JSON object)\n",
    "with open(file_path, 'r') as file:\n",
    "    line_count = 0  # Counter for input lines\n",
    "    for line in file:\n",
    "        line_count += 1\n",
    "        try:\n",
    "            # Parse each line as a JSON object\n",
    "            json_object = json.loads(line)\n",
    "            # Replace string literals and concatenate the desired fields\n",
    "            left_context = replace_string_literal(json_object['left_context']).replace('\\n', '\\\\n').replace('=', '\\\\u003d').replace(\"\\\\\", \"\\\\\\\\\").replace(\"\\\"\", \"\\\\\\\"\").replace(\"\\r\", \"\\\\r\").replace(\"\\t\", \"\\\\t\")\n",
    "            right_context = replace_string_literal(json_object['right_context']).replace('\\n', '\\\\n').replace('=', '\\\\u003d').replace(\"\\\\\", \"\\\\\\\\\").replace(\"\\\"\", \"\\\\\\\"\").replace(\"\\r\", \"\\\\r\").replace(\"\\t\", \"\\\\t\")\n",
    "            target_seq = replace_string_literal(json_object['target_seq']).replace('\\n', '\\\\n').replace('=', '\\\\u003d').replace(\"\\\\\", \"\\\\\\\\\").replace(\"\\\"\", \"\\\\\\\"\").replace(\"\\r\", \"\\\\r\").replace(\"\\t\", \"\\\\t\")\n",
    "            formatted_string = f\"{left_context} {target_seq} {right_context}\"\n",
    "            # Enclose the concatenated string in quotes\n",
    "            enclosed_string = f\"\\\"{formatted_string}\\\"\"\n",
    "            results.append(enclosed_string)\n",
    "        except json.JSONDecodeError:\n",
    "            # Log the error and append a placeholder line\n",
    "            print(f\"Error decoding JSON on input line {line_count}\")\n",
    "            results.append(f\"\\\"Error in JSON format on line {line_count}\\\"\")\n",
    "\n",
    "# Print the line counts\n",
    "print(f\"Total input lines read: {line_count}\")\n",
    "print(f\"Total output lines prepared: {len(results)}\")\n",
    "\n",
    "# Write all prepared lines to the output file\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    for item in results:\n",
    "        output_file.write(item + '\\n')\n",
    "\n",
    "print(f\"Data has been processed and saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has 1309842 lines.\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the JSON file\n",
    "file_path = '/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/finetune_raw/java-small-json/java-small.train.json'\n",
    "\n",
    "# Initialize a counter for the lines\n",
    "line_count = 0\n",
    "\n",
    "# Open the file and count the lines\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        line_count += 1\n",
    "\n",
    "# Print the number of lines in the file\n",
    "print(f'The file has {line_count} lines.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if hasattr(dataset, 'asts') and hasattr(dataset, 'codes') and dataset.asts and dataset.codes:\n",
    "#     if len(dataset.asts) != len(dataset.codes):\n",
    "#         print(\"The lengths of 'asts' and 'codes' do not match.\")\n",
    "#     else:\n",
    "#         ast_code_pairs = list(zip(dataset.asts, dataset.codes))\n",
    "        \n",
    "#         unique_ast_code_pairs = set(ast_code_pairs)\n",
    "        \n",
    "#         print(\"Unique ASTs with corresponding Codes:\")\n",
    "#         for ast, code in unique_ast_code_pairs:\n",
    "#             print(\"AST:\", ast, \"Code:\", code)\n",
    "# else:\n",
    "#     print(\"Either 'asts' or 'codes' attribute is empty or does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# filename = '/home/user1-selab3/Documents/pre_train.pk'\n",
    "# # filename = '/home/user1-selab3/Documents/research-shradha/CODE-SPT-Code/dataset/dataset_saved/pre_train.pk'\n",
    "\n",
    "# with open(filename, 'rb') as file:\n",
    "#     dataset = pickle.load(file)\n",
    "#     print(len(dataset))\n",
    "\n",
    "# if hasattr(dataset, 'asts') and isinstance(dataset.asts, list) and len(dataset.asts) >= 226483:\n",
    "#     sliced_data = dataset.asts[45423:226484]  \n",
    "\n",
    "#     print(\"Length of sliced data:\", len(sliced_data))\n",
    "#     if sliced_data:\n",
    "#         print(\"Sample of 'asts' from sliced data:\", sliced_data[0])\n",
    "#     else:\n",
    "#         print(\"Sliced data is empty.\")\n",
    "# else:\n",
    "#     print(\"The 'asts' attribute does not exist or does not contain enough data.\")\n",
    "\n",
    "# if hasattr(dataset, 'asts') and dataset.asts:\n",
    "#     extracted_data = ' '.join(str(ast) for ast in dataset.asts)\n",
    "#     components = extracted_data.split() \n",
    "#     unique_components = list(set(components))  \n",
    "#     unique_components.sort()  \n",
    "\n",
    "#     print(\"Unique components in 'asts':\")\n",
    "#     for component in unique_components:\n",
    "#         print(component)\n",
    "# else:\n",
    "#     print(\"The 'asts' attribute is empty or does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm4code-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
