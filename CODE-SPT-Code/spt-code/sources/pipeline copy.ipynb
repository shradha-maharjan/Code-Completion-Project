{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation below loads a pre-trained BART model and custom vocabularies for code, nl, and ast. However, only the vocab for code is used. The configuration and model are loaded from the model_dir. \n",
    "First, input text is encoded using the custom vocabulary and then it uses the configured BART model to generate candidate sequences with beam search. And finally, the output sequences is decoded along with their probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code 1 tokenized input: ([151, 205, 3098, 9, 10, 64, 196, 15, 943, 389, 15, 333, 3813, 9, 10, 15, 389, 9, 10, 28, 34, 383, 48, 35, 15, 389, 9, 10, 28, 66], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Candidate 1: this . lock (Probability: 28.29%)\n",
      "Candidate 2: this . writelock (Probability: 19.12%)\n",
      "Candidate 3: this . readwritelock (Probability: 19.01%)\n",
      "Candidate 4: this . lock lock (Probability: 18.09%)\n",
      "Candidate 5: this . fs lock (Probability: 15.49%)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartConfig\n",
    "from data.vocab import load_vocab\n",
    "\n",
    "def load_model_and_vocab(model_dir, vocab_dir):\n",
    "    config_path = os.path.join(model_dir, 'config.json')\n",
    "    model_path = os.path.join(model_dir, 'model.safetensors')\n",
    "    config = BartConfig.from_json_file(config_path)\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_path, config=config)\n",
    "    model.eval()  \n",
    "\n",
    "    code_vocab = load_vocab(vocab_dir, \"code\")\n",
    "    nl_vocab = load_vocab(vocab_dir, \"nl\")\n",
    "    ast_vocab = load_vocab(vocab_dir, \"ast\")\n",
    "    \n",
    "    return model, code_vocab#, nl_vocab, ast_vocab\n",
    "\n",
    "def generate_candidates(model, input_text, vocab, num_beams=5, max_length=50):\n",
    "    input_ids, attention_mask = vocab.encode_sequence(input_text)\n",
    "    input_ids = torch.tensor([input_ids])  \n",
    "    attention_mask = torch.tensor([attention_mask])\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        num_return_sequences=num_beams,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "    \n",
    "    candidates = vocab.decode_batch(outputs.sequences.tolist())\n",
    "    probabilities = torch.softmax(outputs.sequences_scores, dim=0).tolist()\n",
    "    \n",
    "    return candidates, probabilities\n",
    "\n",
    "def main():\n",
    "    model_dir = \"/home/user1-system11/Documents/research-shradha/deploy-spt-code/spt-code/outputs/pre_train_20241125_001741/models\"  \n",
    "    vocab_dir = \"/home/user1-system11/Documents/research-shradha/deploy-spt-code/spt-code/outputs/pre_train_20241125_001741/vocabs\"  \n",
    "    \n",
    "    incomplete_code = \"public void writeLock() {    this.fsLock.longReadLock().lock();     [MSK] .lock();}\" \n",
    "    \n",
    "    model, code_vocab = load_model_and_vocab(model_dir, vocab_dir)\n",
    "    print(\"Code 1 tokenized input:\", code_vocab.encode_sequence(incomplete_code))\n",
    "    \n",
    "    candidates, probabilities = generate_candidates(model, incomplete_code, code_vocab)\n",
    "    \n",
    "    for idx, (candidate, prob) in enumerate(zip(candidates, probabilities)):\n",
    "        print(f\"Candidate {idx + 1}: {candidate} (Probability: {prob:.2%})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Login Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/user1-system11/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1-system11/miniconda3/envs/llm4code-project/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\")  # Replace with your Hugging Face access token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the tokenizer from a JSON file located in saved vocabs directory (use code_tokenizer.json) and save the tokenizer and its metadata to a specific directory and then upload the tokenizer to the Hugging Face Hub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/shradha01/llm-coding-tasks-tokenizer/commit/d3934091da68e08bea0ab0c238a5f9784bd88cf1', commit_message='Upload tokenizer', commit_description='', oid='d3934091da68e08bea0ab0c238a5f9784bd88cf1', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"/home/user1-system11/Documents/research-shradha/deploy-spt-code/spt-code/outputs/pre_train_20241125_001741/vocabs/code/code_tokenizer.json\"\n",
    ")\n",
    "\n",
    "# Save the tokenizer and all relevant files to a directory\n",
    "tokenizer.save_pretrained(\"/home/user1-system11/Documents/research-shradha/deploy-spt-code/spt-code/outputs/pre_train_20241125_001741/vocabs/code\")\n",
    "\n",
    "# Push the tokenizer to the Hugging Face Hub\n",
    "tokenizer.push_to_hub(\"llm-coding-tasks-tokenizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load a model saved in a specific directory and push it to the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n",
      "model.safetensors: 100%|██████████| 1.05G/1.05G [00:20<00:00, 50.7MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/shradha01/llm-coding-tasks-model/commit/41654e68ebea92f0637f9323d501e37d3b4a01b7', commit_message='Upload model', commit_description='', oid='41654e68ebea92f0637f9323d501e37d3b4a01b7', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model_dir = \"/home/user1-system11/Documents/research-shradha/deploy-spt-code/spt-code/outputs/pre_train_20241125_001741/models\"  \n",
    "repo_name = \"llm-coding-tasks-model\"  \n",
    "\n",
    "model = AutoModel.from_pretrained(model_dir)\n",
    "\n",
    "model.push_to_hub(repo_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n",
      "Model and tokenizer loaded successfully!\n",
      "Initializing pipeline...\n",
      "\n",
      "Input code: public void writeLock() { this.fsLock.longReadLock().lock(); [MSK] .lock(); }\n",
      "\n",
      "Tokenization debug:\n",
      "Input IDs: tensor([[ 151,  205, 3098,    9,   10,   64,  196,   15,  943,  389,   15,  333,\n",
      "         3813,    9,   10,   15,  389,    9,   10,   28,    4,   15,  389,    9,\n",
      "           10,   28,   66]])\n",
      "Decoded Input: public void writelock ( ) { this. fs lock. long readlock ( ). lock ( ) ; [MSK]. lock ( ) ; }\n",
      "\n",
      "Generating predictions with probabilities...\n",
      "Candidate 1: this. lock (Probability: 0.5484)\n",
      "Candidate 2: this. writelock (Probability: 0.3635)\n",
      "Candidate 3: this. locks (Probability: 0.2878)\n",
      "Candidate 4: this. constants (Probability: 0.2645)\n",
      "Candidate 5: timeunit. milliseconds (Probability: 0.2481)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch\n",
    "\n",
    "def main():\n",
    "    model_name = \"shradha01/llm-coding-tasks-model\"  \n",
    "    tokenizer_name = \"shradha01/llm-coding-tasks-tokenizer\"  \n",
    "\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    try:\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "        print(\"Model and tokenizer loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model or tokenizer: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"Initializing pipeline...\")\n",
    "    try:\n",
    "        code_completion_pipeline = pipeline(\n",
    "            \"text2text-generation\", model=model, tokenizer=tokenizer\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing pipeline: {e}\")\n",
    "        return\n",
    "\n",
    "    incomplete_code = \"public void writeLock() { this.fsLock.longReadLock().lock(); [MSK] .lock(); }\"\n",
    "    print(\"\\nInput code:\", incomplete_code)\n",
    "\n",
    "    print(\"\\nTokenization debug:\")\n",
    "    encoded = tokenizer(incomplete_code, return_tensors=\"pt\")\n",
    "    print(\"Input IDs:\", encoded[\"input_ids\"])\n",
    "    print(\"Decoded Input:\", tokenizer.decode(encoded[\"input_ids\"][0]))\n",
    "\n",
    "    print(\"\\nGenerating predictions with probabilities...\")\n",
    "    try:\n",
    "        # Generate outputs with model directly to access logits\n",
    "        outputs = model.generate(\n",
    "            encoded[\"input_ids\"],\n",
    "            max_length=50,\n",
    "            num_beams=5,\n",
    "            num_return_sequences=5,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True\n",
    "        )\n",
    "\n",
    "        # Decode generated sequences\n",
    "        decoded_candidates = [\n",
    "            tokenizer.decode(output, skip_special_tokens=True)\n",
    "            for output in outputs.sequences\n",
    "        ]\n",
    "\n",
    "        # Calculate probabilities from logits\n",
    "        probs = []\n",
    "        for sequence_idx, score in enumerate(outputs.sequences_scores):\n",
    "            prob = torch.exp(score).item()  # Convert log score to probability\n",
    "            probs.append(prob)\n",
    "\n",
    "        # Display candidates and their probabilities\n",
    "        for idx, (candidate, prob) in enumerate(zip(decoded_candidates, probs)):\n",
    "            print(f\"Candidate {idx + 1}: {candidate.strip()} (Probability: {prob:.4f})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "#output: this.fsLock.writeLock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n",
      "Model and tokenizer loaded successfully!\n",
      "Initializing pipeline...\n",
      "\n",
      "Input code: @ override public int hashCode ( ) { final long v = currentValue . get ( ) ; return ( int ) v ^ ( int ) ( [MSK] ) ; }\n",
      "\n",
      "Tokenization debug:\n",
      "Input IDs: tensor([[  33,  476,  151,  125, 2202,    9,   10,   64,  161,  333,   59,   30,\n",
      "         5257,   15,   87,    9,   10,   28,  120,    9,  125,   10,   59,   36,\n",
      "            9,  125,   10,    9,    4,   10,   28,   66]])\n",
      "Decoded Input: @ override public int hashcode ( ) { final long v = currentvalue. get ( ) ; return ( int ) v ^ ( int ) ( [MSK] ) ; }\n",
      "\n",
      "Generating predictions with probabilities...\n",
      "Candidate 1: super. hashcode ( ) (Probability: 0.6413)\n",
      "Candidate 2: long. min_value (Probability: 0.5478)\n",
      "Candidate 3: v > > > 32 (Probability: 0.3460)\n",
      "Candidate 4: long. height (Probability: 0.2960)\n",
      "Candidate 5: long. max_value (Probability: 0.2699)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch\n",
    "\n",
    "def main():\n",
    "    model_name = \"shradha01/llm-coding-tasks-model\"  \n",
    "    tokenizer_name = \"shradha01/llm-coding-tasks-tokenizer\"  \n",
    "\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    try:\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "        print(\"Model and tokenizer loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model or tokenizer: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"Initializing pipeline...\")\n",
    "    try:\n",
    "        code_completion_pipeline = pipeline(\n",
    "            \"text2text-generation\", model=model, tokenizer=tokenizer\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing pipeline: {e}\")\n",
    "        return\n",
    "\n",
    "    \n",
    "    incomplete_code = \"@ override public int hashCode ( ) { final long v = currentValue . get ( ) ; return ( int ) v ^ ( int ) ( [MSK] ) ; }\"\n",
    "    print(\"\\nInput code:\", incomplete_code)\n",
    "\n",
    "    print(\"\\nTokenization debug:\")\n",
    "    encoded = tokenizer(incomplete_code, return_tensors=\"pt\")\n",
    "    print(\"Input IDs:\", encoded[\"input_ids\"])\n",
    "    print(\"Decoded Input:\", tokenizer.decode(encoded[\"input_ids\"][0]))\n",
    "\n",
    "    print(\"\\nGenerating predictions with probabilities...\")\n",
    "    try:\n",
    "        # Generate outputs with model directly to access logits\n",
    "        outputs = model.generate(\n",
    "            encoded[\"input_ids\"],\n",
    "            max_length=50,\n",
    "            num_beams=5,\n",
    "            num_return_sequences=5,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True\n",
    "        )\n",
    "\n",
    "        # Decode generated sequences\n",
    "        decoded_candidates = [\n",
    "            tokenizer.decode(output, skip_special_tokens=True)\n",
    "            for output in outputs.sequences\n",
    "        ]\n",
    "\n",
    "        # Calculate probabilities from logits\n",
    "        probs = []\n",
    "        for sequence_idx, score in enumerate(outputs.sequences_scores):\n",
    "            prob = torch.exp(score).item()  # Convert log score to probability\n",
    "            probs.append(prob)\n",
    "\n",
    "        # Display candidates and their probabilities\n",
    "        for idx, (candidate, prob) in enumerate(zip(decoded_candidates, probs)):\n",
    "            print(f\"Candidate {idx + 1}: {candidate.strip()} (Probability: {prob:.4f})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm4code-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
