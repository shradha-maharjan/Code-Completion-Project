{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['training_args.bin', 'model.safetensors', 'cap', 'generation_config.json', 'config.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_path = \"/home/user1-system11/Documents/research-shradha/CODE-SPT-Code/spt-code/outputs/pre_train_20241112_201020/models/\"\n",
    "print(os.listdir(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and vocabularies loaded successfully.\n",
      "Code vocab size: 919\n",
      "AST vocab size: 34\n",
      "NL vocab size: 1878\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import BartConfig\n",
    "from models.bart import BartForClassificationAndGeneration\n",
    "from data.vocab import load_vocab\n",
    "\n",
    "# Paths to the model and vocab directories\n",
    "model_dir = \"/home/user1-system11/Documents/research-shradha/CODE-SPT-Code/spt-code/outputs/pre_train_20241115_232553/models\"\n",
    "vocab_dir = \"/home/user1-system11/Documents/research-shradha/CODE-SPT-Code/spt-code/outputs/pre_train_20241115_232553/vocabs\"\n",
    "\n",
    "# Load model configuration and weights\n",
    "config_path = os.path.join(model_dir, \"config.json\")\n",
    "model_weights_path = os.path.join(model_dir, \"model.safetensors\")\n",
    "\n",
    "config = BartConfig.from_json_file(config_path)\n",
    "model = BartForClassificationAndGeneration.from_pretrained(model_weights_path, config=config)\n",
    "\n",
    "# Load vocabularies\n",
    "code_vocab = load_vocab(vocab_root=vocab_dir, name=\"code\")\n",
    "ast_vocab = load_vocab(vocab_root=vocab_dir, name=\"ast\")\n",
    "nl_vocab = load_vocab(vocab_root=vocab_dir, name=\"nl\")\n",
    "\n",
    "print(f\"Model and vocabularies loaded successfully.\")\n",
    "print(f\"Code vocab size: {len(code_vocab)}\")\n",
    "print(f\"AST vocab size: {len(ast_vocab)}\")\n",
    "print(f\"NL vocab size: {len(nl_vocab)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EOS_TOKEN', 'MSK_TOKEN', 'PAD_TOKEN', 'SEP_TOKEN', 'SOS_TOKEN', 'START_VOCAB', 'UNK_TOKEN', '_Vocab__special_symbols', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'add_special_symbols', 'decode', 'decode_batch', 'encode_batch', 'encode_sequence', 'eos_processor', 'get_eos_index', 'get_index', 'get_mask_index', 'get_pad_index', 'get_sos_index', 'get_token', 'get_unk_index', 'ignore_case', 'index_offset', 'method', 'name', 'num_special_token', 'pad_token_id', 'restore_index', 'save', 'save_pickle', 'save_pretrained', 'sep_processor', 'sos_processor', 'tokenizer', 'transfer_index']\n"
     ]
    }
   ],
   "source": [
    "print(dir(code_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code 1 tokenized input: ([148, 209, 2909, 13, 14, 69, 197, 19, 923, 383, 19, 335, 3651, 13, 14, 19, 383, 13, 14, 32, 38, 46659, 40, 19, 383, 13, 14, 32, 71], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Candidate 1: this . readwritelock . readlock ( ) (Probability: 30.31%)\n",
      "Candidate 2: this . readwritelock . writelock ( ) (Probability: 21.15%)\n",
      "Candidate 3: this . parents (Probability: 16.34%)\n",
      "Candidate 4: this . readwritelock . getapplication lock ( ) (Probability: 16.31%)\n",
      "Candidate 5: this . delegate (Probability: 15.90%)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartConfig\n",
    "from data.vocab import load_vocab\n",
    "\n",
    "def load_model_and_vocab(model_dir, vocab_dir):\n",
    "    config_path = os.path.join(model_dir, 'config.json')\n",
    "    model_path = os.path.join(model_dir, 'model.safetensors')\n",
    "    config = BartConfig.from_json_file(config_path)\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_path, config=config)\n",
    "    model.eval()  \n",
    "\n",
    "    code_vocab = load_vocab(vocab_dir, \"code\")\n",
    "    nl_vocab = load_vocab(vocab_dir, \"nl\")\n",
    "    ast_vocab = load_vocab(vocab_dir, \"ast\")\n",
    "    \n",
    "    return model, code_vocab#, nl_vocab, ast_vocab\n",
    "\n",
    "def generate_candidates(model, input_text, vocab, num_beams=5, max_length=50):\n",
    "    input_ids, attention_mask = vocab.encode_sequence(input_text)\n",
    "    input_ids = torch.tensor([input_ids])  \n",
    "    attention_mask = torch.tensor([attention_mask])\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        num_return_sequences=num_beams,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "    \n",
    "    candidates = vocab.decode_batch(outputs.sequences.tolist())\n",
    "    probabilities = torch.softmax(outputs.sequences_scores, dim=0).tolist()\n",
    "    \n",
    "    return candidates, probabilities\n",
    "\n",
    "def main():\n",
    "    model_dir = \"/home/user1-system11/Documents/research-shradha/CODE-SPT-Code-TreeSitterV3/SPT-Code/outputs/pre_train_20241105_105459/models\"  \n",
    "    vocab_dir = \"/home/user1-system11/Documents/research-shradha/CODE-SPT-Code-TreeSitterV3/SPT-Code/outputs/pre_train_20241105_105459/vocabs\"  \n",
    "    \n",
    "    incomplete_code = \"public void writeLock() {    this.fsLock.longReadLock().lock();     [MSK] .lock();}\" \n",
    "    \n",
    "    model, code_vocab = load_model_and_vocab(model_dir, vocab_dir)\n",
    "    print(\"Code 1 tokenized input:\", code_vocab.encode_sequence(incomplete_code))\n",
    "    \n",
    "    candidates, probabilities = generate_candidates(model, incomplete_code, code_vocab)\n",
    "    \n",
    "    for idx, (candidate, prob) in enumerate(zip(candidates, probabilities)):\n",
    "        print(f\"Candidate {idx + 1}: {candidate} (Probability: {prob:.2%})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/user1-system11/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1-system11/miniconda3/envs/llm4code-project/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\")  # Replace with your Hugging Face access token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/shradha01/code_completion_tokenizer', endpoint='https://huggingface.co', repo_type='model', repo_id='shradha01/code_completion_tokenizer')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import create_repo\n",
    "\n",
    "repo_name = \"code_completion_tokenizer\"\n",
    "create_repo(repo_id=repo_name, token=\"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\", private=False)  # Adjust `private` as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "# Load the individual tokenizers\n",
    "ast_tokenizer = Tokenizer.from_file(\"/home/user1-system11/Documents/research-shradha/CODE-SPT-Code-TreeSitterV3/SPT-Code/outputs/pre_train_20241116_103947/multi-tokenizer/ast_tokenizer.json\")\n",
    "code_tokenizer = Tokenizer.from_file(\"/home/user1-system11/Documents/research-shradha/CODE-SPT-Code-TreeSitterV3/SPT-Code/outputs/pre_train_20241116_103947/multi-tokenizer/code_tokenizer.json\")\n",
    "nl_tokenizer = Tokenizer.from_file(\"/home/user1-system11/Documents/research-shradha/CODE-SPT-Code-TreeSitterV3/SPT-Code/outputs/pre_train_20241116_103947/multi-tokenizer/nl_tokenizer.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ast_vocab = ast_tokenizer.get_vocab()\n",
    "code_vocab = code_tokenizer.get_vocab()\n",
    "nl_vocab = nl_tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_vocab = {**ast_vocab, **code_vocab, **nl_vocab}  # Combines dictionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_vocab = {token: idx for idx, (token, _) in enumerate(merged_vocab.items())}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.models import WordLevel\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "unified_tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "\n",
    "special_tokens = [\"[PAD]\", \"[SOS]\", \"[EOS]\", \"[SEP]\", \"[MSK]\"]\n",
    "unified_tokenizer.model = WordLevel(unk_token=\"[UNK]\")\n",
    "\n",
    "unified_tokenizer.model = WordLevel(unk_token=\"[UNK]\", vocab=merged_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for token in special_tokens:\n",
    "    if token not in merged_vocab:\n",
    "        merged_vocab[token] = len(merged_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [52]\n",
      "Decoded: [UNK]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "unified_tokenizer.enable_padding(pad_id=merged_vocab[\"[PAD]\"], pad_token=\"[PAD]\")\n",
    "\n",
    "encoded = unified_tokenizer.encode(\"This is a test sentence.\")\n",
    "print(\"Encoded:\", encoded.ids)\n",
    "\n",
    "decoded = unified_tokenizer.decode(encoded.ids)\n",
    "print(\"Decoded:\", decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_tokenizer.save(\"/home/user1-system11/Documents/research-shradha/CODE-SPT-Code-TreeSitterV3/SPT-Code/outputs/pre_train_20241116_103947/multi-tokenizer/unified_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1-system11/miniconda3/envs/llm4code-project/lib/python3.11/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/shradha01/multi-purpose-tokenizer/commit/c7fa6ea69d507c600b1d6a352b86c15c35105ca4', commit_message='Upload tokenizer', commit_description='', oid='c7fa6ea69d507c600b1d6a352b86c15c35105ca4', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"/home/user1-system11/Documents/research-shradha/CODE-SPT-Code-TreeSitterV3/SPT-Code/outputs/pre_train_20241116_103947/multi-tokenizer/unified_tokenizer.json\")\n",
    "tokenizer.push_to_hub(\"multi-purpose-tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"/home/user1-system11/Documents/research-shradha/CODE-SPT-Code-TreeSitterV3/SPT-Code/outputs/pre_train_20241105_105459/vocabs/code/code-tokenizer.json\")\n",
    "tokenizer.push_to_hub(\"code-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/shradha01/code-tokenizer-01/commit/761df7e731c83678eced702a1958faa589831af5', commit_message='Upload tokenizer', commit_description='', oid='761df7e731c83678eced702a1958faa589831af5', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"/home/user1-system11/Documents/research-shradha/CODE-SPT-Code-TreeSitterV3/SPT-Code/outputs/pre_train_20241105_105459/vocabs/code/code_tokenizer.json\"\n",
    ")\n",
    "\n",
    "# Save the tokenizer and all relevant files to a directory\n",
    "tokenizer.save_pretrained(\"/home/user1-system11/Documents/research-shradha/CODE-SPT-Code-TreeSitterV3/SPT-Code/outputs/pre_train_20241105_105459/vocabs/code\")\n",
    "\n",
    "# Push the tokenizer to the Hugging Face Hub\n",
    "tokenizer.push_to_hub(\"code-tokenizer-01\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n",
      "model.safetensors: 100%|██████████| 1.05G/1.05G [00:23<00:00, 44.5MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/shradha01/Code-completion-01/commit/c2609253f8a45a249c48993be8d29ae6ed3b17ad', commit_message='Upload model', commit_description='', oid='c2609253f8a45a249c48993be8d29ae6ed3b17ad', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model_dir = \"/home/user1-system11/Documents/research-shradha/CODE-SPT-Code-TreeSitterV3/SPT-Code/outputs/pre_train_20241105_105459/models\"  \n",
    "repo_name = \"Code-completion-01\"  \n",
    "\n",
    "model = AutoModel.from_pretrained(model_dir)\n",
    "\n",
    "model.push_to_hub(repo_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Load the model and tokenizer from the Hugging Face Hub\n",
    "model_name = \"shradha01/Code-completion-01\"\n",
    "tokenizer_name = \"shradha01/code-tokenizer-01\"#\"shradha01/multi-purpose-tokenizer\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "code_completion_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete_code = \"public void writeLock() {    this.fsLock.longReadLock().lock();     [MSK] .lock();}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: tensor([[ 148,  209, 2909,   13,   14,   69,  197,   19,  923,  383,   19,  335,\n",
      "         3651,   13,   14,   19,  383,   13,   14,   32,    4,   19,  383,   13,\n",
      "           14,   32,   71]])\n",
      "Tokens: ['public', 'void', 'writelock', '(', ')', '{', 'this', '.', 'fs', 'lock', '.', 'long', 'readlock', '(', ')', '.', 'lock', '(', ')', ';', '[MSK]', '.', 'lock', '(', ')', ';', '}']\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer(incomplete_code, return_tensors=\"pt\")\n",
    "print(\"Token IDs:\", encoded.input_ids)\n",
    "print(\"Tokens:\", [tokenizer.decode([id]) for id in encoded.input_ids[0]])\n",
    "\n",
    "if any(id is None for id in encoded.input_ids[0]):\n",
    "    print(\"Found out-of-vocabulary tokens!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if \"[MSK]\" not in tokenizer.get_vocab():\n",
    "#     print(\"`[MSK]` token is missing from the vocabulary!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if \"[MSK]\" not in tokenizer.get_vocab():\n",
    "#     tokenizer.add_tokens([\"[MSK]\"])\n",
    "#     model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_input = tokenizer(incomplete_code, return_tensors=\"pt\")\n",
    "# print(\"Input IDs:\", encoded_input.input_ids)\n",
    "# print(\"Tokens:\", [tokenizer.decode([token_id]) for token_id in encoded_input.input_ids[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocabulary size: 50000\n",
      "Model vocabulary size: 80092\n",
      "Resized model embeddings to match tokenizer vocabulary size.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer_vocab_size = len(tokenizer)\n",
    "print(\"Tokenizer vocabulary size:\", tokenizer_vocab_size)\n",
    "\n",
    "model_vocab_size = model.config.vocab_size\n",
    "print(\"Model vocabulary size:\", model_vocab_size)\n",
    "\n",
    "if tokenizer_vocab_size != model_vocab_size:\n",
    "    model.resize_token_embeddings(tokenizer_vocab_size)\n",
    "    print(\"Resized model embeddings to match tokenizer vocabulary size.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 148,  209, 2909,   13,   14,   69,  197,   19,  923,  383,   19,  335,\n",
      "         3651,   13,   14,   19,  383,   13,   14,   32,    4,   19,  383,   13,\n",
      "           14,   32,   71]])\n",
      "Decoded Tokens: public void writelock ( ) { this. fs lock. long readlock ( ). lock ( ) ; [MSK]. lock ( ) ; }\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoded_input = tokenizer(incomplete_code, return_tensors=\"pt\")\n",
    "print(\"Input IDs:\", encoded_input.input_ids)\n",
    "print(\"Decoded Tokens:\", tokenizer.decode(encoded_input.input_ids[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate 1: new public lock ( )\n",
      "Candidate 2: new public lock ( 10 )\n",
      "Candidate 3: new public lock ( lock )\n",
      "Candidate 4: new public lock ( 10 , this .\n",
      "Candidate 5: new public lock ( 10 , timeunit .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "candidates = code_completion_pipeline(\n",
    "    incomplete_code,\n",
    "    max_length=10,\n",
    "    num_return_sequences=5,\n",
    "    num_beams=5\n",
    ")\n",
    "\n",
    "for idx, candidate in enumerate(candidates):\n",
    "    print(f\"Candidate {idx + 1}: {candidate['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n",
      "Initializing pipeline...\n",
      "\n",
      "Input code: public void writeLock() { this.fsLock.longReadLock().lock(); [MSK] .lock(); }\n",
      "\n",
      "Tokenization debug:\n",
      "Input IDs: tensor([[ 148,  209, 2909,   13,   14,   69,  197,   19,  923,  383,   19,  335,\n",
      "         3651,   13,   14,   19,  383,   13,   14,   32,    4,   19,  383,   13,\n",
      "           14,   32,   71]])\n",
      "Decoded Input: public void writelock ( ) { this. fs lock. long readlock ( ). lock ( ) ; [MSK]. lock ( ) ; }\n",
      "\n",
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1-system11/miniconda3/envs/llm4code-project/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/user1-system11/miniconda3/envs/llm4code-project/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate 1: new public lock ( )\n",
      "Candidate 2: new public lock ( 10 )\n",
      "Candidate 3: new public lock ( 10 , this . lock )\n",
      "Candidate 4: new public lock ( 10 , timeunit . milliseconds )\n",
      "Candidate 5: new public lock ( lock )\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "def main():\n",
    "    model_name = \"shradha01/Code-completion-01\"  \n",
    "    tokenizer = \"shradha01/code-tokenizer-01\"  \n",
    "\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    try:\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
    "\n",
    "        special_tokens = {\"additional_special_tokens\": [\"[MSK]\"]}\n",
    "        tokenizer.add_special_tokens(special_tokens)\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        print(\"Model and tokenizer loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model or tokenizer: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"Initializing pipeline...\")\n",
    "    try:\n",
    "        code_completion_pipeline = pipeline(\n",
    "            \"text2text-generation\", model=model, tokenizer=tokenizer\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing pipeline: {e}\")\n",
    "        return\n",
    "\n",
    "    incomplete_code = \"public void writeLock() { this.fsLock.longReadLock().lock(); [MSK] .lock(); }\"\n",
    "    print(\"\\nInput code:\", incomplete_code)\n",
    "\n",
    "    print(\"\\nTokenization debug:\")\n",
    "    encoded = tokenizer(incomplete_code, return_tensors=\"pt\")\n",
    "    print(\"Input IDs:\", encoded[\"input_ids\"])\n",
    "    print(\"Decoded Input:\", tokenizer.decode(encoded[\"input_ids\"][0]))\n",
    "\n",
    "    if tokenizer.convert_tokens_to_ids(\"[MSK]\") == tokenizer.unk_token_id:\n",
    "        print(\"Warning: `[MSK]` token is not recognized. Adding it to the tokenizer.\")\n",
    "        tokenizer.add_special_tokens({\"additional_special_tokens\": [\"[MSK]\"]})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    print(\"\\nGenerating predictions...\")\n",
    "    try:\n",
    "        candidates = code_completion_pipeline(\n",
    "            incomplete_code,\n",
    "            max_length=50,  \n",
    "            num_beams=5,    \n",
    "            num_return_sequences=5, \n",
    "            temperature=0.7,  \n",
    "            top_k=50,         \n",
    "            top_p=0.95        \n",
    "        )\n",
    "\n",
    "        for idx, candidate in enumerate(candidates):\n",
    "            print(f\"Candidate {idx + 1}: {candidate['generated_text']}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding token not found. Adding [PAD] as pad_token.\n",
      "Model and tokenizer loaded successfully!\n",
      "\n",
      "Input code: public Token<? extends TokenIdentifier> getToken(Text alias) {    return  [MSK] ;}\n",
      "\n",
      "Tokenized Input IDs: [[148, 465, 33, 36, 872, 465, 1451, 35, 3970, 13, 497, 1494, 14, 69, 121, 4, 32, 71]]\n",
      "Attention Mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "Special Tokens: [AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), AddedToken(\"[MSK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True)]\n",
      "\n",
      "Generating predictions with probabilities...\n",
      "Candidate 1: public token identifier. gettoken ( alias ) (Probability: 0.2223)\n",
      "Candidate 2: public token identifier. parse ( text alias ) (Probability: 0.2213)\n",
      "Candidate 3: public token identifier. to identifier ( alias ) (Probability: 0.2059)\n",
      "Candidate 4: public token identifier. gettoken ( text alias ) (Probability: 0.1831)\n",
      "Candidate 5: public token identifier. to identifier ( alias. gettext ( ) ) (Probability: 0.1674)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Define repository and subfolder\n",
    "    model_name = \"shradha01/Code-completion-01\"  # Main model repository\n",
    "    tokenizer_name = \"shradha01/code-tokenizer-01\"  # Subfolder for tokenizer\n",
    "\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    try:\n",
    "        # Load model\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "        special_tokens = {\"additional_special_tokens\": [\"[MSK]\"]}\n",
    "        tokenizer.special_tokens_map[\"[MSK]\"] = 46659\n",
    "        if tokenizer.pad_token is None:\n",
    "            print(\"Padding token not found. Adding [PAD] as pad_token.\")\n",
    "            special_tokens[\"pad_token\"] = \"[PAD]\"\n",
    "\n",
    "        tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "        # Resize the model embeddings to accommodate new tokens\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        print(\"Model and tokenizer loaded successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model or tokenizer: {e}\")\n",
    "        return\n",
    "\n",
    "    # Example input\n",
    "    incomplete_code = \"public Token<? extends TokenIdentifier> getToken(Text alias) {    return  [MSK] ;}\"\n",
    "    print(\"\\nInput code:\", incomplete_code)\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "            incomplete_code,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=model.config.max_position_embeddings,\n",
    "        )\n",
    "    incomplete_code = incomplete_code.replace(\"[MSK]\", \"38 [MSK] 40\")\n",
    "\n",
    "    print(\"\\nTokenized Input IDs:\", inputs[\"input_ids\"].tolist())\n",
    "    print(\"Attention Mask:\", inputs[\"attention_mask\"].tolist())\n",
    "    print(\"Special Tokens:\", tokenizer.all_special_tokens_extended)\n",
    "\n",
    "\n",
    "    # Generate predictions with beam search\n",
    "    print(\"\\nGenerating predictions with probabilities...\")\n",
    "    try:\n",
    "        # Perform beam search to get multiple candidates\n",
    "        output_sequences = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=50,  # Maximum length of the generated sequence\n",
    "            num_beams=5,    # Beam search for diverse outputs\n",
    "            num_return_sequences=5,  # Number of candidates\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )\n",
    "\n",
    "        # Decode the candidates\n",
    "        candidates = [\n",
    "            tokenizer.decode(seq, skip_special_tokens=True)\n",
    "            for seq in output_sequences[\"sequences\"]\n",
    "        ]\n",
    "\n",
    "        # Calculate probabilities from scores\n",
    "        scores = output_sequences[\"sequences_scores\"]  # Log-probabilities\n",
    "        probabilities = F.softmax(scores, dim=0)  # Convert to probabilities\n",
    "\n",
    "        # Display results with probabilities\n",
    "        for idx, (candidate, prob) in enumerate(zip(candidates, probabilities)):\n",
    "            print(f\"Candidate {idx + 1}: {candidate} (Probability: {prob:.4f})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "# def main():\n",
    "#     # Define repository and subfolder\n",
    "#     model_name = \"claudios/sptcode\"  # Main model repository\n",
    "#     tokenizer_subfolder = \"code_tokenizer_fast\"  # Subfolder for tokenizer\n",
    "\n",
    "#     print(\"Loading model and tokenizer...\")\n",
    "#     try:\n",
    "#         # Load model\n",
    "#         model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "#         # Load tokenizer from subfolder\n",
    "#         tokenizer = AutoTokenizer.from_pretrained(model_name, subfolder=tokenizer_subfolder)\n",
    "\n",
    "#         # Add special tokens if needed\n",
    "#         special_tokens = {\"additional_special_tokens\": [\"[MSK]\"]}\n",
    "#         tokenizer.add_special_tokens(special_tokens)\n",
    "#         model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#         print(\"Model and tokenizer loaded successfully!\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading model or tokenizer: {e}\")\n",
    "#         return\n",
    "\n",
    "#     # Initialize pipeline\n",
    "#     print(\"Initializing pipeline...\")\n",
    "#     try:\n",
    "#         code_completion_pipeline = pipeline(\n",
    "#             \"text2text-generation\", model=model, tokenizer=tokenizer\n",
    "#         )\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error initializing pipeline: {e}\")\n",
    "#         return\n",
    "\n",
    "#     # Example input\n",
    "#     incomplete_code = \"public void writeLock() { this.fsLock.longReadLock().lock(); [MSK] .lock(); }\"\n",
    "#     print(\"\\nInput code:\", incomplete_code)\n",
    "\n",
    "#     # Debug tokenization\n",
    "#     print(\"\\nTokenization debug:\")\n",
    "#     encoded = tokenizer(incomplete_code, return_tensors=\"pt\")\n",
    "#     print(\"Input IDs:\", encoded[\"input_ids\"])\n",
    "#     print(\"Decoded Input:\", tokenizer.decode(encoded[\"input_ids\"][0]))\n",
    "\n",
    "#     # Check if `[MSK]` is recognized\n",
    "#     if tokenizer.convert_tokens_to_ids(\"[MSK]\") == tokenizer.unk_token_id:\n",
    "#         print(\"Warning: `[MSK]` token is not recognized. Adding it to the tokenizer.\")\n",
    "#         tokenizer.add_special_tokens({\"additional_special_tokens\": [\"[MSK]\"]})\n",
    "#         model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#     # Generate predictions\n",
    "#     print(\"\\nGenerating predictions...\")\n",
    "#     try:\n",
    "#         candidates = code_completion_pipeline(\n",
    "#             incomplete_code,\n",
    "#             max_length=50,  # Maximum length of the generated sequence\n",
    "#             num_beams=5,    # Beam search for diverse outputs\n",
    "#             num_return_sequences=5,  # Number of candidates\n",
    "#             temperature=0.7,  # Adjust randomness\n",
    "#             top_k=50,         # Limit the sampling pool\n",
    "#             top_p=0.95        # Nucleus sampling\n",
    "#         )\n",
    "\n",
    "#         # Display results\n",
    "#         for idx, candidate in enumerate(candidates):\n",
    "#             print(f\"Candidate {idx + 1}: {candidate['generated_text']}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error during prediction: {e}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm4code-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
