{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['training_args.bin', 'model.safetensors', 'cap', 'generation_config.json', 'config.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_path = \"/home/user1-system11/Documents/research-shradha/CODE-SPT-Code/spt-code/outputs/pre_train_20241112_201020/models/\"\n",
    "print(os.listdir(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and vocabularies loaded successfully.\n",
      "Code vocab size: 919\n",
      "AST vocab size: 34\n",
      "NL vocab size: 1878\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import BartConfig\n",
    "from models.bart import BartForClassificationAndGeneration\n",
    "from data.vocab import load_vocab\n",
    "\n",
    "# Paths to the model and vocab directories\n",
    "model_dir = \"/home/user1-system11/Documents/research-shradha/CODE-SPT-Code/spt-code/outputs/pre_train_20241115_232553/models\"\n",
    "vocab_dir = \"/home/user1-system11/Documents/research-shradha/CODE-SPT-Code/spt-code/outputs/pre_train_20241115_232553/vocabs\"\n",
    "\n",
    "# Load model configuration and weights\n",
    "config_path = os.path.join(model_dir, \"config.json\")\n",
    "model_weights_path = os.path.join(model_dir, \"model.safetensors\")\n",
    "\n",
    "config = BartConfig.from_json_file(config_path)\n",
    "model = BartForClassificationAndGeneration.from_pretrained(model_weights_path, config=config)\n",
    "\n",
    "# Load vocabularies\n",
    "code_vocab = load_vocab(vocab_root=vocab_dir, name=\"code\")\n",
    "ast_vocab = load_vocab(vocab_root=vocab_dir, name=\"ast\")\n",
    "nl_vocab = load_vocab(vocab_root=vocab_dir, name=\"nl\")\n",
    "\n",
    "print(f\"Model and vocabularies loaded successfully.\")\n",
    "print(f\"Code vocab size: {len(code_vocab)}\")\n",
    "print(f\"AST vocab size: {len(ast_vocab)}\")\n",
    "print(f\"NL vocab size: {len(nl_vocab)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EOS_TOKEN', 'MSK_TOKEN', 'PAD_TOKEN', 'SEP_TOKEN', 'SOS_TOKEN', 'START_VOCAB', 'UNK_TOKEN', '_Vocab__special_symbols', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'add_special_symbols', 'decode', 'decode_batch', 'encode_batch', 'encode_sequence', 'eos_processor', 'get_eos_index', 'get_index', 'get_mask_index', 'get_pad_index', 'get_sos_index', 'get_token', 'get_unk_index', 'ignore_case', 'index_offset', 'method', 'name', 'num_special_token', 'pad_token_id', 'restore_index', 'save', 'save_pickle', 'save_pretrained', 'sep_processor', 'sos_processor', 'tokenizer', 'transfer_index']\n"
     ]
    }
   ],
   "source": [
    "print(dir(code_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate 1: this . readwritelock . readlock ( ) (Probability: 30.31%)\n",
      "Candidate 2: this . readwritelock . writelock ( ) (Probability: 21.15%)\n",
      "Candidate 3: this . parents (Probability: 16.34%)\n",
      "Candidate 4: this . readwritelock . getapplication lock ( ) (Probability: 16.31%)\n",
      "Candidate 5: this . delegate (Probability: 15.90%)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartConfig\n",
    "from data.vocab import load_vocab\n",
    "\n",
    "def load_model_and_vocab(model_dir, vocab_dir):\n",
    "    config_path = os.path.join(model_dir, 'config.json')\n",
    "    model_path = os.path.join(model_dir, 'model.safetensors')\n",
    "    config = BartConfig.from_json_file(config_path)\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_path, config=config)\n",
    "    model.eval()  \n",
    "\n",
    "    code_vocab = load_vocab(vocab_dir, \"code\")\n",
    "    nl_vocab = load_vocab(vocab_dir, \"nl\")\n",
    "    ast_vocab = load_vocab(vocab_dir, \"ast\")\n",
    "    \n",
    "    return model, code_vocab#, nl_vocab, ast_vocab\n",
    "\n",
    "def generate_candidates(model, input_text, vocab, num_beams=5, max_length=50):\n",
    "    input_ids, attention_mask = vocab.encode_sequence(input_text)\n",
    "    input_ids = torch.tensor([input_ids])  \n",
    "    attention_mask = torch.tensor([attention_mask])\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        num_return_sequences=num_beams,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "    \n",
    "    candidates = vocab.decode_batch(outputs.sequences.tolist())\n",
    "    probabilities = torch.softmax(outputs.sequences_scores, dim=0).tolist()\n",
    "    \n",
    "    return candidates, probabilities\n",
    "\n",
    "def main():\n",
    "    model_dir = \"/home/user1-system11/Documents/research-shradha/CODE-SPT-Code-TreeSitterV3/SPT-Code/outputs/pre_train_20241105_105459/models\"  \n",
    "    vocab_dir = \"/home/user1-system11/Documents/research-shradha/CODE-SPT-Code-TreeSitterV3/SPT-Code/outputs/pre_train_20241105_105459/vocabs\"  \n",
    "    \n",
    "    incomplete_code = \"public void writeLock() {    this.fsLock.longReadLock().lock();     [MSK] .lock();}\" \n",
    "    \n",
    "    model, code_vocab = load_model_and_vocab(model_dir, vocab_dir)\n",
    "    \n",
    "    candidates, probabilities = generate_candidates(model, incomplete_code, code_vocab)\n",
    "    \n",
    "    for idx, (candidate, prob) in enumerate(zip(candidates, probabilities)):\n",
    "        print(f\"Candidate {idx + 1}: {candidate} (Probability: {prob:.2%})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1-system11/miniconda3/envs/llm4code-project/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/user1-system11/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\")  # Replace with your Hugging Face access token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/shradha01/code_completion_tokenizer', endpoint='https://huggingface.co', repo_type='model', repo_id='shradha01/code_completion_tokenizer')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import create_repo\n",
    "\n",
    "repo_name = \"code_completion_tokenizer\"\n",
    "create_repo(repo_id=repo_name, token=\"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\", private=False)  # Adjust `private` as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "# Load the individual tokenizers\n",
    "ast_tokenizer = Tokenizer.from_file(\"/home/user1-system11/Documents/research-shradha/CODE-SPT-Code-TreeSitterV3/SPT-Code/outputs/pre_train_20241116_103947/multi-tokenizer/ast_tokenizer.json\")\n",
    "code_tokenizer = Tokenizer.from_file(\"/home/user1-system11/Documents/research-shradha/CODE-SPT-Code-TreeSitterV3/SPT-Code/outputs/pre_train_20241116_103947/multi-tokenizer/code_tokenizer.json\")\n",
    "nl_tokenizer = Tokenizer.from_file(\"/home/user1-system11/Documents/research-shradha/CODE-SPT-Code-TreeSitterV3/SPT-Code/outputs/pre_train_20241116_103947/multi-tokenizer/nl_tokenizer.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ast_vocab = ast_tokenizer.get_vocab()\n",
    "code_vocab = code_tokenizer.get_vocab()\n",
    "nl_vocab = nl_tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_vocab = {**ast_vocab, **code_vocab, **nl_vocab}  # Combines dictionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_vocab = {token: idx for idx, (token, _) in enumerate(merged_vocab.items())}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.models import WordLevel\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "unified_tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "\n",
    "special_tokens = [\"[PAD]\", \"[SOS]\", \"[EOS]\", \"[SEP]\", \"[MSK]\"]\n",
    "unified_tokenizer.model = WordLevel(unk_token=\"[UNK]\")\n",
    "\n",
    "unified_tokenizer.model = WordLevel(unk_token=\"[UNK]\", vocab=merged_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for token in special_tokens:\n",
    "    if token not in merged_vocab:\n",
    "        merged_vocab[token] = len(merged_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [52]\n",
      "Decoded: [UNK]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "unified_tokenizer.enable_padding(pad_id=merged_vocab[\"[PAD]\"], pad_token=\"[PAD]\")\n",
    "\n",
    "encoded = unified_tokenizer.encode(\"This is a test sentence.\")\n",
    "print(\"Encoded:\", encoded.ids)\n",
    "\n",
    "decoded = unified_tokenizer.decode(encoded.ids)\n",
    "print(\"Decoded:\", decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_tokenizer.save(\"/home/user1-system11/Documents/research-shradha/CODE-SPT-Code-TreeSitterV3/SPT-Code/outputs/pre_train_20241116_103947/multi-tokenizer/unified_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1-system11/miniconda3/envs/llm4code-project/lib/python3.11/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/shradha01/multi-purpose-tokenizer/commit/c7fa6ea69d507c600b1d6a352b86c15c35105ca4', commit_message='Upload tokenizer', commit_description='', oid='c7fa6ea69d507c600b1d6a352b86c15c35105ca4', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"/home/user1-system11/Documents/research-shradha/CODE-SPT-Code-TreeSitterV3/SPT-Code/outputs/pre_train_20241116_103947/multi-tokenizer/unified_tokenizer.json\")\n",
    "tokenizer.push_to_hub(\"multi-purpose-tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/shradha01/code-tokenizer/commit/125e94d6d049d81e3abf8464f7a6ad01e87c626c', commit_message='Upload tokenizer', commit_description='', oid='125e94d6d049d81e3abf8464f7a6ad01e87c626c', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"/home/user1-system11/Documents/research-shradha/CODE-SPT-Code-TreeSitterV3/SPT-Code/outputs/pre_train_20241116_103947/vocabs/code/code_tokenizer.json\")\n",
    "tokenizer.push_to_hub(\"code-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n",
      "/home/user1-system11/miniconda3/envs/llm4code-project/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/user1-system11/miniconda3/envs/llm4code-project/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "model.safetensors: 100%|██████████| 1.05G/1.05G [00:21<00:00, 47.7MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/shradha01/Code-completion-model/commit/a24df0e84856e88dc732f11c04f9a8ba1145027a', commit_message='Upload model', commit_description='', oid='a24df0e84856e88dc732f11c04f9a8ba1145027a', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model_dir = \"/home/user1-system11/Documents/research-shradha/CODE-SPT-Code-TreeSitterV3/SPT-Code/outputs/pre_train_20241116_103947/models\"  \n",
    "repo_name = \"Code-completion-model\"  \n",
    "\n",
    "model = AutoModel.from_pretrained(model_dir)\n",
    "\n",
    "model.push_to_hub(repo_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1-system11/miniconda3/envs/llm4code-project/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/user1-system11/miniconda3/envs/llm4code-project/lib/python3.11/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/user1-system11/miniconda3/envs/llm4code-project/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/user1-system11/miniconda3/envs/llm4code-project/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/user1-system11/miniconda3/envs/llm4code-project/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Load the model and tokenizer from the Hugging Face Hub\n",
    "model_name = \"shradha01/code-completion\"\n",
    "tokenizer_name = \"shradha01/code-tokenizer\"#\"shradha01/multi-purpose-tokenizer\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "code_completion_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete_code = \"public void writeLock() {    this.fsLock.longReadLock().lock();     [MSK] .lock();}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: tensor([[ 145,  206, 2851,   11,   12,   67,  193,   17,  925,  378,   17,  330,\n",
      "         3533,   11,   12,   17,  378,   11,   12,   30,    4,   17,  378,   11,\n",
      "           12,   30,   69]])\n",
      "Tokens: ['public', 'void', 'writelock', '(', ')', '{', 'this', '.', 'fs', 'lock', '.', 'long', 'readlock', '(', ')', '.', 'lock', '(', ')', ';', '[MSK]', '.', 'lock', '(', ')', ';', '}']\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer(incomplete_code, return_tensors=\"pt\")\n",
    "print(\"Token IDs:\", encoded.input_ids)\n",
    "print(\"Tokens:\", [tokenizer.decode([id]) for id in encoded.input_ids[0]])\n",
    "\n",
    "if any(id is None for id in encoded.input_ids[0]):\n",
    "    print(\"Found out-of-vocabulary tokens!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if \"[MSK]\" not in tokenizer.get_vocab():\n",
    "#     print(\"`[MSK]` token is missing from the vocabulary!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if \"[MSK]\" not in tokenizer.get_vocab():\n",
    "#     tokenizer.add_tokens([\"[MSK]\"])\n",
    "#     model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_input = tokenizer(incomplete_code, return_tensors=\"pt\")\n",
    "# print(\"Input IDs:\", encoded_input.input_ids)\n",
    "# print(\"Tokens:\", [tokenizer.decode([token_id]) for token_id in encoded_input.input_ids[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocabulary size: 50000\n",
      "Model vocabulary size: 2831\n",
      "Resized model embeddings to match tokenizer vocabulary size.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer_vocab_size = len(tokenizer)\n",
    "print(\"Tokenizer vocabulary size:\", tokenizer_vocab_size)\n",
    "\n",
    "model_vocab_size = model.config.vocab_size\n",
    "print(\"Model vocabulary size:\", model_vocab_size)\n",
    "\n",
    "if tokenizer_vocab_size != model_vocab_size:\n",
    "    model.resize_token_embeddings(tokenizer_vocab_size)\n",
    "    print(\"Resized model embeddings to match tokenizer vocabulary size.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 145,  206, 2851,   11,   12,   67,  193,   17,  925,  378,   17,  330,\n",
      "         3533,   11,   12,   17,  378,   11,   12,   30,    4,   17,  378,   11,\n",
      "           12,   30,   69]])\n",
      "Decoded Tokens: public void writelock ( ) { this. fs lock. long readlock ( ). lock ( ) ; [MSK]. lock ( ) ; }\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoded_input = tokenizer(incomplete_code, return_tensors=\"pt\")\n",
    "print(\"Input IDs:\", encoded_input.input_ids)\n",
    "print(\"Decoded Tokens:\", tokenizer.decode(encoded_input.input_ids[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate 1: totalmemory totalmemory totalmemory totalmemory setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo able_ able_ able_ able_ able_ able_ able_ able_ able_ able_ able_ able_ able_ able_ able_ able_ able_ able_ able_ able_ able_ able_ able_ setvideo setvideo setvideo setvideo setvideo\n",
      "Candidate 2: totalmemory totalmemory totalmemory totalmemory setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo able_ able_ able_ able_ able_ able_ able_ able_ able_ able_ able_ able_ able_ able_ able_ able_ able_ able_ able_ able_ able_ able_ able_ able_ able_ able_ able_ able_\n",
      "Candidate 3: totalmemory totalmemory totalmemory totalmemory totalmemory totalmemory setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo\n",
      "Candidate 4: totalmemory totalmemory totalmemory totalmemory totalmemory setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo\n",
      "Candidate 5: totalmemory totalmemory totalmemory totalmemory setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo setvideo\n"
     ]
    }
   ],
   "source": [
    "\n",
    "candidates = code_completion_pipeline(\n",
    "    incomplete_code,\n",
    "    max_length=50,\n",
    "    num_return_sequences=5,\n",
    "    num_beams=5\n",
    ")\n",
    "\n",
    "for idx, candidate in enumerate(candidates):\n",
    "    print(f\"Candidate {idx + 1}: {candidate['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1-system11/miniconda3/envs/llm4code-project/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
      "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Tried to clone a repository in a non-empty folder that isn't a git repository ('/home/user1-system11/Documents/research-shradha/CODE-SPT-Code-TreeSitterV3/SPT-Code/outputs/pre_train_20241105_105459/models'). If you really want to do this, do it manually:\n cd /home/user1-system11/Documents/research-shradha/CODE-SPT-Code-TreeSitterV3/SPT-Code/outputs/pre_train_20241105_105459/models && git init && git remote add origin && git pull origin main\n or clone repo to a new folder and move your existing files there afterwards.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m repository_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode-completion-model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m organization_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshradha01\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Optional, replace with your Hugging Face organization name if any\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mpush_model_to_huggingface\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepository_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morganization_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m, in \u001b[0;36mpush_model_to_huggingface\u001b[0;34m(model_dir, repo_name, organization)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Push model\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m repo \u001b[38;5;241m=\u001b[39m \u001b[43mRepository\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclone_from\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://huggingface.co/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrepo_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m repo\u001b[38;5;241m.\u001b[39mpush_to_hub(commit_message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdd fine-tuned code completion model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel pushed to Hugging Face Hub at: https://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm4code-project/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm4code-project/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:132\u001b[0m, in \u001b[0;36m_deprecate_method.<locals>._inner_deprecate_method.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m     warning_message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m message\n\u001b[1;32m    131\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(warning_message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm4code-project/lib/python3.11/site-packages/huggingface_hub/repository.py:534\u001b[0m, in \u001b[0;36mRepository.__init__\u001b[0;34m(self, local_dir, clone_from, repo_type, token, git_user, git_email, revision, skip_lfs_files, client)\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhuggingface_token \u001b[38;5;241m=\u001b[39m get_token()\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clone_from \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclone_from\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_git_repo(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_dir):\n",
      "File \u001b[0;32m~/miniconda3/envs/llm4code-project/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm4code-project/lib/python3.11/site-packages/huggingface_hub/repository.py:698\u001b[0m, in \u001b[0;36mRepository.clone_from\u001b[0;34m(self, repo_url, token)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;66;03m# Check if the folder is the root of a git repository\u001b[39;00m\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_git_repo(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_dir):\n\u001b[0;32m--> 698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    699\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to clone a repository in a non-empty folder that isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    700\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m a git repository (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m). If you really want to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    701\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m do this, do it manually:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m cd \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m && git init\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    702\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m && git remote add origin && git pull origin main\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m or clone\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    703\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m repo to a new folder and move your existing files there\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    704\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m afterwards.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_local_clone(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_dir, repo_url):\n\u001b[1;32m    708\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    709\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is already a clone of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclean_repo_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    710\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Make sure you pull the latest changes with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    711\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `repo.git_pull()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    712\u001b[0m         )\n",
      "\u001b[0;31mOSError\u001b[0m: Tried to clone a repository in a non-empty folder that isn't a git repository ('/home/user1-system11/Documents/research-shradha/CODE-SPT-Code-TreeSitterV3/SPT-Code/outputs/pre_train_20241105_105459/models'). If you really want to do this, do it manually:\n cd /home/user1-system11/Documents/research-shradha/CODE-SPT-Code-TreeSitterV3/SPT-Code/outputs/pre_train_20241105_105459/models && git init && git remote add origin && git pull origin main\n or clone repo to a new folder and move your existing files there afterwards."
     ]
    }
   ],
   "source": [
    "from huggingface_hub import Repository, HfApi\n",
    "\n",
    "def push_model_to_huggingface(model_dir: str, repo_name: str, organization: str = None):\n",
    "\n",
    "    from huggingface_hub import HfApi, Repository\n",
    "\n",
    "    api = HfApi()\n",
    "\n",
    "    repo_id = \"shradha01/code-completion-model\"\n",
    "    try:\n",
    "        api.create_repo(repo_id=repo_id, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating repository: {e}\")\n",
    "        return\n",
    "\n",
    "    # Push model\n",
    "    repo = Repository(local_dir=model_dir, clone_from=f\"https://huggingface.co/{repo_id}\")\n",
    "    repo.push_to_hub(commit_message=\"Add fine-tuned code completion model\")\n",
    "\n",
    "    print(f\"Model pushed to Hugging Face Hub at: https://huggingface.co/{repo_id}\")\n",
    "\n",
    "# Usage\n",
    "model_directory = \"/home/user1-system11/Documents/research-shradha/CODE-SPT-Code-TreeSitterV3/SPT-Code/outputs/pre_train_20241105_105459/models\"\n",
    "repository_name = \"code-completion-model\"\n",
    "organization_name = \"shradha01\"  # Optional, replace with your Hugging Face organization name if any\n",
    "push_model_to_huggingface(model_directory, repository_name, organization_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1-system11/miniconda3/envs/llm4code-project/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/user1-system11/miniconda3/envs/llm4code-project/lib/python3.11/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/user1-system11/miniconda3/envs/llm4code-project/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/user1-system11/miniconda3/envs/llm4code-project/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BartConfig, GenerationConfig\n",
    "from models.bart import BartForClassificationAndGeneration\n",
    "from data.vocab import load_vocab\n",
    "from utils.trainer import CodeTrainer\n",
    "from utils.callbacks import LogStateCallBack\n",
    "from data.data_collator import collate_fn\n",
    "import os\n",
    "\n",
    "\n",
    "class UserInputDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A simple dataset for wrapping user-provided input code snippets for prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_code, code_vocab, max_length=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_code (str): The user-provided incomplete code snippet.\n",
    "            code_vocab (Vocab): Pretrained vocabulary for code tokens.\n",
    "            max_length (int): Maximum length for tokenized sequences.\n",
    "        \"\"\"\n",
    "        self.max_length = max_length\n",
    "        self.code_vocab = code_vocab\n",
    "        self.pad_id = code_vocab.get_pad_index()  # Padding token index\n",
    "        self.unk_id = code_vocab.get_unk_index()  # Unknown token index\n",
    "\n",
    "        # Tokenize the input code snippet\n",
    "        input_tokens = input_code.split()\n",
    "        self.input_ids = [\n",
    "            code_vocab.get_index(token) if code_vocab.get_index(token) is not None else self.unk_id\n",
    "            for token in input_tokens\n",
    "        ]\n",
    "\n",
    "        # Create attention mask (1 for valid tokens, 0 for padding)\n",
    "        self.attention_mask = [1] * len(self.input_ids)\n",
    "\n",
    "        # Pad or truncate to max_length\n",
    "        if len(self.input_ids) < self.max_length:\n",
    "            self.input_ids += [self.pad_id] * (self.max_length - len(self.input_ids))\n",
    "            self.attention_mask += [0] * (self.max_length - len(self.attention_mask))\n",
    "        else:\n",
    "            self.input_ids = self.input_ids[:self.max_length]\n",
    "            self.attention_mask = self.attention_mask[:self.max_length]\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1  # Single user input case\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(self.attention_mask, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_MODE_GEN: bart_gen\n",
      "MODEL_MODE_CLS: bart_cls\n",
      "MODEL_MODE_SEARCH: bart_search\n"
     ]
    }
   ],
   "source": [
    "import enums\n",
    "print(\"MODEL_MODE_GEN:\", enums.MODEL_MODE_GEN)\n",
    "print(\"MODEL_MODE_CLS:\", enums.MODEL_MODE_CLS)\n",
    "print(\"MODEL_MODE_SEARCH:\", enums.MODEL_MODE_SEARCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, IntervalStrategy, SchedulerType\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "def run_custom_completion(args, model_dir, vocab_dir, input_code):\n",
    "    \"\"\"\n",
    "    Run custom code completion using Seq2SeqTrainingArguments and CodeTrainer.\n",
    "\n",
    "    Args:\n",
    "        args: Arguments with necessary configurations.\n",
    "        model_dir: Path to the saved model.\n",
    "        vocab_dir: Path to the vocabularies.\n",
    "        input_code: User-provided incomplete code.\n",
    "\n",
    "    Returns:\n",
    "        List: Generated completions.\n",
    "    \"\"\"\n",
    "    # Load Model\n",
    "    config = BartConfig.from_pretrained(os.path.join(model_dir, \"config.json\"))\n",
    "    model = BartForClassificationAndGeneration.from_pretrained(\n",
    "        os.path.join(model_dir, \"model.safetensors\"),\n",
    "        config=config\n",
    "    )\n",
    "    model.set_model_mode(enums.MODEL_MODE_GEN)\n",
    "\n",
    "    # Assign generation_config directly to the model\n",
    "    model.generation_config = GenerationConfig(\n",
    "        max_length=50,\n",
    "        num_beams=5,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    # Load Vocabularies\n",
    "    code_vocab = load_vocab(vocab_dir, \"code\")\n",
    "    nl_vocab = load_vocab(vocab_dir, \"nl\")\n",
    "    ast_vocab = load_vocab(vocab_dir, \"ast\")\n",
    "\n",
    "    # Prepare Dataset\n",
    "    dataset = UserInputDataset(input_code=input_code, code_vocab=code_vocab, max_length=args.max_code_len)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=1, collate_fn=lambda batch: collate_fn(batch, args, \"completion\", code_vocab, nl_vocab, ast_vocab)\n",
    "    )\n",
    "\n",
    "\n",
    "     # Initialize Trainer without relying on args.generation_config\n",
    "    trainer = CodeTrainer(\n",
    "        main_args=args,\n",
    "        code_vocab=code_vocab,\n",
    "        ast_vocab=ast_vocab,\n",
    "        nl_vocab=nl_vocab,\n",
    "        task=\"completion\",\n",
    "        model=model,\n",
    "        data_collator=None,\n",
    "        tokenizer=nl_vocab,\n",
    "        compute_metrics=None,  # Metrics are not needed for single prediction\n",
    "        callbacks=[LogStateCallBack()]\n",
    "    )\n",
    "\n",
    "    # Generate Predictions\n",
    "    results = trainer.predict(test_dataset=data_loader)\n",
    "    predictions = results.predictions\n",
    "\n",
    "    # Decode Predictions\n",
    "    decoded_completions = [code_vocab.decode(output.tolist()) for output in predictions]\n",
    "    return decoded_completions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TrainingArguments' object has no attribute 'generation_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m args \u001b[38;5;241m=\u001b[39m Args()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Run the completion function\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m completions \u001b[38;5;241m=\u001b[39m \u001b[43mrun_custom_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_code\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Display the results\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGenerated Completions:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[16], line 45\u001b[0m, in \u001b[0;36mrun_custom_completion\u001b[0;34m(args, model_dir, vocab_dir, input_code)\u001b[0m\n\u001b[1;32m     39\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m     40\u001b[0m     dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, collate_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m batch: collate_fn(batch, args, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion\u001b[39m\u001b[38;5;124m\"\u001b[39m, code_vocab, nl_vocab, ast_vocab)\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     44\u001b[0m  \u001b[38;5;66;03m# Initialize Trainer without relying on args.generation_config\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mCodeTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmain_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcode_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_vocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mast_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mast_vocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnl_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnl_vocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompletion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnl_vocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Metrics are not needed for single prediction\u001b[39;49;00m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mLogStateCallBack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Generate Predictions\u001b[39;00m\n\u001b[1;32m     59\u001b[0m results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(test_dataset\u001b[38;5;241m=\u001b[39mdata_loader)\n",
      "File \u001b[0;32m~/Documents/research-shradha/CODE-SPT-Code/spt-code/sources/utils/trainer.py:14\u001b[0m, in \u001b[0;36mCodeTrainer.__init__\u001b[0;34m(self, main_args, code_vocab, ast_vocab, nl_vocab, task, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, main_args: argparse\u001b[38;5;241m.\u001b[39mNamespace, code_vocab, ast_vocab, nl_vocab, task, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mCodeTrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmain_args \u001b[38;5;241m=\u001b[39m main_args\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcode_vocab \u001b[38;5;241m=\u001b[39m code_vocab\n",
      "File \u001b[0;32m~/miniconda3/envs/llm4code-project/lib/python3.11/site-packages/transformers/trainer_seq2seq.py:72\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     57\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     58\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m     preprocess_logits_for_metrics\u001b[38;5;241m=\u001b[39mpreprocess_logits_for_metrics,\n\u001b[1;32m     68\u001b[0m )\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Override self.model.generation_config if a GenerationConfig is specified in args.\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Priority: args.generation_config > model.generation_config > default GenerationConfig.\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration_config\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     gen_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_generation_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgeneration_config)\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgeneration_config \u001b[38;5;241m=\u001b[39m gen_config\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TrainingArguments' object has no attribute 'generation_config'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example Usage\n",
    "class Args:\n",
    "    max_code_len = 128\n",
    "    # Add other required arguments\n",
    "\n",
    "# Define paths\n",
    "model_dir = \"/home/user1-system11/Documents/research-shradha/CODE-SPT-Code/spt-code/outputs/pre_train_20241115_232553/models\"\n",
    "vocab_dir = \"/home/user1-system11/Documents/research-shradha/CODE-SPT-Code/spt-code/outputs/pre_train_20241115_232553/vocabs\"\n",
    "\n",
    "# User-provided incomplete code snippet\n",
    "input_code = \"for (int i = 0; i < n; i++) {\"\n",
    "\n",
    "# Initialize arguments\n",
    "args = Args()\n",
    "\n",
    "# Run the completion function\n",
    "completions = run_custom_completion(args, model_dir, vocab_dir, input_code)\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nGenerated Completions:\")\n",
    "for i, completion in enumerate(completions, start=1):\n",
    "    print(f\"Completion {i}: {completion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm4code-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
