{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### importing necessary libraries and the tokenizers library, specifically importing the ByteLevelBPETokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tokenizers import ByteLevelBPETokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### defining the parameters including train_data_file, output_dir, vocab_size, and min_freq. (Took the Tokenizer training.txt file of java dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file = \"DATASET\"\n",
    "output_dir = \"Output\"\n",
    "vocab_size = 5000\n",
    "min_freq = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating an instance of the ByteLevelBPETokenizer class from the tokenizers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### processing the args.train_data_file to obtain the absolute path and, if it does not end with \".txt,\" assuming it is a directory and collecting all the \".txt\" files within that directory using the glob method from the pathlib module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/user1-selab3/shradha_test/roberta/DATASET/JAVA/TOKEN/RAW/tokenizer_training.txt']\n"
     ]
    }
   ],
   "source": [
    "paths = os.path.abspath(train_data_file)\n",
    "\n",
    "if not train_data_file.endswith(\".txt\"):\n",
    "    paths = [str(x) for x in Path(paths).glob(\"**/*.txt\")]\n",
    "    print(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The training involves specifying the input files (files parameter), vocabulary size (vocab_size parameter), minimum frequency (min_frequency parameter), and special tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "files: This parameter is set to paths, which is a list of file paths. These files are used as the training data for the tokenizer. It can be a single file or a list of files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocab_size: This parameter is set to args.vocab_size, which is the maximum vocabulary size. The tokenizer will learn subword units until the vocabulary size reaches this specified limit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "min_frequency: This parameter is set to args.min_freq, which is the minimum number of occurrences for a subword unit to be included in the vocabulary. Subword units with a frequency less than this threshold will be discarded.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "These parameters control the training process and influence the size and composition of the vocabulary that the tokenizer will learn. Adjusting vocab_size and min_frequency allows you to control the granularity and size of the vocabulary based on the characteristics of your training data. After training, the tokenizer will have learned a vocabulary that can be used to tokenize text into subword units. The training process involves iteratively merging the most frequent pairs of consecutive subword units until the vocabulary size reaches the specified limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=vocab_size, min_frequency=min_freq, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After training, this line adds additional special tokens <x> and <z> to the tokenizer. Special tokens are often used to represent certain elements in the text, and they are typically defined during the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens([\"<x>\",\"<z>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This block ensures that the specified output directory exists. If it doesn't, it creates the directory. Then, it saves the trained tokenizer model to the output directory using tokenizer.save_model()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/user1-selab3/shradha_test/roberta/Output/vocab.json',\n",
       " '/home/user1-selab3/shradha_test/roberta/Output/merges.txt']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Save files to disk\n",
    "output_dir = os.path.abspath(output_dir)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "tokenizer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
